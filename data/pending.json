[
  {
    "id": "3e8b44b4-8c0f-4d5e-90c9-0fa457316697",
    "title": "“Gavagai!” or the future history of the animal language controversy",
    "authors": [
      "Premack, David"
    ],
    "year": "1985",
    "journal": "Cognition",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885620"
  },
  {
    "id": "592305e5-030c-4e04-a28b-8f4b705a3c28",
    "title": "New dimensions in animal communication: the case for complexity",
    "authors": [
      "Patricelli, Gail L",
      "Hebets, Eileen A"
    ],
    "year": "2016",
    "journal": "Current Opinion in Behavioral Sciences",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885624"
  },
  {
    "id": "19967a8e-41f2-469a-a193-b87b85036c29",
    "title": "Animal Communication Overview",
    "authors": [
      "Rossano,  Federico",
      "Kaufhold,  Stephan P."
    ],
    "year": "2021",
    "journal": "The Cambridge Handbook of Animal Cognition",
    "abstract": "",
    "doi": "10.1017/9781108564113.003",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885628"
  },
  {
    "id": "8031f094-7901-4843-a632-86d5c77a0ca9",
    "title": "Do responses of galliform birds vary adaptively with predator size?",
    "authors": [
      "Palleroni,  Alberto",
      "Hauser,  Marc",
      "Marler,  Peter"
    ],
    "year": "2005",
    "journal": "Animal Cognition",
    "abstract": "",
    "doi": "10.1007/s10071-004-0250-y",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885633"
  },
  {
    "id": "5877b42c-9f15-49af-9e90-e200cfc7d705",
    "title": "Unfamiliar environments impair information processing as measured by behavioral and cardiac orienting responses to auditory stimuli in preweanling and adult rats",
    "authors": [
      "Richardson,  Rick",
      "Siegel,  Michael A.",
      "Campbell,  Byron A."
    ],
    "year": "1988",
    "journal": "Developmental Psychobiology",
    "abstract": "",
    "doi": "10.1002/dev.420210508",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885638"
  },
  {
    "id": "3176d256-7432-41f7-ae4a-3f87560c1b7c",
    "title": "Tympanal hearing in insects",
    "authors": [
      "Hoy, Ronald R",
      "Robert, Daniel"
    ],
    "year": "1996",
    "journal": "Annual review of entomology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885642"
  },
  {
    "id": "8d27f6c8-76c6-43be-89ab-96623e98a5fa",
    "title": "Auditory localization: A comprehensive practical review",
    "authors": [
      "Carlini, Alessandro",
      "Bordeau, Camille",
      "Ambard, Maxime"
    ],
    "year": "2024",
    "journal": "Frontiers in Psychology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885650"
  },
  {
    "id": "4ca44769-2a58-4145-8235-55b8240db13f",
    "title": "Sex detection of chicks based on audio technology and deep learning methods",
    "authors": [
      "Li, Zeying",
      "Zhang, Tiemin",
      "Cuan, Kaixuan",
      "Fang, Cheng",
      "Zhao, Hongzhi",
      "Guan, Chenxi",
      "Yang, Qilian",
      "Qu, Hao"
    ],
    "year": "2022",
    "journal": "Animals",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885654"
  },
  {
    "id": "dd744a09-9a0e-4b51-b3fe-c1802fc173e4",
    "title": "African elephants address one another with individually specific name-like calls",
    "authors": [
      "Pardo, Michael A",
      "Fristrup, Kurt",
      "Lolchuragi, David S",
      "Poole, Joyce H",
      "Granli, Petter",
      "Moss, Cynthia",
      "Douglas-Hamilton, Iain",
      "Wittemyer, George"
    ],
    "year": "2024",
    "journal": "Nature Ecology \\& Evolution",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885659"
  },
  {
    "id": "96536791-e8ac-4d72-a387-9ae48a2fc203",
    "title": "BioCPPNet: automatic bioacoustic source separation with deep neural networks",
    "authors": [
      "Bermant, Peter C"
    ],
    "year": "2021",
    "journal": "Scientific Reports",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885662"
  },
  {
    "id": "443fdbf4-2378-4792-aca3-94349ada6bf8",
    "title": "Modeling animal vocalizations through synthesizers",
    "authors": [
      "Hagiwara, Masato",
      "Cusimano, Maddie",
      "Liu, Jen-Yu"
    ],
    "year": "2022",
    "journal": "arXiv preprint arXiv:2210.10857",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885669"
  },
  {
    "id": "1117ebed-34f2-4898-a1de-3646dcc95cf6",
    "title": "AVES: Animal vocalization encoder based on self-supervision",
    "authors": [
      "Hagiwara, Masato"
    ],
    "year": "2023",
    "journal": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885672"
  },
  {
    "id": "1a72d360-8bcc-4e3e-a7d8-5968a4a47ac6",
    "title": "ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds",
    "authors": [
      "Hagiwara, Masato",
      "Miron, Marius",
      "Liu, Jen-Yu"
    ],
    "year": "2024",
    "journal": "2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885676"
  },
  {
    "id": "010443c7-2edf-4bfa-8044-6b1cc1855f32",
    "title": "NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics",
    "authors": [
      "Robinson, David",
      "Miron, Marius",
      "Hagiwara, Masato",
      "Pietquin, Olivier"
    ],
    "year": "2024",
    "journal": "arXiv preprint arXiv:2411.07186",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885685"
  },
  {
    "id": "f053c156-d190-4e2b-8aad-c3421da6ee4e",
    "title": "Synthetic data enables context-aware bioacoustic sound event detection",
    "authors": [
      "Hoffman, Benjamin",
      "Robinson, David",
      "Miron, Marius",
      "Baglione, Vittorio",
      "Canestrari, Daniela",
      "Elias, Damian",
      "Trapote, Eva",
      "Pietquin, Olivier"
    ],
    "year": "2025",
    "journal": "arXiv preprint arXiv:2503.00296",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885690"
  },
  {
    "id": "d671bbf6-2cd3-43ae-b6e8-862fdf13d83f",
    "title": "Biodenoising: animal vocalization denoising without access to clean data",
    "authors": [
      "Miron, Marius",
      "Keen, Sara",
      "Liu, Jen-Yu",
      "Hoffman, Benjamin",
      "Hagiwara, Masato",
      "Pietquin, Olivier",
      "Effenberger, Felix",
      "Cusimano, Maddie"
    ],
    "year": "2024",
    "journal": "arXiv preprint arXiv:2410.03427",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885695"
  },
  {
    "id": "d900f600-863a-41fa-86c6-5b803a5eef49",
    "title": "Human listeners are able to classify dog (Canis familiaris) barks recorded in different situations.",
    "authors": [
      "Pongr{\\'a}cz, P{\\'e}ter",
      "Moln{\\'a}r, Csaba",
      "Mikl{\\'o}si, Ad{\\'a}m",
      "Cs{\\'a}nyi, Vilmos"
    ],
    "year": "2005",
    "journal": "Journal of comparative psychology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885702"
  },
  {
    "id": "6fe454f1-6a1c-41f2-b919-2e9a03057daa",
    "title": "Acoustic parameters of dog barks carry emotional information for humans",
    "authors": [
      "Pongr{\\'a}cz, P{\\'e}ter",
      "Moln{\\'a}r, Csaba",
      "Miklosi, Adam"
    ],
    "year": "2006",
    "journal": "Applied Animal Behaviour Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885707"
  },
  {
    "id": "328fccf8-4d4d-4f29-9a35-924e3162912f",
    "title": "Can humans discriminate between dogs on the base of the acoustic parameters of barks?",
    "authors": [
      "Moln{\\'a}r, Csaba",
      "Pongr{\\'a}cz, P{\\'e}ter",
      "D{\\'o}ka, Antal",
      "Mikl{\\'o}si, {\\'A}d{\\'a}m"
    ],
    "year": "2006",
    "journal": "Behavioural processes",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885712"
  },
  {
    "id": "5e6d6c4f-b68d-44a4-adf3-cc110138cc34",
    "title": "Classification of dog barks: a machine learning approach",
    "authors": [
      "Molnár,  Csaba",
      "Kaplan,  Frédéric",
      "Roy,  Pierre",
      "Pachet,  Fran\\c{c}ois",
      "Pongrácz,  Péter",
      "Dóka,  Antal",
      "Miklósi,  Ádám"
    ],
    "year": "2008",
    "journal": "Animal Cognition",
    "abstract": "",
    "doi": "10.1007/s10071-007-0129-9",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885716"
  },
  {
    "id": "42eaa8c1-10ba-4aea-aa78-a37ac3a890ae",
    "title": "Comparing supervised learning methods for classifying sex,  age,  context and individual Mudi dogs from barking",
    "authors": [
      "Larrañaga,  Ana",
      "Bielza,  Concha",
      "Pongrácz,  Péter",
      "Faragó,  Tamás",
      "Bálint,  Anna",
      "Larrañaga,  Pedro"
    ],
    "year": "2014",
    "journal": "Animal Cognition",
    "abstract": "",
    "doi": "10.1007/s10071-014-0811-7",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885721"
  },
  {
    "id": "5943c1f0-7f6a-4f13-8099-e9bbab927fdc",
    "title": "Automatic classification of context in induced barking",
    "authors": [
      "P{\\'e}rez-Espinosa, Humberto",
      "P{\\'e}rez-Mart{\\i}nez, Jos{\\'e} Mart{\\i}n",
      "Dur{\\'a}n-Reynoso, Jos{\\'e} {\\'A}ngel",
      "Reyes-Meza, Ver{\\'o}nica"
    ],
    "year": "2015",
    "journal": "Research in Computing Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885725"
  },
  {
    "id": "6280a90e-6932-43bd-ac8d-e1525f8dfc6a",
    "title": "Automatic individual dog recognition based on the acoustic properties of its barks",
    "authors": [
      "P{\\'e}rez-Espinosa, Humberto",
      "Reyes-Meza, Ver{\\'o}nica",
      "Aguilar-Benitez, Emanuel",
      "Sanz{\\'o}n-Rosas, Yuvila M"
    ],
    "year": "2018",
    "journal": "Journal of Intelligent \\& Fuzzy Systems",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885729"
  },
  {
    "id": "e9716035-95b9-4896-adfe-ebb105875677",
    "title": "Automatic classification of dog barking using deep learning",
    "authors": [
      "G{\\'o}mez-Armenta, Jos{\\'e} Ram{\\'o}n",
      "P{\\'e}rez-Espinosa, Humberto",
      "Fern{\\'a}ndez-Zepeda, Jos{\\'e} Alberto",
      "Reyes-Meza, Ver{\\'o}nica"
    ],
    "year": "2024",
    "journal": "Behavioural Processes",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885735"
  },
  {
    "id": "88e4d683-2404-42e0-902b-b5a910e32615",
    "title": "Towards dog bark decoding: Leveraging human speech processing for automated bark classification",
    "authors": [
      "Abzaliev, Artem",
      "Espinosa, Humberto P{\\'e}rez",
      "Mihalcea, Rada"
    ],
    "year": "2024",
    "journal": "arXiv preprint arXiv:2404.18739",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885739"
  },
  {
    "id": "98fdd2a7-f54a-49aa-9da7-f79abc665eaf",
    "title": "Awesome Ears: The Weird World of Insect Hearing",
    "authors": [
      "Stephanie Pain"
    ],
    "year": "2018",
    "journal": "Scientific American",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885744"
  },
  {
    "id": "3eef6cb7-6d17-45e8-b66e-0a8538312f01",
    "title": "Google Is Training a New A.I. Model to Decode Dolphin Chatter—and Potentially Talk Back",
    "authors": [
      "Sara Hashemi"
    ],
    "year": "2025",
    "journal": "Smithsonian Magazine",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885748"
  },
  {
    "id": "146725d7-9703-46f0-8782-a6a651b31ef6",
    "title": "Unsupervised sound separation using mixture invariant training",
    "authors": [
      "Wisdom, Scott",
      "Tzinis, Efthymios",
      "Erdogan, Hakan",
      "Weiss, Ron",
      "Wilson, Kevin",
      "Hershey, John"
    ],
    "year": "2020",
    "journal": "Advances in neural information processing systems",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885755"
  },
  {
    "id": "d1c5a977-9010-444b-9319-5658a20243fb",
    "title": "Improving bird classification with unsupervised sound separation",
    "authors": [
      "Denton, Tom",
      "Wisdom, Scott",
      "Hershey, John R"
    ],
    "year": "2022",
    "journal": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885760"
  },
  {
    "id": "7989ec48-4f19-4b1d-9a28-70eaff530931",
    "title": "DolphinGemma}: How {Google} {AI} is helping decode dolphin communication",
    "authors": [
      "Herzing, Denise",
      "Starner, Thad"
    ],
    "year": "2025",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885765"
  },
  {
    "id": "25d59719-bdda-43fc-9ae2-b24485ce680b",
    "title": "Vocal turn-taking in meerkat group calling sessions",
    "authors": [
      "Demartsev, Vlad",
      "Strandburg-Peshkin, Ariana",
      "Ruffner, Michaela",
      "Manser, Marta"
    ],
    "year": "2018",
    "journal": "Current Biology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885770"
  },
  {
    "id": "4f92b1b0-7202-42e1-9723-a865124ebe8c",
    "title": "Contextual and combinatorial structure in sperm whale vocalisations",
    "authors": [
      "Sharma, Pratyusha",
      "Gero, Shane",
      "Payne, Roger",
      "Gruber, David F",
      "Rus, Daniela",
      "Torralba, Antonio",
      "Andreas, Jacob"
    ],
    "year": "2024",
    "journal": "Nature Communications",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885774"
  },
  {
    "id": "db730311-67f2-4c16-bf70-84b7a6c702ab",
    "title": "Noisereduce: Domain General Noise Reduction for Time Series Signals",
    "authors": [
      "Sainburg, Tim",
      "Zorea, Asaf"
    ],
    "year": "2024",
    "journal": "arXiv preprint arXiv:2412.17851",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885778"
  },
  {
    "id": "865e8f23-7643-47de-aabe-f4f43e58919b",
    "title": "Chimpanzee alarm call production meets key criteria for intentionality",
    "authors": [
      "Schel, Anne Marijke",
      "Townsend, Simon W",
      "Machanda, Zarin",
      "Zuberb{\\\"u}hler, Klaus",
      "Slocombe, Katie E"
    ],
    "year": "2013",
    "journal": "PloS one",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885782"
  },
  {
    "id": "91a85498-d90d-4b4b-9ce1-377f26d04977",
    "title": "The separation of overlapped dolphin signature whistle based on blind source separation",
    "authors": [
      "Deng, Xiaohong",
      "Tao, Yi",
      "Tu, Xingbin",
      "Xu, Xiaomei"
    ],
    "year": "2017",
    "journal": "2017 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885786"
  },
  {
    "id": "3c695862-0c4d-4eeb-acce-bb093c35a6f8",
    "title": "A comparative study of blind source separation for bioacoustics sounds based on FastICA, PCA and NMF",
    "authors": [
      "Hassan, Norsalina",
      "Ramli, Dzati Athiar"
    ],
    "year": "2018",
    "journal": "Procedia Computer Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885821"
  },
  {
    "id": "017c15b5-64cb-484e-94dd-e7c258c1d76d",
    "title": "Separating overlapping bat calls with a bi-directional long short-term memory network",
    "authors": [
      "Zhang, Kangkang",
      "Liu, Tong",
      "Song, Shengjing",
      "Zhao, Xin",
      "Sun, Shijun",
      "Metzner, Walter",
      "Feng, Jiang",
      "Liu, Ying"
    ],
    "year": "2022",
    "journal": "Integrative zoology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885828"
  },
  {
    "id": "22fc8af1-8039-4578-85cc-bb2892dc7328",
    "title": "Separation of overlapping sources in bioacoustic mixtures",
    "authors": [
      "Izadi, Mohammad Rasool",
      "Stevenson, Robert",
      "Kloepper, Laura N"
    ],
    "year": "2020",
    "journal": "The Journal of the Acoustical Society of America",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885833"
  },
  {
    "id": "42e64b96-d4e8-4479-bc0c-9ea8483f58b0",
    "title": "Towards a great ape dictionary: Inexperienced humans understand common nonhuman ape gestures",
    "authors": [
      "Graham, Kirsty E",
      "Hobaiter, Catherine"
    ],
    "year": "2023",
    "journal": "PLoS Biology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885837"
  },
  {
    "id": "9379d5a3-f994-4acf-9086-7636ef49378f",
    "title": "Toward understanding the communication in sperm whales",
    "authors": [
      "Andreas, Jacob",
      "Begu{\\v{s}}, Ga{\\v{s}}per",
      "Bronstein, Michael M",
      "Diamant, Roee",
      "Delaney, Denley",
      "Gero, Shane",
      "Goldwasser, Shafi",
      "Gruber, David F",
      "de Haas, Sarah",
      "Malkin, Peter",
      "others"
    ],
    "year": "2022",
    "journal": "Iscience",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885842"
  },
  {
    "id": "e5b8579e-97fc-4af5-9806-141d1b28a641",
    "title": "Beans: The benchmark of animal sounds",
    "authors": [
      "Hagiwara, Masato",
      "Hoffman, Benjamin",
      "Liu, Jen-Yu",
      "Cusimano, Maddie",
      "Effenberger, Felix",
      "Zacarian, Katie"
    ],
    "year": "2023",
    "journal": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885846"
  },
  {
    "id": "3a1dc68c-9958-43b1-823a-f38cd57333e2",
    "title": "Recording mouse ultrasonic vocalizations to evaluate social communication",
    "authors": [
      "Ferhat, Allain-Thibeault",
      "Torquet, Nicolas",
      "Le Sourd, Anne-Marie",
      "De Chaumont, Fabrice",
      "Olivo-Marin, Jean-Christophe",
      "Faure, Philippe",
      "Bourgeron, Thomas",
      "Ey, Elodie"
    ],
    "year": "2016",
    "journal": "Journal of visualized experiments: JoVE",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885850"
  },
  {
    "id": "c092361d-5f54-4adc-a1e5-1dbdba1b9b6f",
    "title": "The iNaturalist sounds dataset",
    "authors": [
      "Chasmai, Mustafa",
      "Shepard, Alexander",
      "Maji, Subhransu",
      "Van Horn, Grant"
    ],
    "year": "2024",
    "journal": "Advances in Neural Information Processing Systems",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.885855"
  },
  {
    "id": "6890459a-5c60-4c7a-a9ad-aa640de8eadf",
    "title": "Automatic bird vocalization identification based on fusion of spectral pattern and texture features",
    "authors": [
      "Zhang, Sai-hua",
      "Zhao, Zhao",
      "Xu, Zhi-yong",
      "Bellisario, Kristen",
      "Pijanowski, Bryan C"
    ],
    "year": "2018",
    "journal": "2018 IEEE International Conference on acoustics, Speech and signal processing (ICASSP)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "AUTOMATIC BIRD VOCALIZATION IDENTIFICATION BASED ON FUSION OF\nSPECTRAL PATTERN AND TEXTURE FEATURES\nSai-hua Zhanga\n, Zhao Zhaoa,b,*, Zhi-yong Xu a,*, Kristen Bellisariob\n, Bryan C. Pijanowskib\na School of Electronic and Optical Engineering, Nanjing University Sci. & Technol., 210094, China\nb Department of Forestry and Natural Resources, Purdue University, West Lafayette IN47907, USA\nzhaozhao@njust.edu.cn, ezyxu@njust.edu.cn\nABSTRACT\nAutomatic bird species identification from audio field recordings is\nstudied in this paper. We first used a Gaussian mixture model\n(GMM) based energy detector to select representative acoustic\nevents. Two different feature sets consisting of spectral pattern and\ntexture features were extracted for each event. Then, a ReliefFbased feature selection algorithm was employed to select\ndistinguishing features. Finally, classification was performed using\nsupport vector machine (SVM). The main focus of the proposed\nmethod lies in the fusion of a spectral pattern feature with several\ntexture descriptors, which extends our previous work. Experiments\nused an audio dataset comprised of field recordings of 11 bird\nspecies, containing 2762 bird acoustic events and 339 detected\n“unknown” events (corresponding to noise or unknown species\nvocalizations). Experimental results demonstrate superior\nclassification performance compared with that of the state-of-theart method, which renders the proposed method more suitable for\nreal-field recording analysis.\nIndex Terms—bird species identification, spectral pattern feature,\ntexture descriptor, feature selection, support vector machine\n1. INTRODUCTION\nRecent developments of technology have already provided\nconsiderable support to biodiversity monitoring. Birds have been\nused widely as indicators of biodiversity because they are\ndistributed over a wide range of natural habitat and are relatively\neasy to detect. There is also a significant amount of existing\nknowledge on the biology of most of the species by expert\nornithologists through field observations, providing enormous\nsupport to relevant research. As a complement to traditional\nhuman-observer-based survey methods, acoustic analysis of bird\nvocalizations can be used for automated species identification,\nleading to a promising non-intrusive method for bioacoustic\nmonitoring [1, 2].\nThe first stage of acoustic bird species identification is usually\nto segment the continuous recordings into isolated acoustic events.\nSome studies involved manual segmentation [3, 4], which is\nextremely laborious and time-consuming. Recently, various\nmethods have been employed for automated segmentation [5, 6]. In\nparticular, modeling the distribution of short-term energy with a\nGaussian mixture model (GMM) is a more sophisticated acoustic\nactivity detection approach that has been widely used in noisy\nenvironments [7]. Note that bird vocalizations can be divided into\ntwo categories: calls and songs, where the former usually refer to\nisolated monosyllabic sounds and the latter contain a few syllables.\nIn this paper, an acoustic event refers to either a call or a syllable.\nAfter segmentation, each segment is represented by a set of\nfeatures that can discriminate between different classes of bird\nsounds. Then, a recognition algorithm is employed to identify the\nbird species based on the extracted features. In general, features\ndeveloped for acoustic bird species classification roughly include\ntwo categories: frame-level and event-level features [7]. Framelevel features are calculated in each frame, such as Mel-frequency\ncepstral coefficients (MFCCs), linear predictive coding coefficients\n(LPCCs), peak frequency, short-time bandwidth, as well as their\nchanges between adjacent frames [8, 9, 10]. In contrast, event-level\nfeatures focus on a whole acoustic event, rather than a single frame\nwithin it. A variety of event-level features have been investigated,\nincluding highest frequency, average or maximum bandwidth,\nduration, maximum power, and different combinations of these\nfeatures [3, 11]. Besides, some more complex descriptors have also\nbeen proposed, such as harmonic structure, spectral peak tracks,\nspectrogram ridge, and MPEG angular radial transform (ART)\ndescriptor [4, 12, 13, 14]. However, features of both categories\nwere usually investigated using datasets that only involved the\nspecies of interest. Considering that the classifier will have to\nassign some acoustic events not well suited to any existing classes\nto an unknown class when working with real-world datasets, we\nproposed a novel spectral pattern feature in our most recent study\n[7]. This parameterized feature depicts the species-specific spectral\npattern, contributing to superior robustness in real-world scenarios.\nMore recently, texture descriptors have been investigated and\nsuccessfully applied to the task of face recognition [15] and image\nretrieval [16]. It is worth remarking that fusion of different feature\nsets has been used for automatic acoustic classification of anurans\nand proved efficient in performance improvement [9]. Based on\nthese recent studies, we propose a new method by combining the\nspectral pattern feature with texture descriptors in this paper,\nextending our previous work. Note that considering the “curse of\ndimensionality” problem, it's not always appropriate to simply\nconcatenate a series of features. Thus, the ReliefF algorithm [17] is\nemployed for feature ranking and selection, helping to reduce the\nmisclassification rate and computational demands. Experimental\nresults demonstrate that a distinguishing subset of the features\nobtained by feature selection can improve performance for\nautomatic bird species identification.\n2. PROPOSED METHOD\nIn summary, for those numerous methods developed for automatic\nbird sound detection and species identification, a typical analysis\n978-1-5386-4658-8/18/$31.00 ©2018 IEEE 271 ICASSP 2018\nworkflow could contain following three steps: automated\nsegmentation, feature extraction and classification. In accordance\nwith this workflow, the overall block diagram of our method is\ndepicted in Fig. 1. The individual processing steps are described in\nthe following subsections.\n2.1. Automated segmentation\nAfter each field recording is divided into frames, the distribution of\nlog-energies of frames is modeled by a GMM of two mixtures. In\nthis model, one mixture component is fitted to the distribution of\nthe low-energy frames and the other works for the high-energy\nframes. Then, the crossing point of the two components is usually\nselected as the decision threshold [18].\nSpecifically, given the sampling frequency of 32kHz, a\nrecording is divided into frames of 320 samples with an overlap of\n160 samples between adjacent frames. Short-time Fourier\ntransform (STFT) is then implemented to each frame using a\nHamming window with length 512. Finally, the corresponding\nspectrogram S(k,l) is fed into the subsequent step with k and l the\nindices of Fourier coefficient and frame number, respectively.\nConsidering a recording with spectrogram S(k,l), the energy as\nwell as the log-energy of the l-th frame is denoted as\n ( ) 2\n( ,) H\nL\nN\nk N el Skl = = ∑ le l e l ( ) = log ( ) 10 ( ) (1)\nwhere 1≤l≤L and L is the total number of frames in the recording.\nThe frequency range of interest is constructed by NL and NH\ncorresponding to 1kHz and 16kHz, respectively. Assume that the\nlog-energy le is generated by a two-component GMM and the\ncorresponding probability density function can be written as\n ( ) ( )\n2 2\n2\n1\n; , , exp 2 2\nm m\nm m m\nw le\np le w µ\nµ σ = σ π σ\n  − =   −\n \n∑ (2)\nwhere , 1,2 w m m = are the mixing coefficients satisfying\n0 1 ≤ ≤ wm and 2\n1 1 ∑m= wm = . µ m and σ m are the mean and\nstandard deviation of the m-th Gaussian component, respectively.\nThe maximum likelihood solution for the parameter set\n{w m mmm , , , 1,2 µ σ } = can be obtained by the widely used\nexpectation-maximization (EM) algorithm. With the threshold\nchosen as the crossing point of the two Gaussian components, most\nof the promising high-energy frames are selected. Then, every\ncluster of consecutive selected frames is grouped into a single\nevent, i.e. a segment. Those events that are shorter than 20ms will\nbe discarded.\nNote that after the above GMM-based event detection step,\nthere are still some events with faintish energy that cannot be\nidentified with certainty even by ornithologists to be selected.\nTherefore, an event-energy-based post-processing procedure is\nneeded to eliminate them. To be more specific, given an initial\ncandidate events set D AE AE AE = { 1 2 , ,,  K } with K events being\ndetected, the k-th event-energy is calculated as\n\n10log10 ( ) , 1, ,\nk\nk\nl AE\nEAE e l k K ∈\n  =   =\n  ∑ \n\n(3)\nThe maximum is denoted as max k k\nME EAE = . As for the k-th\nevent, if 20 ME EAE dB − ≥k , this event will be discarded. Finally,\nthe remaining events constitute the set RD.\nAfter the event-energy-based sifting and manual inspection on\nthe output of automated segmentation, bird species events and\n“unknown” events (corresponding to noise or unknown species\nvocalizations) are obtained. Then, they are fed into the following\nfeature extraction step.\n2.2. Feature extraction\nIn this step, a feature fusion using the spectral pattern feature and\nfive texture descriptors is explored for bird species classification.\nFurthermore, a feature selection algorithm is incorporated to\nreduce feature dimension and increase accuracy.\n2.2.1. Spectral pattern feature\nThe spectral pattern feature is first proposed in our previous work\n[7]. Given an event in RD, its spectrogram is first filtered by a Melscaled filter bank containing 32 equal-height triangular band-pass\nfilters within the frequency range from 1kHz to 16kHz, leading to\n32 subband time-series x1, ..., x32. Then, an AR model is used to\ncharacterize the spectral pattern property for each subband. As for\nthe i-th subband, the AR model is\n ( ) ( ) ( ) ( ) 1\n( ) ( ) 1 Mi\ni i\ni i i ii xl xl + −+ + − = α α  xl M zl (4)\nwhere zi(l) is a zero-mean white noise excitation and the model\norder Mi is determined by the Akaike information criterion (AIC)\nwith the maximum being experimentally set to 10 in this paper.\nThe coefficients of the AR model constitute the subband feature\n 1 2 10\n() () () [ , , , ] , 1,2, ,32 i i iT\ni sp = αα α   i = (5)\nAfterwards, the corresponding spectral pattern feature vector is\ndenoted as 1 2 32 [ , ,, ] T T TT sp sp sp sp =  . It is worth noting that the\nspecies-specific feature with respect to certain acoustic event is\nconveyed by the model coefficients. This parameterization process\ncan deal with a variety of bird acoustic events with either different\ndurations or different sound unit shapes.\n2.2.2. Texture descriptors\nSince the spectrogram can be viewed as an image, it is feasible to\nuse texture descriptors to depict bird sound events. The state-ofthe-art texture descriptors [19] incorporated in this work are briefly\ndescribed as follows:\n(1) ULBP: a uniform local binary pattern (ULBP) descriptor\ndenoted as lbp. The local binary pattern (LBP) is first computed at\neach pixel, considering the binary differences between the gray\nvalues of a central pixel and those of P pixels in a circular\nneighborhood with radius R pixels. In this work, R and P were set\nField recordings database\nGMM-based event detection\nEvent-energy-based\npost-processing\nTexture descriptors\nextraction\nSVM-based classification\nSpectral pattern\nfeature extraction\nFeature fusion and\nfeature selection\nFeature extraction\nAutomated\nsegmentation\nFig. 1. Overall block diagram of the proposed method.\n272\nto 1 and 8, respectively. Compared with LBP, ULBP is an\nimproved descriptor in which the number of transitions in the\nbinary code sequence between 0 to 1 or 1 to 0 is less than or equal\nto two, producing a feature vector with length 59.\n(2) LBPHF [20]: a LBP histogram Fourier (LBPHF)\ndescriptor denoted as lbf. LBPHF is a rotation-invariant image\ndescriptor that is computed from discrete Fourier transforms (DFT)\nof LBP histograms. Given the histogram h(UP(n,r)) where UP(n,r)\nrepresents a specific uniform LBP pattern with P neighboring\nsampling points, n denotes the row number and r defines the\nrotation of the pattern, we set H n =( , )⋅ to be the DFT of the n-th\nrow of histogram. The LBPHF feature with length 38 refers to\ncorresponding magnitude spectrum.\n(3) LPQ [21]: a local phase quantization (LPQ) descriptor\ndenoted by lpq. Given a rectangular M×M (M was set to 3 in this\nwork) neighborhood Nx at each pixel position x of the image f(x),\ncorresponding 2-D DFT is first computed. Then, the DFT\ncoefficients are quantized into a two bit code: first bit for real part\nand second bit for imaginary part. This gives 8-bit code from four\nquantized coefficients. Finally, after decorrelation and quantization,\na histogram of quantized coefficients from all image positions is\ncomposed and used as a 256-dimensional feature vector.\n(4) HASC: the heterogeneous auto-similarities of\ncharacteristics (HASC) descriptor. HASC is applied to\nheterogeneous dense features maps that simultaneously encode\nlinear relations by covariances (COV) and nonlinear associations\nthrough entropy and mutual information (EMI). For a given\nrectangular patch P in a d-dimensional feature image extracted\nfrom original image (d was set to 6 in this work), containing K\npixels, the COV and EMI matrices are calculated. Then, we\nvectorize both matrices, obtaining cov and emi. Finally, HASC is\ndefined by concatenation as [, ] T TT hac cov emi = with length 42.\n(5) GF: the Gabor filter (GF) descriptor. We used 5 different\nscale levels and 8 different orientations. The mean-squared energy\nand the mean amplitude were calculated for each scale and\norientation. In this way a feature vector gf of size 80 is obtained.\n2.2.3. Feature fusion and feature selection\nAfter all the features above are extracted, a fusion stage is further\nincorporated to obtain a compact feature representation for each\nacoustic event. Specifically, for the i-th event, the concatenated\nfeature vector obtained is\n , , , , , , 1, 2, T T T T T TT\ni i i i i ii =   i K = cfv sp   lbp lbf lpq hac gf …, (6)\nwhere K is the total number of events.\nIt is worth noting that this simple concatenation leads to a\nhigh-dimensional feature vector, which will increase the\ncomputational burden and potentially cast an adverse effect on\nsubsequent classification performance. In this context, feature\nselection, aiming to choose a discernible subset of features to\nreduce the fusion feature length with the lowest information loss,\ncan be employed to remove irrelevant and/or redundant features.\nAs one of the filter based feature selection methods, the Relief\nalgorithm [22] is an effective approach to feature weight estimation.\nReliefF [17] extends two-class Relief algorithm to deal with multiclass problem. With the help of ReliefF algorithm, an attribute\nweight was calculated for each feature, ranging from -1 to 1 with a\nhigh positive weight assigned to an important attribute. Then we\nsorted out N most important features as the effective subset.\nAccording to our preliminary study, we selected N=400 according\nto the best performance for the following classification step.\n2.3. SVM-based classification\nSVM is a robust supervised learning method that has been\nextensively studied for classification and regression. SVM\nclassifiers use a hyperplane and a kernel function for nonlinear\nclassification of two class data. As for the multi-class classification,\nwe employed the “one-versus-one” strategy [23]. Besides, we used\nthe radial basis function (RBF) as the kernel function.\n3. EXPERIMENTAL EVALUATIONS\n3.1. Field recordings description\nIn order to evaluate our method, the field audio recordings used in\nthis work were downloaded from the Xeno-canto Archive\n(http://www.xeno-canto.org/) and the details can be found in [7].\nNote that these are all real-world recordings and each recording\npotentially contains vocalizations of several animal species and\ncompeting noise originating from wind, rain, or anthropogenic\ninterference. The recordings were resampled to a uniform sampling\nfrequency of 32 kHz. There are 11 bird species in the recordings\nthat can be divided into five types based on sound unit shapes,\nincluding constant frequency (CF), frequency modulated whistles\n(FM), broadband pulses (BP), broadband with varying frequency\ncomponents (BVF), and strong harmonics (SH) [1]. After manual\ninspection on the result of the automated segmentation described in\nsubsection 2.1, a total of 2762 acoustic events for 11 bird species\nand simultaneously 339 “unknown” events were available. We\nprovide the description of the dataset used in this study in Table 1.\n3.2. Experimental setup\nFor each detected event, the spectral pattern feature and 5 texture\ndescriptors were calculated and ReliefF algorithm was employed to\nselect discriminative features. The two parameters of the RBF\nkernel, gamma and cost, were set to 0.0625 and 8. The baseline\nmethod only utilized the spectral pattern feature [7]. 10 trials were\nconducted to compare the performance of the proposed method\nand the baseline one. In each trial the dataset was split randomly\ninto 60% training set and 40% testing set to acquire statistically\nrelevant results. Meanwhile, the percentage of each class was kept\nas same as 60:40.\n3.3. Performance metrics\nBoth methods were evaluated by means of three performance\nmetrics for each class including precision (P), recall (R) and Fscore which are denoted as\nTP P\nTP FP = + TP R\nTP FN = +\n2 - PR F score\nP R = +\n(7)\nwhere TP is the number of detected true positive events for each\nclass. FP and FN are the numbers of false positive and false\nnegative events for each class, respectively. After 10 trials,\naveraged metrics for each method were calculated.\nMeanwhile, the overall performance was measured in terms of\nthe classification accuracy defined as CA N N = × ( CA Te ) 100%\nwhere NCA is the number of correctly classified events and NTe is\nthe total number of testing events.\n4. RESULTS AND DISCUSSION\nIn this section, we first present the comparative results among\ndifferent features in Table 2 after 10 trials. In order to guarantee a\nfair comparison, all features were equipped with the same SVM\nclassifier. It can be observed that the worst classification accuracy\nachieved is 86.6% when only those texture descriptors were\n273\nTable 1. Details of species and corresponding field recordings\nused in this work.\nBird species Call/\nSong\nSound\nunit shape\nNumber\nof events\nBlue Jay (B-J) Call SH 251\nSong Sparrow (S-S) Call SH 259\nMarsh Wren (M-W) Call BP 249\nCommon Yellowthroat (C-YT) Call BP 256\nChipping Sparrow (C-S) Call FM 253\nAmerican Yellow Warbler (A-Y-W) Call FM 247\nGreat Blue Heron (G-B-H) Call BVF 247\nAmerican Crow (A-C) Call BVF 253\nCedar Waxwing (C-WW) Call CF 246\nHouse Finch (H-F) Song SH 249\nIndigo Bunting (I-BT) Song FM 252\nTable 2. Comparison of various features in terms of classification\naccuracy (CA) .\nFeatures Dimension of\nfeature vector\nCA\n(%)\nSP (baseline method) 320 93.7\nHASC+LBP+LPQ+LBPHF+GF 475 86.6\nSP+HASC+LBP+LPQ+LBPHF+GF\nwithout feature selection 795 94.0\nSP+HASC+LBP+LPQ+LBPHF+GF\nwith feature selection (this work) 400 96.7\ncombined. At the same time, fusion of spectral pattern feature with\ntexture descriptors without feature selection achieved similar\nperformance to that using only spectral pattern feature (baseline\nmethod) in terms of classification accuracy. However, the fusion of\nspectral pattern feature with texture descriptors plus ReliefF-based\nfeature selection (this work) clearly outperformed the methods\nabove, approximately 2.7% higher than the method without feature\nselection. This demonstrates that feature selection algorithm can\nsort out more robust and discriminating feature subset, which has a\ndimension of 400, only 50.3% of the original concatenated feature\ndefined in Eq. (6).\nIn order to further investigate and compare the performance\nbetween the proposed method and baseline one, the averaged\nperformance metrics of the two methods for each class are\nprovided in Table 3. One should note that the performance metrics\nof the proposed method are almost all greater than those of the\nbaseline method, except the precision of C-WW and Unknown\nclass, which merely decreased by 0.3% and 0.4%. It is worth\nremarking that a simple comparison on these metrics, however, is\nnot completely reliable since the winning algorithm may\noccasionally perform well due to the randomness in data split.\nHypothesis test is usually employed for statistically reliable\ncomparison between algorithms.\nHere, we used Mann-Whitney test [24], which aims to assess\nthe statistical significance of the differences between two groups.\nThe Mann­Whitney test is the non-parametric equivalent of the\nindependent samples t test. For both the proposed method and the\nbaseline one, we calculated the classification accuracy values for\n10 trials. Then, the Mann-Whitney test was employed to compare\nthe statistical difference of the two methods. The significance level\nα was set to 0.05 and the corresponding estimate value\nTable 3. Averaged precision and recall as well as the\ncorresponding F-score for each species between the proposed\nmethod with the baseline method.\nRecall (%) Precision (%) F-score\nClasses This\nwork\nBaseline\nmethod\nThis\nwork\nBaseline\nmethod\nThis\nwork\nBaseline\nmethod\nB-J 98.7 97.9 99.0 97.9 0.989 0.979\nS-S 97.0 92.6 95.4 88.4 0.962 0.904\nM-W 96.8 90.1 97.5 94.5 0.971 0.922\nC-YT 95.6 91.0 95.1 89.6 0.953 0.903\nC-S 97.7 93.8 97.0 94.2 0.973 0.940\nA-Y-W 97.7 93.8 99.0 90.7 0.983 0.922\nG-B-H 96.3 91.4 98.1 94.8 0.972 0.930\nA-C 99.2 97.9 99.5 96.7 0.994 0.973\nC-WW 96.8 95.6 98.7 99.0 0.977 0.973\nH-F 95.1 94.7 95.3 92.4 0.951 0.935\nI-BT 95.7 94.0 94.5 93.3 0.950 0.936\nUnknown 95.2 91.7 94.0 94.4 0.945 0.930\n4 p 1.55 10 0.05 − =×< , suggesting that there is significant\ndifference between the two methods. Most recent study in\ncognitive science confirmed that spectral shape is the primary cue\nto bird sounds recognition [25, 26]. Therefore, in our previous\nstudy [7], the spectral pattern feature was proposed to describe the\nspectral shape information of bird vocalizations, achieving\ncomparable identification performance with respect to the species\nof interest and superior robustness in real-world scenarios when\ncompared with other recent approaches. In this work, considering\nthe results of classification accuracy (Table 2), performance\nmetrics for each class (Table 3) and the hypothesis test, we can\nconclude that the proposed method provides comparable\nrobustness in real-field environments as well as superior\nidentification performance regarding the species of interest—that is,\nthe proposed approach outperforms the baseline method. This\nimprovement can be attributed to the fact that the feature subset\nobtained by feature fusion and feature selection can depict the\nspecies-specific spectral shape information of the time-frequency\ndistribution in each sub-band, as well as the spatial variation and\narrangement of orientations information in the full-band.\n5. CONCLUSIONS\nAiming to improve the audio parameterization process in bird\nspecies identification tasks, we proposed an automatic acoustic\nclassification method based on feature fusion in this work. After\nGMM-based acoustic event detection and event-energy-based postprocessing procedure, representative acoustic events were selected.\nFor each event, two different feature sets, the spectral pattern\nfeature and texture descriptors, were extracted. As for the\ncombination of the two sets, ReliefF-based feature selection\nalgorithm was employed to select a distinguishing feature subset.\nExperimental results using real-world recordings showed that the\nproposed method outperformed the state-of-the-art robust approach.\nThe improved performance makes the proposed method more\neffective for the application of acoustic monitoring in terrestrial\nenvironments.\n6. ACKNOWLEDGMENT\nThis work was supported by the National Natural Science\nFoundations of P.R. China under Grants No. 61401203 and No.\n61171167; and the State Scholarship Fund of China [grant number\n201606840023].\n274\n7. REFERENCES\n[1] T.S. Brandes, “Automated sound recording and analysis\ntechniques for bird surveys and conservation,” Bird\nConservation International, vol. 18, no. S1, pp. S163-S173,\n2008.\n[2] K. Kaewtip, L.N. Tan, A. Alwan and C.E. Taylor, “A robust\nautomatic bird phrase classifier using dynamic time-warping\nwith prominent region identification,” IEEE International\nConference on Acoustics, Speech, and Signal Processing\n(ICASSP), Vancouver, Canada, pp. 768-772, 2013.\n[3] M.A. Acevedo, C.J. Corrada-Bravo, H. Corrada-Bravo, L.J.\nVillanueva-Rivera and T.M. Aide, “Automated classification\nof bird and amphibian calls using machine learning: a\ncomparison of methods,” Ecological Informatics, vol. 4, no. 4,\npp. 206-214, 2009.\n[4] C.H. Lee, S.B. Hsu, J.L. Shi and C.H. Chou, “Continuous\nbirdsong recognition using Gaussian mixture modeling of\nimage shape features,” IEEE Transactions on Multimedia, vol.\n15, no. 2, pp. 454-464, 2012.\n[5] L. Neal, F. Briggs, R. Raich, and X.Z. Fern, “Time-frequency\nsegmentation of bird song in noisy acoustic environments,”\nIEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP), Prague, Czech Republic, pp.\n2012-2015, 2011.\n[6] A.G. Oliveira, T.M. Ventura, T.D. Ganchev, J.M. Figueiredo,\nO. Jahn, M.I. Marques and K.L. Schuchmann. “Bird acoustic\nactivity detection based on morphological filtering of the\nspectrogram,” Applied Acoustics, vol. 98, pp. 34-42, 2015.\n[7] Z. Zhao, S.H. Zhang, Z.Y. Xu and K. Bellisario, N.H. Dai, H.\nOmrani and B.C. Pijanowski, “Automated bird acoustic event\ndetection and robust species classification,” Ecological\nInformatics, vol. 39, pp. 99-108, 2017.\n[8] T.S. Brandes, “Feature vector selection and use with hidden\nMarkov models to identify frequency-modulated bioacoustic\nsignals amidst noise,” IEEE Transactions on Audio, Speech,\nand Language Processing, vol. 16, no. 6, pp. 1173-1180,\n2008.\n[9] J.J. Noda, C.M. Travieso, and D. Sánchez-Rodríguez,\n“Methodology for automatic bioacoustic classification of\nanurans based on feature fusion,” Expert Systems with\nApplications, vol. 50, pp. 100-106, 2016.\n[10] I. Potamitis, S. Ntalampiras, O. Jahn and K. Riede,\n“Automatic bird sound detection in long real-field recordings:\nApplications and tools,” Applied Acoustics, vol. 80, pp. 1-9,\n2014.\n[11] T. Schrama, M. Poot, M. Robb and H. Slabbekoorn,\n“Automated monitoring of avian flight calls during nocturnal\nmigration,” International Expert Meeting on IT-based\nDetection of Bioacoustical Patterns, pp. 131-134, 2007.\n[12] P. Jančovič and M. Köküer, “Acoustic Recognition of\nMultiple Bird Species Based on Penalized Maximum\nLikelihood,” IEEE Signal Processing Letters, vol. 22, no. 10,\npp. 1585-1589, 2015.\n[13] A. Harma and P. Somervuo, “Classification of the harmonic\nstructure in bird vocalization,” IEEE International\nConference on Acoustics, Speech, and Signal Processing\n(ICASSP), Montreal, Canada, pp. 701-704, 2004.\n[14] X.Y Dong, M. Towsey, J.L. Zhang, J. Banks and P. Roe, “A\nnovel representation of bioacoustic events for content-based\nsearch in field audio data,” IEEE International Conference on\nDigital Image Computing: Techniques and Applications\n(DICTA), pp. 1-6, 2013.\n[15] A. Lumini, L. Nanni and S. Brahnam, “Ensemble of texture\ndescriptors and classifiers for face recognition,” Applied\nComputing and Informatics, vol. 13, no. 1, pp. 79-91, 2017.\n[16] B. Jyothi, Y. MadhaveeLatha and P.G.K. Mohan, “Region\nbased texture descriptor for content based medical image\nretrieval using second order moments,” IEEE 2nd\nInternational Conference on Innovations in Information\nEmbedded and Communication Systems (ICIIECS), pp. 1-4,\n2015.\n[17] M. Robnik-Šikonja and I. Kononenko, “Theoretical and\nempirical analysis of ReliefF and RReliefF,” Machine\nLearning, vol. 53, no. 1-2, pp. 23-69, 2003.\n[18] M. Sahidullah and G. Saha, “Comparison of speech activity\ndetection techniques for speaker recognition,” Journal of\nImmunotherapy, vol. 33, no. 33, pp. 609-617, 2012.\n[19] L. Nanni, Y.M.G. Costa, D.R. Lucio, C.N. Silla Jr, and S.\nBrahnam, “Combining visual and acoustic features for audio\nclassification tasks,” Pattern Recognition Letters, vol. 88, pp.\n49-56, 2017.\n[20] G.Y. Zhao, T. Ahonen, J. Matas and M. Pietikäinen,\n“Rotation-invariant image and video description with local\nbinary pattern features,” IEEE Transactions on Image\nProcessing, vol. 21, no. 4, pp. 1465-1477, 2012.\n[21] V. Ojansivu and J. Heikkilä, “Blur insensitive texture\nclassification using local phase quantization,” International\nConference on Image and Signal Processing (ICISP), pp.\n236-243, 2008.\n[22] I. Kononenko, “Estimating attributes: analysis and extensions\nof RELIEF,” European Conference on Machine Learning,\nSpringer, Berlin, pp. 171-182, 1994.\n[23] C.W. Hsu and C.J. Lin, “A comparison of methods for\nmulticlass support vector machines,” IEEE Transactions on\nNeural Networks, vol. 13, no. 2, pp. 415-425, 2002.\n[24] A. Hart, “Mann-Whitney test is not just a test of medians:\ndifferences in spread can be important,” British Medical\nJournal, vol. 323, no. 7309, pp. 391-393, 2001.\n[25] M.R. Bregman, A.D. Patel and T.Q. Gentner, “Songbirds use\nspectral shape, not pitch, for sound pattern recognition,”\nProceedings of the National Academy of Sciences, vol. 113,\nno. 6, pp. 1666-1671, 2016.\n[26] R.V. Shannon, “Is birdsong more like speech or music?,”\nTrends in Cognitive Sciences, vol. 20, no. 4, pp. 245-247,\n2016.\n275",
    "affiliations": [
      {
        "country": "China",
        "discipline": "Other",
        "university": "Nanjing University Sci. & Technol."
      },
      {
        "country": "United States",
        "discipline": "Biology",
        "university": "Purdue University"
      }
    ],
    "species_categories": [
      "Bird"
    ],
    "specialized_species": [
      "Blue Jay",
      "Song Sparrow",
      "Marsh Wren",
      "Common Yellowthroat",
      "Chipping Sparrow",
      "American Yellow Warbler",
      "Great Blue Heron",
      "American Crow",
      "Cedar Waxwing",
      "House Finch",
      "Indigo Bunting"
    ],
    "computational_stages": [
      "Data Collection",
      "Pre-processing",
      "Sequence Representation",
      "Meaning Identification"
    ],
    "linguistic_features": [
      "Discreteness and Syntax",
      "Semanticity"
    ],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886024",
    "updated_at": "2026-02-10T20:16:27.999185"
  },
  {
    "id": "fd3b6fa3-4f1c-4600-802c-20fb54a568fa",
    "title": "Recognizing Bird Species in Audio Recordings using Deep Convolutional Neural Networks.",
    "authors": [
      "Piczak, Karol J"
    ],
    "year": "2016",
    "journal": "CLEF (working notes)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "Recognizing bird species in audio recordings\nusing deep convolutional neural networks\nKarol J. Piczak\nInstitute of Electronic Systems, Warsaw University of Technology\nK.Piczak@stud.elka.pw.edu.pl\nAbstract. This paper summarizes a method for purely audio-based\nbird species recognition through the application of convolutional neural\nnetworks. The approach is evaluated in the context of the LifeCLEF\n2016 bird identification task - an open challenge conducted on a dataset\ncontaining 34 128 audio recordings representing 999 bird species from\nSouth America. Three different network architectures and a simple ensemble model are considered for this task, with the ensemble submission\nachieving a mean average precision of 41.2% (official score) and 52.9%\nfor foreground species.\nKeywords: bird species identification, convolutional neural networks,\naudio classification, BirdCLEF 2016\n1 Introduction\nReliable systems that would allow for large-scale bird species recognition from\naudio recordings could become a very valuable tool for researchers and governmental agencies interested in ecosystem monitoring and biodiversity preservation.\nIn contrast to field observations made by expert and hobbyist ornithologists,\nautomated networks of acoustic sensors [1–4] are not limited by environmental\nand physiological factors, tirelessly delivering vast amounts of data far surpassing\nhuman resources available for manual analysis.\nOver the years, there have been numerous efforts to develop and evaluate\nmethods of automatic bird species recognition based on auditory data [5]. Unfortunately, with more than 500 species in the EU itself [6] and over 10 000\nworldwide [7], most experiments and competitions in this area seemed rather\nlimited when compared to the scope of real-world problems. The NIPS 2013\nmulti-label bird species classification challenge [8] encompassed 87 sound classes,\nwhereas the ICML 2013 [9] and MLSP 2013 [10] counterparts were even more\nconstrained (35 and 19 species respectively).\nThe annual BirdCLEF challenge [11], part of the LifeCLEF lab [12] organized\nby the Conference and Labs of the Evaluation Forum, vastly expanded on this topic\nby evaluating competing approaches on a real-world sized dataset comprising\naudio recordings of 501 (BirdCLEF 2014 ) and 999 bird species from South\nAmerica (BirdCLEF 2015-2016 ). The richness of this dataset, built from fieldrecordings gathered through the Xeno-canto project [13], provides a benchmark\nwhich is much closer to actual practical applications.\nPast BirdCLEF submissions have evaluated a plethora of techniques based\non statistical features and template matching [14, 15], mel-frequency cepstral\ncoefficients (MFCC ) [16, 17] and spectral features [18], unsupervised feature\nlearning [19–21], as well as deep neural networks with MFCC features [22]. However, to the best of the author’s knowledge, neural networks with convolutional\narchitectures have not yet been applied in the context of bird species identification, apart from visual recognition tasks [23]. Therefore, the goal of this work is\nto verify whether an approach utilizing deep convolutional neural networks for\nclassification could be suitable for analyzing audio recordings of singing birds.\n2 Bird identification with deep convolutional neural\nnetworks\n2.1 Data pre-processing\nThe BirdCLEF 2016 dataset consists of three parts. In the training set, there are\n24 607 audio recordings with a duration varying between less than a second and\nup to 45 minutes. The training set was annotated with a single encoded label\nfor the main species and potentially with a less uniform list of additional species\nwhich are most prominently present in the background. The main part of the\nevaluation set has been left unchanged when compared to BirdCLEF 2015 - 8 596\ntest recordings (1 second to 11 minutes each) of a dominant species with others\nin the background. The new part of the 2016 challenge comprises 925 soundscape\nrecordings (MP3 files, mostly 10 minutes long) that are not targeting a specific\ndominant species and may contain an arbitrary number of singing birds.\nThe approach presented in this paper concentrated solely on evaluating singlelabel classifiers suitable for recognition of the foreground (main) species present in\nthe recording. At the beginning, all recordings were converted to a unified WAV\nformat (44 100 Hz, 16 bit, mono) from which mel-scaled power spectrograms were\ncomputed using the librosa [24] package with FFT window length of 2048 frames,\nhop length of 512, 200 mel bands (HTK formula) with a max frequency cap at\n16 kHz. Perceptual weighting using peak power as reference was performed on all\nspectrograms. Subsequently, all spectrograms were processed and normalized with\nsome simple scaling and thresholding to enhance foreground elements. 25 lowest\nand 5 highest bands were discarded. Additionally, total variation denoising was\napplied with a weight of 0.1 to achieve further smoothing of the spectrograms\n(the implementation of Chambolle’s algorithm [25] provided by scikit-image [26]\nwas used for this purpose). An example of the results of this processing pipeline\ncan be seen in Figure 1.\n80% of training recordings were randomly chosen for network learning, while\n20% of the dataset was set aside for local validation purposes. Each recording was\nthen split into shorter segments with percentile thresholding in order to discard\nsilent parts. As a final outcome of this process, 85 712 segments of varying length0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0\n0\n50\n100\n150\nLIFECLEF2014_BIRDAMAZON_XC_WAV_RN1.wav / ruficapillus\n80\n70\n60\n50\n40\n30\n20\n100.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0\n0\n50\n100\n150\nLIFECLEF2014_BIRDAMAZON_XC_WAV_RN1.wav / ruficapillus\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.00.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0\n0\n50\n100\n150\nLIFECLEF2014_BIRDAMAZON_XC_WAV_RN10043.wav / longipennis\n80\n70\n60\n50\n40\n30\n20\n10\n00.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0\n0\n50\n100\n150\nLIFECLEF2014_BIRDAMAZON_XC_WAV_RN10043.wav / longipennis\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.00.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0\n0\n50\n100\n150\nLIFECLEF2014_BIRDAMAZON_XC_WAV_RN10060.wav / delalandi\n80\n70\n60\n50\n40\n30\n20\n10\n00.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0\n0\n50\n100\n150\nLIFECLEF2014_BIRDAMAZON_XC_WAV_RN10060.wav / delalandi\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Fig. 1: Raw and processed spectrograms\nwere created for training - each labeled with a single target species. In order to\naccommodate a fixed input size expectation of most network architectures, all the\nsegments were adjusted on-the-fly during training by either trimming or padding\nso as to achieve a desired segment length of 430 frames (5 seconds). This also\nallowed for some significant data augmentation - shorter segments being inserted\nwith a random offset and padded with -1 values, while longer segments trimmed\nat random points to get a 5-second-long excerpt. Finally, the input vectors were\nstandardized.\n2.2 Network architectures\nNumerous convolutional architectures loosely based on the author’s previous\nwork in environmental sound classification [27] were evaluated, with 3 models\nbeing chosen for final submissions (schematically compared in Table 1). All the\nmodels were implemented using the Keras Deep Learning library [28]. Each\narchitecture processed input segments of spectrograms (170 bands × 430 frames)\ninto a softmax output of 999 units (one-hot encoding all the target species in the\ndataset) providing a probability prediction of the dominant species present in\nthe analyzed segment. Final prediction for a given audio recording was computed\nby averaging the decisions made across all segments of a single file. The multilabel character of the evaluation data was simplistically addressed in the final\nsubmission by providing a ranked list of the most probable dominant species\nencountered for each file, thresholded at a probability of 1%.Table 1: Architectures of the evaluated networks\nRun 1 Run 2 Run 3\nDROP, 0.05 DROP, 0.05\nCONV-600, 170 × 5 CONV-80, 167 × 6 CONV-320, 167 × 10\nLReLU LReLU LReLU\nM-P, 1 × 426 M-P, 4 × 6 (1 × 3) M-P, 4 × 10 (1 × 5)\nDROP, 0.3 CONV-160, 1 × 2 DROP, 0.05\nFC, 3000 LReLU CONV-640, 1 × 2\nPReLU M-P, 1 × 2 (1 × 2) LReLU\nDROP, 0.3 CONV-240, 1 × 2 M-P, 1 × 2 (1 × 2)\nSOFTMAX, 999 LReLU DROP, 0.05\nM-P, 1 × 2 (1 × 2) CONV-960, 1 × 2\nCONV-320, 1 × 2 LReLU\nLReLU M-P, 1 × 2 (1 × 2)\nM-P, 1 × 2 (1 × 2) DROP, 0.05\nDROP, 0.5 CONV-1280, 1 × 2\nSOFTMAX, 999 LReLU\nM-P, 1 × 2 (1 × 2)\nDROP, 0.25\nSOFTMAX, 999\nDROP - dropout, CONV-N - convolutional layer with N filters of given size, LReLU -\nLeaky Rectified Linear Units, M-P - max-pooling with pooling size (and stride size),\nFC - fully connected layer, PReLU - Parametric Rectified Linear Units, SOFTMAX -\noutput softmax layer\nRun 1 - Submission-14.txt\nThis model was inspired by recent work of Phan et al. [29] which considered\nshallow architectures with 1-Max pooling. The main idea here is to use a single\nconvolutional layer with numerous filters that would allow learning specialized\ntemplates of sound events, and then to use their maximum activation value\nthroughout the whole time span of the recording.\nThe actual model consists of a single convolutional layer of 600 rectangular\nfilters (170 × 5) with LeakyReLUs (rectifier activation with a small non-active\ngradient, α = 0.3) and dropout probability of 5%. The activation values are then\n1-max pooled (pooling size of 1 × 426) into a chain of 600 single scalar valuesrepresenting the maximum activation of each learned filter over the entire input\nsegment. Further processing is achieved through a fully connected layer of 3 000\nunits with dropout probability of 30% and Parametric ReLU [30] activations. The\noutput softmax layer (999 fully connected units) also has a dropout probability\nof 30%. All layer weights are initialized with a uniform scaled distribution [30]\n(denoted in Keras by he uniform) with biases of the initial layer set to 1.\nRun 2 - Submission-6.txt\nThis submission was based on a model with 4 convolutional layers and some\nsmall regularization:\n– Convolutional layer of 80 filters (167 × 6) with L1 regularization of 0.001 and\nLeakyReLU (α = 0.3) activation,\n– Max-pooling layer with 4 × 6 pooling size and stride size of 1 × 3,\n– Convolutional layer of 160 filters (1 × 2) with L2 regularization of 0.001 and\nLeakyReLU (α = 0.3) activation,\n– Max-pooling layer with 1 × 2 pooling size and stride size of 1 × 2,\n– Convolutional layer of 240 filters (1 × 2) with L2 regularization of 0.001 and\nLeakyReLU (α = 0.3) activation,\n– Max-pooling layer with 1 × 2 pooling size and stride size of 1 × 2,\n– Convolutional layer of 320 filters (1 × 2) with L2 regularization of 0.001 and\nLeakyReLU (α = 0.3) activation,\n– Max-pooling layer with 1 × 2 pooling size and stride size of 1 × 2,\n– Output softmax layer (999 units) with dropout probability of 50% and\nL2 regularization of 0.001.\nWeight initializations are performed in the same manner as already described. The\nsmaller vertical size of filters in the first layer allows for some minor invariance\nin the frequency domain. No further dense (fully connected) layers are utilized\nbetween the output layer and the last convolutional layer.\nRun 3 - Submission-9.txt\nThis run was also performed by a model with 4 convolutional layers, same\ninitialization technique, however the size of the filters learned is considerably\nwider, thus more filters are utilized in each layer:\n– Convolutional layer of 320 filters (167×10) with dropout of 5% and LeakyReLU\n(α = 0.3) activation,\n– Max-pooling layer with 4 × 10 pooling size and stride size of 1 × 5,\n– Convolutional layer of 640 filters (1 × 2) with dropout of 5% and LeakyReLU\n(α = 0.3) activation,\n– Max-pooling layer with 1 × 2 pooling size and stride size of 1 × 2,\n– Convolutional layer of 960 filters (1 × 2) with dropout of 5% and LeakyReLU\n(α = 0.3) activation,– Max-pooling layer with 1 × 2 pooling size and stride size of 1 × 2,\n– Convolutional layer of 1280 filters (1 × 2) with dropout of 5% and LeakyReLU\n(α = 0.3) activation,\n– Max-pooling layer with 1 × 2 pooling size and stride size of 1 × 2,\n– Output softmax layer (999 units) with dropout probability of 25%.\nRun 4 - Submission-ensemble.txt\nThe final run consisted of a simple meta-model averaging the predictions of the\naforementioned submissions.\n2.3 Training procedure\nAll network models were trained using a categorical cross-entropy loss function\nwith a stochastic gradient descent optimizer (learning rate of 0.001, Nesterov\nmomentum of 0.9). Training batches contained 100 segments each. Validation was\nperformed locally on the hold-out set (20% of the original training data available)\nby selecting a random subset on each epoch (approximately 2 500 files each time)\nand calculating the model’s prediction accuracy. This metric was assumed as\na proxy for the expected mean average precision without background species -\ncategory which was reported as M AP2 in BirdCLEF 2015 results.\nEach model was trained for a number of epochs (30–102). The training time\nfor a single model on a single GTX 980 Ti card was in the range of 30–60 hours.\nThe results of final validation for each of the trained models are presented in\nTable 2, whereas Figure 2 depicts a small selection of filters learned by one of\nthe models.\nTable 2: Local validation results for each run\nRun 1 Run 2 Run 3\nM AP2 proxy 45.1% 50.0% 49.5%\nFig. 2: Example of filters learned in the first convolutional layer3 Submission results & discussion\nThe official results of the BirdCLEF 2016 challenge are presented in Table 3\nand Figure 3. There were 6 participating groups which submitted 18 runs in\ntotal. The submission described in this work resulted in a 3rd place among\nparticipating teams with individual runs achieving 6th, 8th, 9th and 10th official\nscore (1st column - MAP with background species and soundscape files). The\nanalysis of these results and the experience gathered during the BirdCLEF 2016\nchallenge allows for the following remarks:\n– With almost 1 000 bird species, the BirdCLEF dataset creates a demanding\nchallenge for any machine audition system. In this context, an approach\nbased on convolutional neural networks seems to be valid and promising\nfor the analysis of bioacoustical data. Looking at comparable results from\nthe very last year, surpassing a foreground only MAP of 50% is definitely\na success. However, this year’s top performing submission was still able to\nremarkably improve on this evaluation metric.\n– The performance of the described networks is quite consistent between models.\nIt seems that a decent convolutional architecture with proper training and\ninitialization regime should be able to learn a reasonable approximation of\nthe classifying function based on the provided data, and minor architectural\ndecisions may not be of the utmost importance in this case.\n– Very poor performance in the soundscape category confirms that the presented\napproach has a strong bias against multi-label scenarios - a thing which is not\nsurprising when considering the applied learning scheme, which was rather\nforcefully extended to the multi-label case. Not only does learning on a single\ntarget label for each recording impose some constraints in this process, but\nthe whole pre-processing step may also be detrimental in this situation. Thus\nit seems that further work should concentrate more on what is learned (data\nsegmentation and pre-processing, labeling, input/output layers) than how\n(internal network architecture).\n– A promising feature of the dataset lies in the good correspondence between\nresults obtained through local validation and evaluation of the private ground\ntruth by the organizers. This means that the dataset is both rich and uniform\nenough for such estimations to be of value - an aspect which should help in\nfurther efforts in improving the described solution.\n– A very simple ensembling method was quite beneficial in the case of the\nevaluated models. This shows that more sophisticated approaches could yield\nsome additional gains - both when it comes to meta-model blending and\nin-model averaging. A progressive increase of the dropout rate was one of the\nfacets which was actually considered during the experiments. Unfortunately,\nthese attempts had to be preemptively stopped due to the time constraints\nencountered in the final stage of the competition.Conclusion\nThe top results achieved this year in the foreground category of the BirdCLEF\nchallenge are very promising - a MAP of almost 70% with 1 000 species is definitely\nsomething which could be called an expert system. The presented method based\non convolutional neural networks has a slightly weaker, yet still very decent\nperformance of 52.9%, warranting further investigation of this approach.\nAt the same time, the performance of all teams in the soundscape category\nis not overwhelming, to say the least. This raises some interesting questions:\nIs this kind of problem so hard and conceptually different that it would require\na completely overhauled approach? Considering that uniform ground-truth labeling is much harder in this case, what is the impact of this aspect on the whole\nevaluation process?\nOne thing is certain though - there is still a lot of room for improvement, and\ndespite a constant stream of enhancements presented by new submissions, the\nbar is set even higher in every consecutive BirdCLEF challenge.\nAcknowledgments\nI would like to thank the organizers of BirdCLEF and the Xeno-canto Foundation\nfor an interesting challenge and a remarkable collection of publicly available audio\nrecordings of singing birds.Cube 4\nCube 3\nCube 2\nMarioTB 1\nMarioTB 4\nMarioTB 3\nBME TMIT 2\nBME TMIT 3\nMarioTB 2\nBME TMIT 4\nBME TMIT 1\nCube 1\nDYNI LSIS 1\nBIG 1\nBirdCLEF 2015\nSubmission\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nMAP\nWUT 4\nWUT 2\nWUT 3\nWUT 1\nwith background\nforeground only\nsoundscapes only\nFig. 3: BirdCLEF 2016 resultsTable 3: Results of BirdCLEF 2016 submissions\nTeam Run MAP\nwith background foreground only soundscapes only\nCube 4 0.555 0.686 0.072\nCube 3 0.536 0.660 0.078\nCube 2 0.522 0.649 0.066\nMarioTsaBerlin 1 0.519 0.585 0.137\nMarioTsaBerlin 4 0.472 0.551 0.129\nWUT 4 0.412 0.529 0.036\nMarioTsaBerlin 3 0.396 0.456 0.130\nWUT 2 0.376 0.483 0.032\nWUT 3 0.352 0.455 0.029\nWUT 1 0.350 0.453 0.027\nBME TMIT 2 0.338 0.426 0.053\nBME TMIT 3 0.337 0.426 0.059\nMarioTsaBerlin 2 0.336 0.399 0.000\nBME TMIT 4 0.335 0.424 0.053\nBME TMIT 1 0.323 0.407 0.054\nCube 1 0.284 0.364 0.020\nDYNI LSIS 1 0.149 0.183 0.037\nBIG 1 0.021 0.021 0.004\nBirdCLEF 2015 - 1st - - 0.454 -\nReferences\n1. Cai, J. et al.: Sensor network for the monitoring of ecosystem: Bird species recognition. Proceedings of the 3rd IEEE International Conference on Intelligent Sensors,\nSensor Networks and Information. IEEE, 2007.\n2. Mporas, I. et al.: Integration of temporal contextual information for robust acoustic\nrecognition of bird species from real-field data. International Journal of Intelligent\nSystems and Applications, 5 (7), 9–15, 2013.\n3. Wimmer, J. et al.: Sampling environmental acoustic recordings to determine bird\nspecies richness. Ecological Applications, 23 (6), 1419–1428, 2013.\n4. BirdVox. https://wp.nyu.edu/birdvox/ (accessed 24/05/2016).\n5. Stowell, D. and Plumbley, M. D.: Birdsong and C4DM: A survey of UK birdsong\nand machine recognition for music researchers. Centre for Digital Music, Queen\nMary University of London, Technical report C4DM-TR-09-12, 2011.\n6. http://ec.europa.eu/environment/nature/legislation/birdsdirective/\n(accessed 24/05/2016).\n7. IOC World Bird List. http://www.worldbirdnames.org/ (accessed 30/06/2016).\n8. Glotin, H. et al.: Proceedings of Neural Information Processing Scaled for Bioacoustics. NIPS, 2013.\n9. Glotin, H. et al.: Proceedings of the first workshop on Machine Learning for\nBioacoustics. ICML, 2013.10. Briggs, F. et al.: The 9th annual MLSP competition: New methods for acoustic classification of multiple simultaneous bird species in a noisy environment. Proceedings\nof the IEEE International Workshop on Machine Learning for Signal Processing\n(MLSP), IEEE, 2013.\n11. Go ̈eau, H. et al.: LifeCLEF bird identification task 2016. CLEF working notes 2016.\n12. Joly, A. et al.: LifeCLEF 2016: multimedia life species identification challenges.\nProceedings of CLEF 2016.\n13. Xeno-canto project. http://www.xeno-canto.org (accessed 24/05/2016).\n14. Lasseck, M.: Improved automatic bird identification through decision tree based\nfeature selection and bagging. CLEF working notes 2015.\n15. Lasseck, M.: Large-scale identification of birds in audio recordings. CLEF working\nnotes 2014.\n16. Joly, A., Leveau, V., Champ, J. and Buisson, O.: Shared nearest neighbors match kernel\nfor bird songs identification - LifeCLEF 2015 challenge. CLEF working notes 2015.\n17. Joly, A., Champ, J. and Buisson, O.: Instance-based bird species identification with\nundiscriminant features pruning. CLEF working notes 2014.\n18. Ren, L. Y., Dennis, J. W. and Dat, T. H.: Bird classification using ensemble\nclassifiers. CLEF working notes 2014.\n19. Stowell, D.: BirdCLEF 2015 submission: Unsupervised feature learning from audio.\nCLEF working notes 2015.\n20. Stowell, D. and Plumbley, M. D.: Audio-only bird classification using unsupervised\nfeature learning. CLEF working notes 2014.\n21. Stowell, D. and Plumbley, M. D.: Automatic large-scale classification of bird sounds\nis strongly improved by unsupervised feature learning. PeerJ 2:e488, 2014.\n22. Koops, H. V., Van Balen, J. and Wiering, F.: A deep neural network approach to\nthe LifeCLEF 2014 bird task. CLEF working notes 2014.\n23. Branson, S., Van Horn, G., Belongie, S. and Perona, P.: Bird species categorization\nusing pose normalized deep convolutional nets. arXiv preprint arXiv:1406.2952, 2014.\n24. McFee, B. et al.: librosa: 0.4.1. Zenodo. 10.5281/zenodo.32193, 2015.\n25. Chambolle, A.: An algorithm for total variation minimization and applications.\nJournal of Mathematical Imaging and Vision, 20 (1-2), 89–97, 2004.\n26. van der Walt, S. et al.: scikit-image: Image processing in Python. PeerJ 2:e453, 2014.\n27. Piczak, K. J.: Environmental sound classification with convolutional neural networks.\nProceedings of the IEEE International Workshop on Machine Learning for Signal\nProcessing (MLSP), IEEE, 2015.\n28. Chollet, F.: Keras. https://github.com/fchollet/keras (accessed 24/05/2016).\n29. Phan, H., Hertel, L., Maass, M. and Mertins, A.: Robust audio event recognition with\n1-Max pooling convolutional neural networks. arXiv preprint arXiv:1604.06338, 2016.\n30. He, K., Zhang, X., Ren, S. and Sun, J.: Delving deep into rectifiers: Surpassing\nhuman-level performance on ImageNet classification. Proceedings of the IEEE\nInternational Conference on Computer Vision, IEEE, 2015.",
    "affiliations": [
      {
        "country": "Poland",
        "discipline": "Other",
        "university": "Warsaw University of Technology"
      }
    ],
    "species_categories": [
      "Bird"
    ],
    "specialized_species": [],
    "computational_stages": [
      "Data Collection",
      "Pre-processing",
      "Meaning Identification"
    ],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886028",
    "updated_at": "2026-02-10T20:16:32.003218"
  },
  {
    "id": "b4a63c90-6e01-4d79-9b56-f4d5e88927f0",
    "title": "An incremental technique for real-time bioacoustic signal segmentation",
    "authors": [
      "Colonna, Juan Gabriel",
      "Cristo, Marco",
      "J{\\'u}nior, Mario Salvatierra",
      "Nakamura, Eduardo Freire"
    ],
    "year": "2015",
    "journal": "Expert Systems with Applications",
    "abstract": "",
    "doi": "",
    "analysis_notes": "An incremental technique for real-time bioacoustic signal segmentation\nJuan Gabriel Colonna ⇑, Marco Cristo, Mario Salvatierra Júnior, Eduardo Freire Nakamura\nAv. Rodrigo Otavio 6200, Institute of Computing (Icomp), Federal University of Amazonas (UFAM), Manaus, Brazil\na r t i c l e i n f o\nArticle history:\nAvailable online 28 May 2015\nKeywords:\nBioacoustic signal segmentation\nWireless Sensor Networks\nUnsupervised learning\nStream data mining\na b s t r a c t\nA bioacoustical animal recognition system is composed of two parts: (1) the segmenter, responsible for\ndetecting syllables (animal vocalization) in the audio; and (2) the classifier, which determines the\nspecies/animal whose the syllables belong to. In this work, we first present a novel technique for automatic segmentation of anuran calls in real time; then, we present a method to assess the performance\nof the whole system. The proposed segmentation method performs an unsupervised binary classification\nof time series (audio) that incrementally computes two exponentially-weighted features (Energy and\nZero Crossing Rate). In our proposal, classical sliding temporal windows are replaced with counters that\ngive higher weights to new data, allowing us to distinguish between a syllable and ambient noise (considered as silences). Compared to sliding-window approaches, the associated memory cost of our proposal is lower, and processing speed is higher. Our evaluation of the segmentation component\nconsiders three metrics: (1) the Matthews Correlation Coefficient for point-to-point comparison; (2)\nthe WinPR to quantify the precision of boundaries; and (3) the AEER for event-to-event counting. The\nexperiments were carried out in a dataset with 896 syllables of seven different species of anurans. To\nevaluate the whole system, we derived four equations that helps understand the impact that the precision and recall of the segmentation component has on the classification task. Finally, our experiments\nshow a segmentation/recognition improvement of 37%, while reducing memory and data communication. Therefore, results suggest that our proposal is suitable for resource-constrained systems, such as\nWireless Sensor Networks (WSNs).\nÓ 2015 Elsevier Ltd. All rights reserved.\n1. Introduction\nForest degradation is a worldwide concern. The success of\necosystem preservation depends on our ability to detect ecological\nstress in early stages. In this context, anurans (frogs and toads)\nhave been used by biologists as an indicator of ecological stress\n(Carey et al., 2001). However, monitoring anurans on-site, by\nhuman experts, may be too expensive or even unfeasible, depending on the size of the target area. Thus, unassisted monitoring\nstrategies can be adopted, such as the detection of anuran calls\nusing sensor networks (Colonna, Cristo, & Nakamura, 2014;\nRibas, Colonna, Figueiredo, & Nakamura, 2012). In such strategies,\nthe sound acquisition is performed by the sensors in a\nnon-intrusive way, which allow us to monitor the environment\nfor a long-term period.\nTo acquire the anuran calls, sensors are equipped with microphones to gather data (Fig. 1). Unlike other type of sensors, the\naudio acquisition deals with high sampling frequencies, resulting\nin a lot of data to be processed and transmitted. The set of all sen-\nsors, distributed in an area of interest, comprises a Wireless Sensor\nNetwork (WSN) (Akyildiz, Su, Sankarasubramaniam, & Cayirci,\n2002). The main advantage of this kind of network is the ability\nthe sensors have to collaborate with each other (Nakamura,\nLoureiro, & Frery, 2007). As a large number of sensors has to be\nused, their costs have to be low, which leads to the development\nof simple devices with limited resources (memory, processing,\nand bandwidth) (Nakamura, Loureiro, Boukerche, & Zomaya,\n2014). Given such constraints, we have to cope with many scientific and technological challenges to effectively use WSNs (Khan,\nPathan, & Alrajeh, 2012).\nMachine learning techniques with WSNs have already been\nused for automatic recognition of anuran species (Hu et al., 2009;\nPotamitis, Ntalampiras, Jahn, & Riede, 2014; Ribas et al., 2012;\nWang et al., 2003). These techniques are based on classifiers (e.g.\nSVM, C4.5 decision trees and kNN) to automate the task of recognizing smaller portions of anuran calls, called syllables. Before classifying the syllables, the calls need to be segmented, i.e., we need to\nidentify the start and the end of every syllable. The precision of the\nsegmentation technique affects the further steps in the species’\nidentification method (Fig. 1), therefore, impacting on the classification performance.\nhttp://dx.doi.org/10.1016/j.eswa.2015.05.030\n0957-4174/Ó 2015 Elsevier Ltd. All rights reserved.\n⇑ Corresponding author.\nE-mail addresses: juancolonna@icomp.ufam.edu.br (J.G. Colonna), marco.cristo@\nicomp.ufam.edu.br (M. Cristo), mario@icomp.ufam.edu.br (M. Salvatierra Júnior),\nnakamura@icomp.ufam.edu.br (E.F. Nakamura).\nExpert Systems with Applications 42 (2015) 7367–7374\nContents lists available at ScienceDirect\nExpert Systems with Applications\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / e s w aFig. 2 shows three syllables of different species recorded under\ndistinct noise conditions. The last segment shows a combination of\ntwo syllables. This figure is useful to illustrate the challenge related\nto proposing a segmentation technique to recognize the four different patterns without compromising the accuracy of the entire\nrecognition system. The challenge of finding the boundaries (a, b,\nc, d, e, f, g, h) can be viewed as an unsupervised binary classification\nproblem, which must be identified when the signal behavior\nchanges. After that, the classification component determines the\nspecies whose that segment (syllable) belongs to.\nOur main contribution is a real-time incremental technique for\nsegmenting anuran syllables in audio streams.\nIn contrast to the use of sliding windows (Jaafar, Ramli, &\nShahrudin, 2013; Jaafar & Ramli, 2013; Rahman & Bhuiyan,\n2012), our incremental strategy stores only simple time series\nstatistics. As such, it has a memory reduction rate of 1=N (N is\nthe size of the sliding window), while reducing the algorithmic\ntime complexity from OðN \u0002 ðn=ðN \u0003 mÞÞÞ to OðnÞ (N is the window\nsize, m is the window overlapping size, and n is the size of the\nstream).\nAs an additional contribution, we derive a methodology for\nassessing the whole system performance by considering it a multilevel classifier.\nThe remainder of this work is organized as follows. In Sections 2\nand 3, we present the motivation and the problem statement,\nrespectively. Section 4 presents an overview of related work. Our\nproposal for incremental transformation is presented in\nSection 6. The evaluation metrics are discussed in Section 7. The\nparameters, experimental protocol and results obtained are\ndescribed in Section 8. Finally, in Section 9, we present our conclusions and point out future directions.\n2. Motivation\nSignal segmentation models have been extensively studied in\nhuman speech recognition. However, these models are not well\nsuited for anuran calls, which have different characteristics\n(Rickwood & Taylor, 2008). For a better feature extraction and\nimprovements of species classification, it is important to select\nthe most representative parts of an anuran call, since these calls\nusually contain long periods of environmental noise (Evangelista,\nPriolli, Silla, Angelico, & Kaestner, 2014).\nThe majority of the approaches for automatic call segmentation\ninvolves non-sequential procedures that consume large amounts\nof memory (Garcia, Marcias-Toro, Vargas-Bonilla, Daza, & López,\n2014; Härmä, 2003; Xie et al., 2015). Moreover, these types of\napproaches are not suitable for data stream scenarios, in which\nlarge amounts of data must be processed in real-time by\nresource-constrained systems/networks (Nakamura et al., 2014).\nAs segmentation is the first step of the recognition framework\n(Fig. 1), this has a direct impact on the species identification rate.\nTherefore, we must understand the relationship between the segmentation and the classification components.\n3. Problem statement\nA syllable is one elementary bioacoustic unit for classification. A\ncontinuous call, emitted by an individual frog, is composed of several syllables repeated along the time. Fig. 3 shows a typical call of\nthe Leptodactylus hylaedactylus species with three syllables. The\nbeginning, middle, and end-points of the syllables are delineated\nby vertical lines depicting three different types of changes that\ncharacterize the vocalization: (1) an abrupt change in the signal\nlevel – e.g., the change from noise to syllable indicated by the first\nvertical dotted line; (2) a gradual change, upward or downward –\ne.g., a gradual increase of noise or a soft signal attenuation, as seen\nbetween the second and third vertical dotted lines; and (3) recurrent change patterns – e.g. the three similar syllables repeated over\ntime.\nThe problem of syllable segmentation is to detect the beginning\nand the end of a syllable. Thus, considering a specific audio stream,\nwe aim at extracting all syllables. The time intervals between the\nsyllables (ambient sound/noise) are not useful to detect the animal.\nIn fact, the noise sections are discarded, because: (a) they increase\nthe misclassification rate; (b) they increase transmission costs; and\n(c) they reduce the WSN lifetime. For this reason, in real situations,\nit is convenient to detect changes in the monitored signals to\ndecide when to start and stop data communication or data processing. How to measure the quality of the segmentation is an additional problem, related to assign the correct species to the\ncorresponding extracted syllables (classification).\n4. Related work\nAutomatic sound segmentation has been widely studied, usually focusing on music and human voice streams (Theodorou,\nMporas, & Fakotakis, 2014). In such studies, it is common to prioritize robust solutions even if it results in higher costs. Foote (2000)\nfocused on music using a kernel function for segmentation, while\nSarkar and Sreenivas (2005) addressed speech, employing the\nAverage Level Crossing Rate (ALCR). The problem of segmenting\ndifferent sources like music, speech, and environmental sound\nfrom movies was addressed by Giannakopoulos, Pikrakis, and\nTheodoridis (2008) by using a technique based on eight spectral\nband frequencies.\nSimilar expensive approaches were also employed in the study\nof bioacoustic signals. For instance, Potamitis et al. (2014) applied\nthe costly Hartley Transform, with good results. Evangelista et al.\n(2014) used the spectrogram to extract two features that represent\nthe syllables. To obtain these syllables, the histogram of the energy\nFig. 1. The three basic steps of a species identification framework.\nFig. 2. Four syllable patterns under different noise conditions.\n7368 J.G. Colonna et al. / Expert Systems with Applications 42 (2015) 7367–7374of the signal and its spectral centroid are computed, thereafter a\nmedian filter is applied to find the threshold levels and perform\nthe segmentation. The quality of the segmentation depends on\nthe threshold values and, to properly choose these values, the\nknowledge of the entire signal is required.\nFrequency based methods have been shown to be more robust\nto background noises. For example, Härmä (2003) and Xie et al.\n(2015) apply image processing techniques to the spectrogram of\nthe signal (Fig. 3). These approaches repeat several processing\nsteps, beginning from the best syllable until extract all syllables.\nThis procedure looses the temporal order, so it is inadequate\nfor real time scenarios. Beyond the computational cost of the\nFourier Transform (FFT), more memory is required to handle\nthe entire image, which makes this approach unsuitable for\nresource-constrained systems.\nNeal, Briggs, Raich, and Fern (2011) use a Random Forest classifier to recognize spectrogram frames. The use of a supervised classifier has two advantages: (1) it is robust to retrieval the desired\nsyllables and (2) segmentation and classification may be performed in a single step. However, this approach is computationally\nexpensive, because of the high cost associated with FFT and the use\nof many decision tree models. Furthermore, the set of possible animals must be known a priori.\nA naive strategy to segment audio samples consists in finding\nthreshold values, such that if the sample amplitude is smaller than\nthe threshold, the sample is classified as noise (Cheng, Sun, & Ji,\n2010; Colonna, Ribas, dos Santos, & Nakamure, 2012; Huang,\nYang, Yang, & Chen, 2009). While simple, such an approach is very\nsensitive to random noise of short duration and high amplitude. To\nimprove segmentation accuracy, Fagerlund (2007) used the energy\nvalue of the signal in dB as a temporal feature to make the method\nmore robust to noise. This approach has the following shortcomings: (1) devices need enough memory to store the whole stream\nbeing processed; (2) the whole audio stream is processed several\ntimes to segment the existing syllables. As a consequence, the syllables cannot be segmented by resource-constrained devices in\nreal time.\nOther methods are based on more restricted strategies that use\na limited number of successive samples from a sliding window\n(Jaafar et al., 2013; Jaafar & Ramli, 2013; Rahman & Bhuiyan,\n2012). In particular, these methods used a combination of two\nlow-cost temporal features to improve the segmentation performance. These features are the signal Energy (E) and the Zero\nCrossing Rate (ZCR). For every iteration of this method, a new\nwindow is processed sequentially. This method is more resilient\nto noise, because it requires that the two thresholds are satisfied\nat the same time, one for Energy and one for ZCR.\nAn indirect way to extract the fundamental frequency of the\nanuran calls, without performing the FFT, is to compute the pitch\nbased on the auto-correlation function. This feature, used to find\nstrong frequency components in the signal, was employed by\nGarcia et al. (2014). The authors conducted an interesting evaluation, which compares the totally recovered syllables to the manually segmented syllables. This evaluation approach is more\nconsistent with a segmentation than a classification task.\nUnfortunately, the authors have not reported the rate of missing\nsyllables.\nMost of the previous segmentation methods are not suitable to\nreal-time processing in resource-constrained systems, because\nthey use large segments of the sound signal. These methods are\nnot appropriate in our context, since we consider that the future\nof the incoming signal is unknown. Table 1 summarizes the key\nfeatures of each related work. We also observed that the use of\ntemporal features along with sequential processing result in simpler implementations that require fewer hardware resources.\nFurthermore, window-based methods are able to achieve good\nperformance by remembering a fixed number of past samples,\nwhich suggests that past information can be incrementally\nupdated.\nAnother contribution of our work is related to how we can\nassess a sound classification system that performs segmentation.\nIn a previous work, Han, Muniandy, and Dayou (2011) have studied\nthe impact of the segmentation on the classification rate. As\nobserved by the authors, there is no consensus on how to asses\nthe final performance, because of issues such as misclassifications\ndue to unrecognized segments. Thus, to evaluate our framework,\nwe propose a new way to evaluate the whole system, described\nin Section 7.2.\n5. Background\nThe signal energy allows us to know when the amplitude\nincreases, while the ZCR provides an approximation of the main\nfrequency. These two temporal features, commonly used in audio\nprecessing tools, are given by:\nEN ¼ 1\nN\nXN\ni¼1\nx2\ni ; ð1Þ\nFig. 3. Three syllables of the Leptodactylus hylaedactylus call: signal amplitude (above) and spectrogram (below).\nJ.G. Colonna et al. / Expert Systems with Applications 42 (2015) 7367–7374 7369ZCRN ¼ 1\n2N\nXN\ni¼1\njsignðxiÞ \u0003 signðxi\u00031Þj; ð2Þ\nin which x i is the amplitude of the audio signal and N the frame size.\nThe function signðÞ is defined as:\nsignðx iÞ ¼ þ1; if x i P 0\n\u00031; if x i < 0\n\u001a\nð3Þ\nBy applying a sliding window, a set of consecutive frames is\ngenerated. If N is much longer than the size of the syllable, many\nunnecessary values will be used to compute EN and ZCRN , probably\ncausing syllable losses. In the opposite case, when N has a very\nshort duration, impulse noises can be interpreted as part of a syllable resulting in a false detection. The requirement that two\nthresholds (T E for EN and T Z for ZCRN ) must be satisfied simultaneously may prevent this issue (Jaafar et al., 2013; Jaafar & Ramli,\n2013; Rahman & Bhuiyan, 2012). Thus, a combination rule is\ndefined as:\nT h ¼ 1; if EN P T E and Z N P T Z\n0; otherwise:\n\u001a\nð4Þ\nThe memory cost associated with this approach is equal to N\nand the time to process an entire signal, of size n, depends on\nthe overlap parameter m (OðN \u0002 ðn=ðN \u0003 mÞÞÞ). Note that parameters N and m affect the segmentation accuracy (Section 8), as it is\nhard to define a pair of values that operate correctly through successive windows.\nHerein, from now on, we refer to this method as EZ-WIN.\n6. A new incremental segmentation approach\nIn this section, we present an incremental variation of segmentation techniques for extracting anuran calls from audio streams. In\ncontrast to sliding-window approaches, the incremental proposal\nis more efficient regarding memory and processing usage.\nAccording to Jaber (2013), incremental techniques should satisfy the following requirements: (a) adaptability – to adapt to gradual changes without ignoring the abrupt ones; (b) knowledge\ntransfer – to deal with new samples without forgetting past decisions; (c) appropriate reaction time – to detect change as fast as\npossible; and (d) low error rate – to avoid the increase of false positive and false negative rates. Based on these requirements, we are\ninterested in segmentation techniques that are sensitive to change,\nrobust to noise and false alarms, and efficient regarding memory,\nprocessing time, and energy. To accomplish this, we redesign previous methods (Jaafar & Ramli, 2013) to remember past\ninformation by using metrics that can be calculated incrementally,\nwithout relying on data windows.\nWe start by presenting an incremental version for computing\nthe Energy Eq. (1). This equation averages all observed x2\ni values\nin a window of size N. We can rewrite the Energy En with weights\nw1; w2; . . . wn P 0, similarly to the exponentially-weighted mean\n(Finch, 2009) En ¼ 1\nW n\nPn\ni¼1w ix2\ni , in which W n ¼ Pn\ni¼1w i and a ¼ wn\nW n .\nThus:\nEn ¼ 1\nW n\nwnx2\nn þ Xn\u00031\ni¼1\nwix2\ni\n!\nEn ¼ 1\nW n\nwnx2\nn þ W n\u00031E n\u00031\n\u0003 \u0004\nEn ¼ 1\nW n\nwnx2\nn þ ðW n \u0003 wnÞE n\u00031\n\u0003 \u0004\nEn ¼ 1\nW n\nðW nE n\u00031 þ wnðx2\nn \u0003 En\u00031ÞÞ\nEn ¼ En\u00031 þ wn\nW n\nðx2\nn \u0003 E n\u00031Þ\nEn ¼ En\u00031 þ aðx2\nn \u0003 E n\u00031Þ ð5Þ\nEq. (5) does not require a window of size N to estimate current\nenergy En. In this new formulation, the parameter a controls the\ntrade-off between the weight of the current sample and the historical data.\nSimilarly, we can derive an incremental version of ZCR as\nZ n ¼ 1\n2W n\nPn\ni¼1w iyi, where yi ¼ jsignðxiÞ \u0003 signðxi\u00031Þj. Thus, we can\nrewrite ZCR (Eq. (2)) as:\n2Z n ¼ 1\nW n\nwny n þ Xn\u00031\ni¼1\nwiyi\n!\n2Z n ¼ 1\nW n\nðwny n þ 2W n\u00031Z n\u00031Þ\n2Z n ¼ 1\nW n\nð2W nZ n\u00031 þ wnðyn \u0003 2Z n\u00031ÞÞ\n2Z n ¼ 2Z n\u00031 þ wn\nW n\nðy n \u0003 2Z n\u00031Þ\n2Z n ¼ 2Z n\u00031 þ aðy n \u0003 2Z n\u00031Þ;\nwhich, divided by 2, results in Eq. (6).\nZ n ¼ Z n\u00031 þ a jsignðx nÞ \u0003 signðx n\u00031Þj\n2 \u0003 Z n\u00031\n\u0012 \u0013\n: ð6Þ\nEqs. (5) and (6) provide E and ZCR values for each input sample\nwithout sliding windows. These equations are able to adapt to\ngradual changes, such as the progressive increase of noise without\nTable 1\nSummary of related works. The abbreviations Acc, P, R, F1, and AEER stands for Accuracy, Precision, Recall, F-Score, and Acoustic Event Error Rate, respectively.\nAuthors Features Procedure Evaluation form\nHan et al. (2011) Spectral-entropy Manual Classification Acc.\nRahman and Bhuiyan (2012) E and ZCR Non-sequential No. of syllables\nJaafar et al. (2013) E and ZCR Sequential No. of syllables Classification Acc.\nHuang et al. (2009) Amplitude Non-sequential Classification Acc.\nColonna et al. (2012) Amplitude Non-sequential Classification Acc.\nCheng et al. (2010) Amplitude Non-sequential No. of syllables Classification Acc.\nFagerlund (2007) Energy Non-sequential Classification Acc.\nNeal et al. (2011) Spectrogram Sequential ROC curves\nHärmä (2003) Spectrogram Non-sequential No. of Syllables Confusion Matrix\nXie et al. (2015) Spectrogram Non-sequential No. of syllables Classification Acc.\nEvangelista et al. (2014) Energy Spectral Centroid Non-sequential Classification Acc.\nPotamitis et al. (2014) Hartley Transform Sequential Classifier P, R and F1.\nOur Approach Incremental E Sequential AEER.\nIncremental ZCR Real-Time Whole system P, R and F1.\n7370 J.G. Colonna et al. / Expert Systems with Applications 42 (2015) 7367–7374losing abrupt changes. They support knowledge transferring since\nthey do not forget past decisions when new samples are processed.\nBoth equations have few parameters to adjust and satisfy memory\nand processing constraints of the sensor nodes. The memory cost in\nthe case of equations is Oð1Þ and the time cost is OðnÞ. In this work,\nwe refer to this incremental method as EZ-I.\nTo reduce the probability of false positives, we can use a mode\nfilter.1 We refer to the version of EZ-I with a mode filter as EZ-IMF.\nThis modification allows the current sample to be identified as a syllable if most of the past S samples were syllables. Algorithm 1 presents EZ-IMF and Fig. 4 illustrates its application. In this algorithm,\nEn and Z n are updated for each sample x n (lines 2–3). If both thresholds are overstepped and the mode filter, implemented with a single\nmc counter, is smaller than S, then mc is incremented by one (lines 4\nand 5). If the sample is not identified as a syllable, the mc is decremented (lines 6 and 7). Once identified as a syllable, the sample is\nsent for further processing if it satisfies the condition of the mode filter (line 10).\nAlgorithm 1. EZ-IMF Segmenter.\n1: function CHANGED ETECTION x n; a; Eðn\u00031Þ; Zðn\u00031Þ; S\n2: En ¼ Eðn\u00031Þ þ aðx2\nn \u0003 Eðn\u00031ÞÞ\n3: Z n ¼ Z n\u00031 þ a jsignðxn Þ\u0003signðxn\u00031 Þj\n2 \u0003 Z n\u00031\n\u0010 \u0011\n4: if En P T E and Z n P T Z and mc < S then\n5: mc = mc + 1\n6: else if mc > 0 then\n7: mc = mc\u00031\n8: end if\n9: if mc P S\n2 then\n10: SEND x n\n11: end if\n12: end function\nBecause of the a parameter, our approach has a slight delay to\ndetect the beginning of each syllable. The same problem is also\nobserved in the methods based on sliding windows, because of\nthe window length and the overlapping factor.\n7. Evaluation methodology\nTo evaluate our method, we compare the segmentation result\nagainst the perfect segmentation (manual segmentation by a specialist), quantifying three type of errors: point-to-point,\nboundaries-to-boundaries and event-to-event.\nTo count event-to-event errors, we use a metric that was\ndesigned for segmentation algorithms, namely, the Acoustic\nEvent Error Rate (AEER). The AEER is frequently used in context\ndetection (Giannoulis et al., 2013), and it is defined as:\nAEER ¼ D þ I þ S\nN ; ð7Þ\nin which N is the number of syllables in each audio clip, D is the\nnumber of missed syllables, I the number of extra syllables, and S\nthe number of replaced syllables computed2 as S ¼ minðD; IÞ. This\nmetric considers that an event is correctly segmented if it starts\nand ends within \u0004100 ms of the event’s real boundaries and if it\nhas at least 50% of the real syllable timespan. In addition, duplicated\nevents are considered false alarms. Thus, in the best case, AEER ¼ 0.\nTo measure the accuracy of the estimated boundaries, com-\npared to the perfect segmentation (boundaries-to-boundaries),\nwe propose to apply the approach developed by Scaiano and\nInkpen (2012), known as WinPR. WinPR counts errors from boundaries so that close errors are not masked as in AEER. Once true positives (TP), true negatives (TN), false positives (FP) and false\nnegatives (FN) are defined, traditional metrics such as precision,\nrecall, and F-Score (cf. Eqs. (8)–(10)) can be calculated. These metrics are very useful for comparing the retrieved signal points that\nare relevant and the fraction of relevant points that are retrieved.\nThe higher the value of these metrics, better is the result of the\nboundaries. These metrics are defined as:\nP ¼ TP\nTP þ FP ; ð8Þ\nR ¼ TP\nTP þ FN ; ð9Þ\nF1 ¼ 2 \u0005 P \u0005 R\nP þ R : ð10Þ\nOften methods for audio detection and segmentation are evaluated by using metrics based on a decision table or confusion\nmatrix. Thus, each point of the real signal is compared to each\npoint of the automatically segmented signal (point-to-point\napproach). The result of the segmentation is binary (signal or not\nsignal) and TP, TN, FP and FN figures are interpreted slightly different from WinPR. Thus, for completeness, we also present True\nPositive Rate (TPR), False Negative Rate (FNR) and the Matthews\nCorrelation Coefficient (MCC) (Powers, 2007), according to the\npoint-to-point approach (cf. Eqs. (11)–(13)). Higher values of TPR\nand lower values of FNR indicate little signal losses. As for MCC,\nthe output values may vary from \u00031 to 1, indicating higher\n(MCC ¼ 1), lower (MCC ¼ 0) and negative (MCC ¼ \u00031) correlation.\nTPR ¼ TP\nTP þ FN ; ð11Þ\nFNR ¼ FN\nFN þ TP ; ð12Þ\nMCC ¼ ðTP \u0005 TNÞ \u0003 ðFP \u0005 FNÞ\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nðTP þ FPÞðTP þ FNÞðTN þ FPÞðTN þ FNÞ\np : ð13Þ\nAs the dataset is not balanced, we provide macro-averaging of\nthe metrics, as recommended by Sokolova and Lapalme (2009).\nIn other words, we first compute the metrics for each species\nand, then, average of these values.\n7.1. Species classifier\nTo evaluate the whole system, we first segmented the syllables\nand annotated them. Thus, for each segmented syllable, we identified the corresponding species. To automatically classify the syllables, they are represented by a set of twelve Mel Frequency\nCepstral Coefficients (MFCC) obtained from a filter bank with 24\nfilters. The classifier we use is kNN (with k ¼ 1) and classification\nresults were evaluated by using Leave-One-Out Cross-Validation.\nAs our main objective is to show the performance of the complete\nsystem, we chose this feature set and classifier based on the framework defined by Colonna et al. (2012). In next section we present\nour approach for the recognition problem as a multi-level system\nwhere two tasks, segmentation and classification, are chained.\n7.2. Multi-level classifier\nAs pointed out before, the segmentation and classification tasks\nare treated as a multi-level classifier with unsupervised and supervised layers. In this setting the output of the segmenter (the first\nclassifier) is the input for the species recognizer (the second\n1 A mode filter considers the most frequent value that was observed as the correct\nvalue.\n2 The implementation of Giannoulis et al. (2013) can be found at http://c4dm.eecs.\nqmul.ac.uk/sceneseventschallenge/.\nJ.G. Colonna et al. / Expert Systems with Applications 42 (2015) 7367–7374 7371classifier) that makes the final decision. To illustrate their operation, suppose that a true syllable is recognized by the segmenter\n(true positive TPs). Given that input, the second classifier may produce one of the four possible outputs: ‘‘it is the target species’’\n(TPc ), ‘‘it is not the target’’ (TNc ) and two type of errors, FPc and\nFNc . Thus, the true positive rate obtained by the final classifier is\ngiven by Eq. (14).\nFrom the previous example, it is clear that, for each different\ninput (signal or noise), the final classification result depends on\nthe segmenter response. All possible combinations (cf. Fig. 5) of\ncorrect and incorrect decisions made by segmenter and classifier\nare summarized by Eqs. (14)–(17):\nTPf ¼ TPs \u0005 TPc ; ð14Þ\nTNf ¼ TPs \u0005 TNc þ TNs; ð15Þ\nFNf ¼ TPs \u0005 FNc þ FNs; ð16Þ\nFPf ¼ TPs \u0005 FPc þ FPsðTPc þ FPc þ FNc þ TNc Þ; ð17Þ\nin which subscripts s, c and f stand for the outputs of segmenter,\nclassifier and final result, respectively.\nIn this system, when the segmenter produces a TNs or a FNs,\nnothing is sent to the classifier, preventing a misclassification or\ncausing a syllable losses. This characteristic leads to less combinations of decisions between segmenter and classifier. In this case,\nthe final result can be computed by Eqs. (15) and (16).\n8. Experiments and results\nTo evaluate our methods, we used a dataset with 15 frogs from\nseven different species for a total of 896 syllables. The audio signals\nin this dataset have a frequency of 8 kHz and 8 bits per sample. The\na value was set to 0.01 and we used the thresholds values\nT E ¼ 0:02 and T Z ¼ 0:3. The experiments were carried out in\nMatlab (detailed scripts and the dataset are available at http://\nbit.ly/1b8bvyE).\nTable 2 shows a comparison between our methods (EZ-I and\nEZ-IMF) and the sliding window versions (EZ-WIN). For EZ-WIN,\noverlap was set as 50%, a value commonly used. We used window\nsizes of 64, 128, 256, 512, and 1024 units (the larger the size, the\nlarger the memory used to remember the past). From this table,\nit is clear that large window sizes improve AEER at the expense\nof memory. Small window sizes improve precision, but decrease\nrecall and increase AEER. These results show that the recall of\nthe sliding window techniques is consistently weak compared to\nour approaches.\nIn addition, our method with the lowest error rate, EZ-IMF,\nmisses about 50% of the segments annotated by the human expert.\nIn general, given the higher recall, the higher TPR and the lower\nFNR of our methods, it is clear that they have a better trade-off\nbetween the number of missing syllables and the transmission of\nunnecessary samples. Finally, the MCC values of our approaches\n(EZ-I and EZ-IMF) indicate more positive relationship than\nEZ-WIN approaches.\nWhen comparing EZ-I and EZ-IMF, the EZ-I performs very\npoorly regarding AEER. This large error rate occurred because\nEZ-I generates many short segments for the species Osteocephalus\nO. Most of the segments were correctly recognized as part of\nFig. 4. Three syllables of Leptodactylus hylaedactylus call. (a) Segmentation output in red by EZ-IMF. (b) Plot of incremental Energy (Eq. (5)). (c) Plot of incremental ZCR (Eq.\n(6)). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)\nFig. 5. Interaction between segmenter and classifier as a whole system. Only\nrecognized segments are sent to the species classifier.\nTable 2\nAverage results for EZ-I, EZ-IMF (S = 40) and EZ-WIN (with N varying from 64 to 1024\nand overlap = 50%).\nTechnique TPR FNR MCC P R F1 AEER\nEZ-I 0.53 0.46 0.59 0.93 0.53 0.62 19.61\nEZ-IMF 0.51 0.48 0.57 0.90 0.51 0.60 4.36\nEZ-WIN, N = 64 0.12 0.87 0.24 0.99 0.12 0.19 6.69\nEZ-WIN, N = 128 0.14 0.85 0.27 0.99 0.14 0.22 5.66\nEZ-WIN, N = 256 0.16 0.83 0.29 0.93 0.16 0.24 3.05\nEZ-WIN, N = 512 0.14 0.85 0.27 0.92 0.14 0.21 1.58\nEZ-WIN, N = 1024 0.07 0.92 0.14 0.57 0.07 0.11 1.34\n7372 J.G. Colonna et al. / Expert Systems with Applications 42 (2015) 7367–7374syllables. However, it found many more segments than the actual\nnumber, which decreases the precision increasing the error rate.\nBy using the mode filter, EZ-IMF was able to find the correct\nnumber of segments (reducing AEER except for the species\nRhinella G.), although it was not able to detect the beginning of\nthe segments as precisely as EZ-I (which resulted in slightly worse\nfigures for P and R).\nThe TPR and FNR are good idicators of signal loss (considering a\npoint-to-point evaluation). In these results, the values of FNR are\nconsistent with the R (recall), indicating more signal loss for the\nwindowing approaches. The low recall in the case of N ¼ 1024\nindicates that this value is much larger than the length of syllables,\nwhich may result in the loss of the whole syllable in some cases.\nTable 3 presents the results achieved by EZ-IMF and EZ-WIN3\nfor each species. It is clear that the most challenging species were\nOsteocephalus O. and Rhinella G. By carefully inspecting the segmentations we note that even when both methods find more syllables\n(false positives) than the perfect segmentation (large AEER),\nEZ-IMF tends to find them within actual syllables which leads to\nsmaller F1 values.\nNow let us evaluate a complete system to classify sound, showing the impact of segmentation on the species recognition.\n8.1. Impacts of segmentation\nWe compute how the proportion of correct segmentations\naffects the final accuracy by Eqs. (14)–(17). Table 4 shows the final\nresults of three metrics. For each segmentation technique, we\nobtained the final value as the macro-average of all species. The\nlast line is the amount of memory used for compute the\nsegmentation.\nIn this table, PS (perfect segmentation) column represents the\nperformance of kNN to classify segments defined by a human\nexpert, i.e., with neither FPs nor FNs. The last row of Table 4 shows\nthat when we increase the amount of memory, results are not necessarily better. Best values are presented in bold face. As we can\nsee, our segmentation technique outperformed the baselines, considering the F1 metric, which leverages precision and recall. For the\nbaselines, the best result was obtained with a window of size\nN ¼ 256.\n9. Conclusions and final comments\nThe approach presented in this paper is based on the idea of\nincremental processing with a minimal usage of resources. We\ndemonstrate that our method is effective in detecting audio segments of interest. By segmenting the signal before transmission,\nthe method reduces the amount of data sent by the sensors to\nthe sink node. As transmission is the most energy intensive task,\nthis reduction leads to longer lifetime. Additionally, incremental\ncalculation is suitable for a big data context, in which sounds are\nreceived by sink nodes as data streams to be processed in an online\nfashion (Gama & Gaber, 2007).\nWhen we use sliding windows, it is difficult determine the best\nsize. This size may be chosen by using the evaluation methodology\nwe presented in this work. A small window size reacts quickly to\nchanges being more sensitive to noise, which leads to more false\npositives. A larger window size loses more syllables, increasing\nthe false negative rate by failing to quickly adapt to changes. As\nour method is able to continuously adapt to changes, through\nmeans of a forgetting process, it performs better in our application.\nBecause segmentation techniques can retrieve very different\namounts of syllables, using classification accuracy for overall evaluation can be misleading. Thus, we proposed a more robust evaluation methodology to assess the performance of a whole system\ncomposed by a segmenter and a classifier. This solution allowed\nus to better quantify the impact of the segmenter on the final system performance.\nOur main contributions are threefold: (1) improvements in\nrecall and F-score of automatic segmentation, with few syllable\nlosses; (2) lower dependency on memory (O(1)) and real time\nresponse; and (3) a multilevel classifier evaluation method for\nthe entire bioacustical framework. In addition, the proposed\nmethod is able to better adapt to gradual changes and yields fewer\nfalse negatives taking advantage of the ability to evolve over time.\nAs future work, we intend to study how to automatically find, in\nreal time, optimal values for a and threshold parameters T E e T Z ,\nwithout compromising the incremental characteristics of our\nmethod. Also, we intend to develop and expand our evaluation\nmethod to cover more general cases, with more than two classification levels. Our work clearly take advantage of simple characteristics to distinguish noise from signal in an unsupervised fashion.\nThus, we will verify how to derive incremental versions of other\nrobust features used in literature.\nTable 3\nComparison between EZ-IMF and EZ-WIN considering each species.\nSpecies Syllables EZ-IMF EZ-WIN512\nTPR MCC AEER TPR MCC AEER\nLeptodactylus H. 453 0.63 0.74 0.01 0.20 0.39 0.71\nLeptodactylus F. 218 0.65 0.71 0.04 0.15 0.33 0.81\nAmeerega T. 88 0.26 0.35 1.24 0.18 0.28 0.96\nAdenomera A. 73 0.51 0.56 0.27 0.24 0.43 1.07\nHyla Minuta 40 0.43 0.55 1.88 0.10 0.24 1.12\nRhinella G. 17 0.86 0.72 23.85 0.02 0.05 5.18\nOsteocephalus O. 7 0.27 0.46 7.73 0.00 0.08 2.00\nAverage 0.51 0.57 4.36 0.14 0.27 1.58\nTable 4\nPerfect segmentation (PS), Precision, Recall and F-Score of the entire system. Approximation of memory used for the segmentation (Byte).\nMetrics PS W64 W128 W256 W512 W1024 EZ-I EZ-IMF\nPrecision 0.99 0.98 0.99 0.99 0.99 0.83 0.92 0.94\nRecall 0.96 0.09 0.12 0.15 0.12 0.06 0.42 0.49\nF1 0.98 0.16 0.21 0.25 0.21 0.11 0.55 0.62\nMemory used – 64 B 128 B 256 B 512 B 1024 B 2 B 3 B\nBest values of each line are highlighted and are presented in bold face.\n3 We chose N ¼ 512 for Table 3, as it performed well in Table 2, considering the\ncompromise between F1 and AEER.\nJ.G. Colonna et al. / Expert Systems with Applications 42 (2015) 7367–7374 7373Acknowledgements\nThe authors acknowledge the support granted by FAPEAM\nthrough process number 2210.UNI175.3532.03022011 (Anura\nProject – FAPEAM/CNPq PRONEX 023/2009).\nReferences\nAkyildiz, I. F., Su, W., Sankarasubramaniam, Y., & Cayirci, E. (2002). Wireless sensor\nnetworks: A survey. Computer Networks, 38.\nCarey, C., Heyer, W. R., Wilkinson, J., Alford, R. A., Arntzen, J. W., Halliday, T., et al.\n(2001). Amphibian declines and environmental change: Use of remote-sensing\ndata to identify environmental correlates. Conservation Biology, 15.\nCheng, J., Sun, Y., & Ji, L. (2010). A call-independent and automatic acoustic system\nfor the individual recognition of animals: A novel model using four passerines.\nPattern Recognition, 43, 3846–3852.\nColonna, J. G., Cristo, M. A. P., & Nakamura, E. F. (2014). A distribute approach for\nclassifying anuran species based on their calls. In 22nd international conference\non pattern recognition.\nColonna, J. G., Ribas, A. D., dos Santos, E. M., & Nakamure, E. F. (2012). Feature subset\nselection for automatically classifying anuran calls using sensor networks. In\nInternational joint conference on neural networks (IJCNN) (pp. 1–8). IEEE.\nEvangelista, T. L. F., Priolli, T. M., Silla, C. N., Angelico, B. A., & Kaestner, C. A. A.\n. In Automatic segmentation of audio signals for bird species identification\n(pp. 223–228). IEEE.\nFagerlund, S. (2007). Bird species recognition using support vector machines.\nEURASIP Journal of Applied Signal Processing, 2007. 64–64.\nFinch, T. (2009). Incremental calculation of weighted mean and variance. Technical\nReport. University of Cambridge Computing Service.\nFoote, J. (2000). Automatic audio segmentation using a measure of audio novelty.\nIEEE International Conference on Multimedia and Expo (ICME) (Vol. 1,\npp. 452–455). IEEE.\nGama, J., & Gaber, M. M. (2007). Learning from data streams. Springer.\nGarcia, N., Marcias-Toro, E., Vargas-Bonilla, J. F., Daza, J. M., & López, J. D. (2014).\nSegmentation of bio-signals in field recordings using fundamental frequency\ndetection. In 3rd International work conference on bioinspired intelligence\n(IWOBI). IEEE.\nGiannakopoulos, T., Pikrakis, A., & Theodoridis, S. (2008). A novel efficient approach\nfor audio segmentation. In 19th International conference on pattern recognition,\n(ICPR) (pp. 1–4).\nGiannoulis, D., Stowell, D., Benetos, E., Rossignol, M., Lagrange, M., & Plumbley, M. D.\n(2013). A database and challenge for acoustic scene classification and event\ndetection. In European signal processing conference.\nHan, N. C., Muniandy, S. V., & Dayou, J. (2011). Acoustic classification of australian\nanurans based on hybrid spectral-entropy approach. Applied Acoustics, 72,\n639–645.\nHärmä, A. (2003). Automatic identification of bird species based on sinusoidal\nmodeling of syllables. IEEE international conference on acoustics, speech, and\nsignal processing (ICASSP’03) (Vol. 5). IEEE (pp. V–545).\nHu, W., Bulusu, N., Chou, C. T., Jha, S., Taylor, A., & Tran, V. N. (2009). Design and\nevaluation of a hybrid sensor network for cane toad monitoring. ACM\nTransactions Sensors Network, 5, 4:1–4:28.\nHuang, C. J., Yang, Y. J., Yang, D. X., & Chen, Y. J. (2009). Frog classification using\nmachine learning techniques. Expert Systems with Applications, 36, 3737–3743.\nJaafar, H., Ramli, D., & Shahrudin, S. (2013). Mfcc based frog identification system in\nnoisy environment. In International conference on signal and image processing\napplications (ICSIPA) (pp. 123–127). IEEE.\nJaafar, H., & Ramli, D. A. (2013). Automatic syllables segmentation for frog\nidentification system. In 9th International colloquium on signal processing and\nits applications (CSPA) (pp. 224–228). IEEE.\nJaber, G. (2013). An approach for online learning in the presence of concept change\n(Ph.D. thesis). LIMSI-CNRS.\nKhan, S., Pathan, A. K., & Alrajeh, N. A. (2012). Wireless sensor networks: Current\nstatus and future trends. CRC Press.\nNakamura, E. F., Loureiro, A. A. F., Boukerche, A., & Zomaya, A. Y. (2014). Localized\nalgorithms for information fusion in resource constrained networks.\nInformation Fusion, 15, 2–4.\nNakamura, E. F., Loureiro, A. A. F., & Frery, A. C. (2007). Information fusion for\nwireless sensor networks: Methods, models, and classifications. ACM Computing\nSurveys, 39.\nNeal, L., Briggs, F., Raich, R., & Fern, X. Z. (2011). Time-frequency segmentation of\nbird song in noisy acoustic environments. In International conference on\nacoustics, speech and signal processing (ICASSP) (pp. 2012–2015). IEEE.\nPotamitis, I., Ntalampiras, S., Jahn, O., & Riede, K. (2014). Automatic bird sound\ndetection in long real-field recordings: Applications and tools. Applied Acoustics,\n80, 1–9.\nPowers, D. M. W. (2007). Evaluation: from precision, recall and F-factor to ROC,\ninformedness, markedness & correlation. Technical Report. SIE-07-001 School of\nInformatics and Engineering, Flinders University.\nRahman, M., & Bhuiyan, A. (2012). Continuous bangla speech segmentation using\nshortterm speech features extraction approaches. International Journal of\nAdvanced Computer Sciences and Applications, 3, 11.\nRibas, A. D., Colonna, J. G., Figueiredo, C. M. S., & Nakamura, E. F. (2012). Similarity\nclustering for data fusion in wireless sensor networks using k-means. In\nInternational joint conference on neural networks (IJCNN) (pp. 1–7).\nRickwood, P., & Taylor, A. (2008). Methods for automatically analyzing humpback\nsong units. The Journal of the Acoustical Society of America, 123, 1763–1772.\nSarkar, A., & Sreenivas, T. V. (2005). Automatic speech segmentation using average\nlevel crossing rate information. International conference on acoustics, speech, and\nsignal processing (ICASSP) (Vol. 1, pp. 397–400). IEEE.\nScaiano, M., & Inkpen, D. (2012). Getting more from segmentation evaluation. In\nProceedings of the 2012 conference of the north american chapter of the association\nfor computational linguistics: Human language technologies. NAACL HLT ’12 (pp.\n362–366).\nSokolova, M., & Lapalme, G. (2009). A systematic analysis of performance measures\nfor classification tasks. Information Processing & Management, 45, 427–437.\nTheodorou, T., Mporas, I., & Fakotakis, N. (2014). An overview of automatic audio\nsegmentation. International Journal of Information Technology and Computer\nScience (IJITCS), 6, 1–9.\nWang, H., Elson, J., Girod, L., Estrin, D., Yao, K., & Vanderberge, L. (2003). Target\nclassification and localization in habitat monitoring. In Proceedings of the\ninternational conference on speech and signal processing. IEEE.\nXie, J., Towsey, M., Truskinger, A., Eichinski, P., Zhang, J., & Roe, P. (2015). Acoustic\nclassification of australian anurans using syllable features. In Tenth international\nconference on intelligent sensors, sensor networks and information processing (IEEE\nISSNIP 2015).\n7374 J.G. Colonna et al. / Expert Systems with Applications 42 (2015) 7367–7374",
    "affiliations": [
      {
        "country": "Brazil",
        "discipline": "Other",
        "university": "Federal University of Amazonas"
      }
    ],
    "species_categories": [
      "Amphibian"
    ],
    "specialized_species": [
      "Leptodactylus hylaedactylus",
      "Leptodactylus fuscus",
      "Ameerega trivittata",
      "Adenomera andre",
      "Hyla minuta",
      "Rhinella gallardo",
      "Osteocephalus sp."
    ],
    "computational_stages": [
      "Data Collection",
      "Pre-processing",
      "Sequence Representation",
      "Meaning Identification"
    ],
    "linguistic_features": [
      "Semanticity",
      "Recursion"
    ],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886032",
    "updated_at": "2026-02-10T20:17:00.046440"
  },
  {
    "id": "1202756e-2954-4b0a-9614-d530841645e0",
    "title": "A toolbox for animal call recognition",
    "authors": [
      "Towsey, Michael",
      "Planitz, Birgit",
      "Nantes, Alfredo",
      "Wimmer, Jason",
      "Roe, Paul"
    ],
    "year": "2012",
    "journal": "Bioacoustics",
    "abstract": "",
    "doi": "",
    "analysis_notes": "This may be the author’s version of a work that was submitted/accepted\nfor publication in the following source:\nTowsey, Michael, Planitz, Birgit, Nantes, Alfredo, Wimmer, Jason, & Roe,\nPaul\n(2012)\nA toolbox for animal call recognition.\nBioacoustics, 21(2), pp. 107-125.\nThis file was downloaded from: https://eprints.qut.edu.au/51616/\n© Consult author(s) regarding copyright matters\nThis work is covered by copyright. Unless the document is being made available under a\nCreative Commons Licence, you must assume that re-use is limited to personal use and\nthat permission from the copyright owner must be obtained for all other uses. If the document is available under a Creative Commons License (or other specified license) then refer\nto the Licence for details of permitted re-use. It is a condition of access that users recognise and abide by the legal requirements associated with these rights. If you believe that\nthis work infringes copyright please provide details by email to qut.copyright@qut.edu.au\nNotice: Please note that this document may not be the Version of Record\n(i.e. published version) of the work. Author manuscript versions (as Submitted for peer review or as Accepted for publication after peer review) can\nbe identified by an absence of publisher branding and/or typeset appearance. If there is any doubt, please refer to the published source.\nhttps://doi.org/10.1080/09524622.2011.648753i\nTitle: A Toolbox for Animal Call Recognition\nAuthors: Michael Towsey, Birgit Planitz, Alfredo Nantes, Jason Wimmer and Paul\nRoe\nAffiliation: Microsoft QUT eResearch Centre, Queensland University of Technology,\nBrisbane, Australiaii\nAbstract:\nMonitoring the natural environment is becoming increasingly important as habit\ndegradation and climate change reduce the world’s biodiversity. We have developed\ntools, applications and processes to assist ecologists with the collection and analysis\nof acoustic data at large spatial and temporal scales. One of our key objectives is\nautomated animal call recognition, and our approach has three novel attributes. First,\nwe work with raw environmental audio, contaminated by noise and artefacts, and\ncontaining calls that vary greatly in volume depending the animal’s proximity to the\nmicrophone. Second, initial experimentation suggested that no single recogniser could\ndeal with the enormous variety of calls. Therefore, we developed a toolbox of generic\nrecognisers to extract invariant features for each call type. Third, many species are\ncryptic and offer little data with which to train a recogniser. Many popular machine\nlearning methods (e.g. Hidden Markov Models, Neural Networks and Support Vector\nMachines) require large volumes of training and validation data, and considerable\ntime and expertise to prepare. Consequently we adopt bootstrap techniques that can be\ninitiated with little data and refined subsequently. In this paper, we describe the\nrecognition algorithms in our toolbox and present results for their performance on real\necological problems.1\n1 Introduction1\nThe increased availability, power and storage capacity of computing hardware have2\nmade it feasible to gather large volumes of audio data for ecological analysis. In3\naddition, enhanced web services have made it possible to bring that audio data4\ndirectly into the laboratory rather than have ecologists go into the field. However, it is5\nimpossible for ecologists to listen to even a small fraction of the audio data made so6\neasily available (Agranat 2009). Some degree of automated assistance is essential. In7\nconjunction with ecologists, our laboratory has developed an online service8\n<http://sensor.mquter.qut.edu.au> that offers a variety of tools for the analysis of9\nenvironmental recordings. In a previous report, we described various aspects of our10\nsensor network for collecting audio data (Lau et al. 2008). In this report we describe11\nour approach to the problem of automated analysis and, in particular, to automated12\nanimal call recognition.13\n14\nPerhaps due to the importance of birds as indicator species of environmental health,15\nthere is already a considerable body of work published on the detection of bird16\nvocalisations (Acevedo et al. 2009; Anderson et al. 1996; Brandes 2008; Cai et al.17\n2007; Chen & Maher 2006; Juang & Chen 2007; Kwan et al. 2004; McIlraith & Card18\n1997; Somervuo et al. 2006). A common approach has been to adopt the well-19\ndeveloped tools of Automated Speech Recognition (ASR), which extract Mel-20\nFrequency Cepstral Coefficients (MFCCs) as features and use Hidden Markov21\nModels (HMMs) to model the vocalisations. Unfortunately it is not so easy to22\ntranslate ASR to the analysis of environmental recordings because there are far fewer23\nconstraints in the latter task. Two issues are noise and variability. ASR tasks are24\ntypically restricted to environments where noise is tightly constrained, e.g. over the25\ntelephone. By contrast, environmental acoustics contain a wide variety of non-26\nbiological noises having a great range of intensities and a variety of animal sounds272\nthat have nothing to do with the task at hand. Furthermore, the sources can be located1\nany distance from the microphone. Secondly, despite its difficulty, ASR applied to the2\nEnglish language requires the recognition of about 50 phonemes. By contrast, bird3\ncalls offer endless variety; variety of call structure between species, variety between4\npopulations of the one species and variety within and between individuals of the one5\npopulation. Many species have multiple calls (in this paper we do not distinguish bird6\ncalls form bird songs) and many are mimics. To give some indication of the difficulty7\nof bird call recognition, a state-of-the-art commercial system using an ASR approach8\nthat has been under development for more than a decade, achieves, on unseen test9\nvocalisations of 54 species, an average accuracy (defined as the average of precision10\nand recall) of 65% to 75% (Agranat 2009). This accuracy may not be sufficient for11\nsome applications. Furthermore, the software requires the user to tune many12\nparameters which require a good understanding of the underlying algorithms. Some13\nwork has been done in the urban setting on the recognition of acoustic events in14\nauditory scene analysis but these tasks suffer from exactly the same difficulties15\n(Cowling & Sitte 2003; Temko et al. 2006; Zhuang et al. 2008).16\n17\nOur approach to animal call recognition has three attributes which help to distinguish18\nit from many previous approaches to the same problem: real world data, multiple19\nrecognition methods and limited training data.20\n21\n1. Real world data22\nThe results we describe are obtained with real world data that has not been cleaned of23\nartefacts. There is a world of difference between reporting classification results on24\ncarefully cleaned data with balanced training and test sets versus the raw recordings to25\nwhich ecologists actually listen. Constructing a data set containing equal numbers of26\neach call type that have been manually cut from recordings to exclude extraneous273\nnoise is not a realistic approximation to the real situation of very uneven class1\nnumbers and low call density in arbitrary background noise.2\n3\n2. Multiple recognition methods4\nAfter an initial period where we adapted ASR methods to animal call detection, we5\ncame to the conclusion that a one-algorithm-fits-all approach could not deal with the6\nenormous variety of environmental acoustic events. To address this issue, we7\ndeveloped a toolbox of generic recognisers that identify invariant features in calls of8\ninterest. For bird calls consisting of a single syllable (e.g. the currawong, see Figure9\n1(a)), extracting MFCCs and training HMMs may still be a suitable choice. However10\nthe ‘tool-box’ approach allows us to mix and match feature extraction with classifiers11\nto suit generic call types.12\n13\n3. Limited training data14\nDespite the demonstrated accuracy of machine learning methods such as Neural15\nNetworks (NN) and Hidden Markov Models (HMM) on standard datasets, these16\nmethods do not necessarily adapt well to the real world of environmental recordings.17\nMany bird species are cryptic and the large amounts of data required to train a NN or18\nHMM are not available. It is more practical to adopt methods that require just one or a19\nfew instances of a call type. This approach is effective for species whose calls vary20\nlittle within and between populations (e.g. the Lewin’s Rail (Lau et al. 2008)).21\nAnother consideration is the practicality of training of multi-class classifiers. Training22\na 50 bird-call classifier for a given locality may be possible but it is not practical if it23\nmust be repeated every time the ecologist wishes to incorporate a new call instance or24\ncall class.25\n264\nIn this paper we describe a ‘toolbox’ of animal call recognition techniques developed1\nin our laboratory. (It is available at <http://sensor.mquter.qut.edu.au/>.) The main2\nmotivation for our work has been to provide tools that are easy to use (compared to3\nthe more sophisticated techniques of ASR) and which are well adapted to the real4\nproblems confronted by ecologists wishing to process many hours of environmental5\nrecordings. This is not to say that the more sophisticated tools have no value. Rather6\nour tools could be viewed as filters to highlight points of interest in long recordings.7\n8\nWe limit ourselves to terrestrial animals – more specifically we exclude marine9\nanimals whose calls present a different set of problems (Rickwood & Taylor 2008). In10\naddition we avoid birds that mimic. In theory, any animal call used for11\ncommunication purposes should contain invariant features but in practice the12\nrecognition of mimics can be a very difficult task, even for humans.13\n14\nIn Section 2, we describe some different call structures, their invariant features and15\nthe recognition algorithms appropriate for them. In Section 3 we describe experiments16\nwith datasets obtained for selected animal calls and present our results. We conclude17\nthe paper with a discussion of our ongoing work in expanding the toolbox.18\n19\n2 Call Detection Algorithms20\n2.1 Call Structures21\nMany animal calls have a hierarchical structure. A complex bird call, for example,22\nmay be divided into phrases, the phrases into syllables and the syllables into one or23\nmore elements (Catchpole & Slater 1995). Each element may take the form of a24\nwhistle (single tone), chirp (slowly modulated tone), whip (rapidly modulated tone),25\nclick (vertical line in spectrogram), vibrato, shriek, stacked harmonics (simultaneous265\nmultiple tones) or buzz (rapidly repeated click) (Catchpole & Slater 1995). The same1\nsyllable can be repeated multiple times. Figure 1 illustrates the structure of calls used2\nin this work. Each image has been extracted from a spectrogram - the x-axis3\nrepresents time, the y-axis frequency, and the grey scale represents acoustic intensity.4\n5\nThe currawong call (Figure 1(a)) consists of a frequency modulated whistle. The6\ncurlew call consists of multiple syllables having simple to complex structure, with or7\nwithout harmonics. The example shown in (Figure 1(b) is a simple modulated tone.8\nThe male koala has a complexly structured call consisting of inhalations and9\nexhalations lasting for 30 seconds or more. A small portion of its call (illustrating10\nthree oscillatory exhalations) is shown in Figure 1(c)). The cane toad and gecko have11\nmulti-syllable calls. The former consists of a click rapidly repeated for 20 seconds or12\nmore and the latter consists of some five to six clicks slowly repeated (Figure 1(d-e)).13\nThe ground parrot call consists of about 10-13 syllables, each one a descending chirp.14\nSuccessive syllables increase in pitch (Figure 1(f)). The whistle and whip of a whip15\nbird call is illustrated in Figure 1(g)). The whip may be either ascending or16\ndescending. Three examples of stacked harmonic calls are shown in Figure 1(h-j)).17\nHuman vowels typically display a dense stack of harmonics in a spectrogram.18\n19\nWhile all calls exhibit some form of variability (e.g. the periodicity of the gecko call20\ndepends on temperature (Marcellini 1974), each call type nevertheless has one or21\nmore invariant features on which recognition can depend. The same can even be said22\nof certain diffuse, apparently structureless events such as wind (Figure 1(k)) and23\ncanopy rain (Figure 1(l)).24\n256\nOur toolbox contains a suite of classifiers each of which we have found appropriate1\nfor different call structures and syllable types (see list in Table I). Call structure2\ndictates feature selection, which in turn dictates the classification algorithm. For3\nexample to detect calls (or parts of calls) consisting of a single-syllable frequency-4\nmodulated whistle, we use its STFT spectral image for template matching. Spectral5\ntemplates are of limited use, however, if the call is variable and in these cases the6\ninvariant features of a call must be found. The whistle in a whipbird call, for example,7\nvaries in pitch but always consists of a horizontal ‘track’ found within a defined8\nfrequency band. The following section describes the variety of recognition algorithms9\nin our toolbox in more detail.10\n11\n2.2 Feature Extraction and Classification12\nCalls are recorded using the hardware described in Section 3.1. Recordings are13\nsampled at 22,050 Hz (or subsequently down-sampled to this value) and a bit rate of14\n16. We outline the steps used by each recognition algorithm below.15\n16\n2.2.1 MFCCs and Hidden Markov Models17\nAs noted in the introduction we did not find the use of MFCC features and HMMs to18\nwork well for our recognition tasks. Yet we started with this approach because it has19\nbeen reported in the literature for bird call recognition. We include it in this report20\nsimply to compare its performance with other methods.21\n22\nWe implemented the Hidden Markov Model Toolkit (HTK,23\n<http://htk.eng.cam.ac.uk/>), a freely available software library for designing, training247\nand testing HMMs (Young et al. 2006). Although tailored for speech recognition1\ntasks, HTK has also been applied to biological sequences (Akhtar et al. 2007) and bird2\ncall recognition (Trifa et al. 2008). HTK automatically extracts MFCC features before3\ntraining the HMMs. We use most of the default parameter settings, in particular 50%4\nframe-overlap and 12 cepstral coefficients. The frequency band is constrained to5\nmatch the call to be recognized. Rather surprisingly, we obtain best results when6\nomitting signal energy, and the dynamic delta and acceleration features.7\n8\nThis method represents calls as sequences of observations, each observation (a vector9\nof cepstral coefficients derived from a frame) being treated as an emission from a10\ndynamical system whose state transitions are described by a Markov process. In a11\nsimple Markov process each observation would be assigned to a unique system state12\nbut animal calls are too variable for such a restrictive model. In an HMM each13\nobservation is modelled as a probabilistic function of system state and each call type14\nis modelled as a sequence of states. An HMM classifier returns the probability that the15\nobserved sequence would be emitted by a given call model.16\n17\nImportant parameters are the number of model states, the number of emission18\ncategories, the number of training iterations and the estimation of an HMM19\nrepresenting background noise. There are two possibilities for this last: 1) estimate a20\nnoise model from the silence periods in the training instances, or 2) estimate a noise21\nmodel from separate recordings appropriate to the operational environment. We tried22\nboth approaches and had more success with the latter.23\n248\n2.2.2 Spectrograms1\nAll the remaining call recognition algorithms employ features derived from2\nspectrograms prepared using the Short-Time Fourier Transform (STFT). The choice3\nof parameters is guided by those typically used in ASR and by (Brandes 2008;4\nBrandes et al. 2006). The signal is framed using a window of 512 samples (23.2ms)5\nwhich offers a reasonable compromise between time and frequency resolution. A6\nHamming window function is applied to each frame prior to performing a Fast Fourier7\nTransform (FFT), which yields amplitude values for 256 frequency bins, each8\nspanning 43.07 Hz. The spectrum is smoothed with a moving average filter (window9\nwidth = 3). The amplitude values (A) are converted to decibels (dB) using dB =10\n20.log10(A). Note that the dB values at this stage are with respect to a hypothetical11\nsignal having unit amplitude in each frequency bin. Frame overlap varies from 0 to12\n75% depending on the amount of fine structure in the call.13\n14\n2.2.3 Binary Template Matching15\nSome calls, in particular frequency modulated whistles (Figure 1(a-b)), can be16\nrecognised using a simple binary template. The user marquees the call of interest in a17\nnoise reduced spectrogram and extracts a binary representation using an intensity18\nthreshold (typically around 4 dB above background). The template’s on-cells define19\nthe call of interest and its off-cells contribute to an error function. Our template20\nextraction tool allows manual editing to clean up background noise and to idealize the21\nshape.22\n239\nAlthough a binary template does not model the variations in acoustic intensity1\ndistributed through time and frequency, this lack of specificity can be an advantage if2\nit generalises over irrelevant call variability.3\n4\nThe template is passed over all the frequency bins in a user specified band and a score5\nis calculated for each temporal location (i.e. frame). The score may be calculated in a6\nnumber of ways. To pick out faint calls from background noise (that is, to minimise7\nfalse negatives), we use the difference in mean intensity between template on-cells8\nand off-cells:9\nScore = ∑onintensity/con – ∑offintensity/coff,10\nwhere ∑onintensity and ∑offintensity are the sums of the acoustic intensity in the on11\nand off cells respectively and con and coff are the count of on and off cells respectively.12\n1-2 dB is a minimum perceptible difference and we find a suitable threshold score to13\nadjust the recall/sensitivity trade-off lies in the 2.0 to 5.0 dB range.14\nAlternatively, we have used:15\nScore = ( ∑onintensity – ∑offintensity )/con,16\nwhich has the advantage that it imposes stricter penalties on the noise intensity in the17\noff-cells (assuming coff > con) and can be used to lessen false positive errors. Both18\nmetrics work on the principle of matching shape and intensity profiles in images19\n(Brunelli 2009).20\n21\n2.2.4 Oscillation Detection22\nMany animal calls consist of a repeating or oscillating single syllable, for example the23\nLewin’s Rail and the gecko. Even the male koala bellow has a characteristic2410\noscillatory structure that can be used for identification purposes. Oscillation Detection1\n(OD) is performed on the spectrogram using the Discrete Cosine Transform (DCT).2\n3\nThe DCT is a highly effective and sensitive technique, but should be used with4\nappropriate caution. We apply it to the time series in each frequency bin of the5\nspectrogram prior to noise removal since our noise removal algorithm tends to remove6\nfaint oscillations that are nevertheless detectable by DCT (Towsey et al. 2010). The7\nsame limitations apply to the DCT as to the FFT, in particular the requirement for8\nsignal stationarity over the duration of the DCT. In addition, brief acoustic impulses9\ncause the DCT to return spurious oscillatory content.10\n11\nTwo significant parameters for the DCT are the time duration and threshold amplitude12\nrequired to register a ‘hit’. The optimum value for time duration will depend on the13\nexpected oscillation rate and stationarity of the call to be detected. The maximum14\nDCT amplitude is obtained after normalisation of the DCT coefficients to unit length.15\nExperimentation has shown that threshold values between 0.4 and 0.6 are suitable. In16\npractice, it is not difficult to determine appropriate parameter values using training17\ndata as long as call variability (e.g. the oscillation rate) is within definable limits.18\n19\nCall identification depends on recognising concentrations of oscillation ‘hits’ within20\nthe user-defined constraints of frequency band, time duration and oscillation rate.21\nThese constraints are essential to cull false positive detections and are effective if the22\nsought call falls within characteristic bounds. Typical parameter values for male23\nkoala, gecko and cane toad are shown in Table II. The recall/sensitivity trade-off is24\ncontrolled by adjusting the fraction of ‘hit’ bins (within the call’s frequency band)2511\nrequired for a positive detection. For a more detailed description of this method see1\n(Towsey et al. 2010).2\n3\n2.2.5 Segmentation4\nThe speed of the OD algorithm can be significantly improved by avoiding periods of5\nsilence. Our segmentation algorithm extracts acoustic content by calculating the6\nacoustic energy in each signal frame in the frequency band of interest. We determine a7\nbaseline (modal) frame-energy and its standard deviation using the method described8\nin (Towsey et al. 2010). Segmentation involves retaining consecutive frames whose9\nenergy exceeds a user-threshold (defined in terms of noise standard deviations above10\nthe modal noise level). These are expected to be frames that contain potential calls of11\ninterest. In this work, we apply segmentation only in conjunction with OD, in which12\ncontext a filter is required to smooth over oscillatory gaps. The width of the13\nsmoothing window is given by:14\nwidth = 1 / minimum expected oscillation rate for calls of interest15\nTo further reduce computation, frame energy is calculated only for the middle quarter16\nof the user-defined frequency band.17\n18\n2.2.6 Acoustic Event Detection19\nWe developed Acoustic Event Detection (AED) to extract information about the20\ndistribution of acoustic energy in a spectrogram using the minimum of prior21\nknowledge. AED processes the spectrogram like an image and returns rectangular22\nmarquees around isolated acoustic events.23\n2412\nFor a spectrogram:1\nStep 1: Apply a 2D smoothing Wiener filter.2\nStep 2: Noise reduce by removing the modal noise intensity calculated for each3\nfrequency band.4\nStep 3: Convert the noise-reduced spectrogram to a binary image with a user-defined5\nintensity threshold. An effective threshold typically ranges from 6-9 dB, which6\ncorresponds to values in the literature (Brandes 2008). Note that a minimum7\nperceptible acoustic difference is around 1-2 dB.8\nStep 4: Because a single threshold may fragment low intensity events, we join event9\npixels separated by N or fewer pixels in the vertical or horizontal directions. By10\ndefault, N = 1.11\nStep 5: Place a rectangular marquee around the outer limit of each event, defined as12\nany group of contiguous (in 8 directions) black pixels in a white background.13\nStep 6: The previous steps will incorrectly join acoustic events that overlap, but14\nwhich, to both ear and eye, are due to separate sources. We are able to detect and15\nseparate most overlapping events by examining intensity distributions within16\nanomalously large events.17\nStep 7: Cull small events - those whose area falls below a user defined threshold (a18\ndefault value of 200 pixels was usually satisfactory for a typical spectrogram with 25619\nfrequency bins and a frame rate of 86 per second).20\n21\nNote that there are two important user defined parameters in this algorithm: the22\nintensity threshold used to convert the spectrogram to binary form and the small area23\nthreshold, below which events are eliminated. Other thresholds were estimated from24\nthe data itself.2513\n2.2.7 Event Pattern Recognition1\nSome multi-syllable animal calls can be modelled by the 2D distribution of their2\ncomponent syllables in the spectrogram rather than by the actual content of those3\nsyllables. We applied this technique, which we termed Event Pattern Recognition4\n(EPR), to the recognition of ground parrot calls for which it proved effective even5\nwhen the spectrogram was contaminated by numerous other ‘noise’ events. This6\nwould not be possible using MFCC features.7\n8\nA single training example is sufficient to define a call in terms of its component9\nevents (see Figure 2). In recognition mode, this call template is passed over the10\nacoustic events extracted from a test signal (hereafter referred to as the ‘test’ events)11\nusing the AED algorithm in Section 2.2.6. To limit unnecessary computation, only12\n‘test’ events were considered whose centroid lay within a user defined frequency band13\n(3.5 - 4.5 kHz for ground parrots). For any match between template and test-events,14\nthe match-score is calculated as the average overlap between the template events and15\nthe closest ‘test’ events whose centroids fall within the bounds of the template. The16\nfractional overlap between a single template event and its closest ‘test’ neighbour is17\ngiven by:18\nOverlap = ½(x/T + x/E)19\nwhere x = the overlapped area (in pixel units), T = the (pixel) area of the template20\nevent and E = the (pixel) area of the ‘test’ event. The overlap fraction lies between 0.021\n(no overlap) and 1.0 (exact coincidence). The average overlap of all the events in the22\ntemplate gives rise to a score between 0.0 (complete mismatch of all events—actually23\nnot possible because at least the first template event must find some overlap)—and24\n1.0 (complete coincidence of ‘test’ and template events).2514\nThe recall/sensitivity trade-off can be adjusted using an overlap threshold in the range1\n[0, 1]. The optimum value for this threshold should be derived from an ROC curve2\nand strictly speaking the data required to obtain the ROC curve has the status of3\ntraining data. In our case we had so few calls, even in a 6 hour recording, that the4\nresults described in Section 3.3.7 were obtained by optimizing the threshold on the5\navailable data.6\n7\n2.2.8 Spectral Peak Tracking and Syntactic Pattern Recognition8\nSpectral Peak Tracking (SPT) isolates the traces of spectral ridges in a spectrogram9\n(Chen & Maher 2006). It is most useful for the recognition of acoustic events defined10\nby clean whistles. Fortunately many bird calls have this characteristic. SPT is not11\nuseful to detect parrot shrieks or diffuse events such as made by wind and rain.12\n13\nAs originally published, SPT was not well equipped to detect near-vertical tracks or14\nwhips such as made by several Australian birds (for example the whipbird, Figure15\n1(d) and the Golden Whistler). Here we describe a modification to the SPT algorithm16\nfor the better detection of whips.17\n18\nStep 1: Smooth the input spectrogram using a 2D Wiener filter.19\nStep 2: Noise reduce the spectrogram. (Towsey & Planitz 2010).20\nStep 3: Identify (near-) horizontal tracks by identifying maxima in the spectral frame.21\nStep 4: Identify (near-) vertical tracks by identifying maxima in the time series for22\neach frequency bin.23\nStep 5: Overlap the resulting horizontal and vertical tracks.2415\nStep 6: Remove tracks (any arbitrarily branched set of 8-directionally consecutive1\npixels) whose total pixel count is less than a threshold (default value = 15 pixels for2\nspectrograms prepared with default parameter values).3\n4\nThe two important parameters are the dB threshold for noise removal (step 2) and the5\nshort-track threshold (step 6). A difficulty with the algorithm is that it tends to6\nhighlight echoes (as wisps trailing a call) not otherwise obvious in the original7\nspectrogram.8\n9\nSyntactic Pattern Recognition (SPR) depends on the ability to represent a pattern as a10\nsequence of symbols selected from a finite alphabet, each symbol representing a11\n‘primitive’ element of the composite pattern (Bunke & Sanfeliu 1990). This permits12\nthe representation of complex sequential patterns more accurately than can be13\nachieved with ‘flat’ feature vectors of fixed dimensionality. The whipbird call has a14\nsimple two component structure (whistle followed by whip; see Figure 1(g)) that15\nsuggests the possibility for two primitives, a horizontal line segment and a near-16\nvertical line segment. Recognition depends on detecting a sequence of horizontal17\nprimitives (the whistle) followed by vertical primitives (the whip) where the whistle18\ncan be of varying durations and frequency and the whip can be either ascending or19\ndescending. In our implementation, the notion of a whipbird grammar is implicit in20\nthe scoring algorithm.21\n22\nStep 1: The input to SPR is a spectrogram in which spectral tracks have been23\nhighlighted using SPT as described above.2416\nStep 2: Apply a user-defined intensity threshold to convert the spectrogram to a binary1\nmatrix.2\nStep 3: Identify the location of horizontal and vertical line primitives in the3\nspectrogram. The identification of line primitives depends on an additional two4\nparameters, the length of the line primitive and a sensitivity threshold. A primitive is5\nfound when the per cent of on-cells in a line segment exceeds a threshold.6\nStep 4: Assign a horizontal line-primitive score to each frame. We search within a7\nuser-defined bandwidth and time-period for horizontal primitives. The horizontal8\nscore for frame N is the fraction of frames over a previous (user-specified) time period9\ntraversed by a horizontal primitive.10\nStep 5: Assign a vertical line-primitive score to each frame. The vertical score for11\nframe N is the fraction of cells in the subsequent rectangle (enclosed by the user-12\nspecified time period and frequency band of a whip) traversed by a vertical primitive.13\nStep 6: The whipbird score for frame N is the average of its horizontal and vertical14\nscores. Since both scores lie in the interval [0, 1] a threshold in [0, 1] can be used to15\nadjust the recall/sensitivity trade-off for the combined score.16\nStep 7: A hit is predicted where the score exceeds the user defined threshold (as17\ndescribed in step 6) for the number of consecutive frames set by the user for a whip18\nduration (in step 5).19\n20\n2.2.9 Harmonic Detection21\nMany animal calls display harmonics above a fundamental tone. Furthermore, the22\nharmonic tracks often trace paths nearly parallel to the fundamental. Typical examples23\nare the female koala (Figure 1(h)), human vowels (Figure 1(i)), crows (Figure 1(j))24\nand squeaky door hinges. A serious difficulty with using harmonics as a feature for2517\ncall recognition in environmental recordings is that higher harmonics drop out at a1\ndistance. Consequently the spectral pattern of a nearby bird is very different from that2\nof the same bird at a distance. In ASR this problem does not arise because the speaker3\ntalks into a microphone. In environmental recordings, calls will be uttered at any4\ndistance. Nevertheless, Harmonic Detection (HD) can be useful if one knows the calls5\nwill be uttered close by or if one wishes to determine the proximity of a source by the6\nnumber of high frequency harmonics.7\n8\nAlthough the DCT is used in ASR to extract cepstral coefficients from spectra, we did9\nnot find this approach suitable for environmental recordings. For bird calls consisting10\nof a pure whistle (one frequency), the whistle presents to the DCT like an impulse and11\ntherefore returns spurious ‘high harmonic’ content. Instead we counted the number of12\nharmonic tracks appearing within a user defined bandwidth. The score for a given13\nframe is a measure of the average intensity of the harmonic peaks (dB above14\nbackground) in a noise reduced spectrum (Towsey & Planitz 2010) discounted by a15\nfunction of the difference between the number of observed and expected harmonic16\npeaks:17\nFrame HD score = (∑nan / N).w,18\nwhere an is the amplitude of the nth spectral peak, N is the number of observed19\nspectral peaks in the user defined bandwidth, and w is a weighting factor that is a20\nfunction of the difference between the observed number of peaks, N, and the expected21\nnumber E:22\nw = 1.0 if (abs(N - E)) < 323\n= 3 / abs(N - E) otherwise.2418\nThe score array is smoothed with a moving average filter (window = 5) and a hit is1\npredicted where the score exceeds a user defined threshold for a number of2\nconsecutive frames within a user defined minimum and maximum duration.3\n4\n2.2.10 Wind: AED and Diagonal Linear Classifier5\nWind and rain are frequent acoustic ‘contaminants’ of environmental recordings.6\nThere are two reasons why automated recognition of these episodes might be useful.7\nFirst, in most cases the user will want to minimise storage and computation by8\navoiding wind and rain events that mask useful information. Second, although wind9\nand rain can be detected using meteorological instruments, hardware security is a10\nproblem in many locations—indirect evidence of wind and rain could help to interpret11\nother features of a recording. While foam baffles can be used to cover microphones12\nand reduce the effect of wind, in practice we have found that once wet they retain13\nmoisture.14\n15\nWe approached wind detection as a classification task, where the entities being16\nclassified are acoustic events extracted using AED as described above. Gusting wind17\nevents (e.g. Figure 1(k)) are found in the low frequency range and AED is able to18\nmarquee those that would hinder further analysis. We explored a range of event19\nfeatures and adopted four; two describing the distribution of acoustic intensity and20\ntwo describing acoustic entropy.21\n22\nWind Feature 1: Step 1: Calculate the mean pixel intensity for each frequency bin in23\nthe event. Step 2: Calculate the difference, in decibels, between two mean intensity24\nvalues—the maximum value for any bin located below 500Hz and the minimum value2519\nin those bins above the maximum bin (see Figure 3(b)). The intensity difference is1\ngreater for wind events.2\n3\nWind Feature 2: Calculate the difference, in Hertz, between two frequency values—4\nthe frequency at which the maximum mean intensity is located and the frequency at5\nwhich the minimum mean intensity is located (Figure 3(b)). The frequency difference6\nis greater for wind events.7\n8\nWind Feature 3: Step 1: Calculate the entropy of the pixel intensity values in each9\nfrequency bin. Step 2: Calculate the difference between two entropy values—the10\nminimum entropy value for any frequency bin located below 500Hz and the11\nmaximum entropy value for any bin above the location of the minimum (see Figure12\n3(c)). The entropy difference is greater for wind events.13\n14\nWind Feature 4: Calculate the difference, in Hertz, between two frequency values—15\nthe frequency at which the maximum entropy is located and that at which the16\nminimum entropy is located (Figure 3(c)). The frequency difference is greater for17\nwind events.18\n19\nFor training and test data we extracted all events whose minimum and maximum20\nfrequencies were <500 Hz and <2 kHz respectively. Events had to be longer than one21\nsecond for reliable tagging but the extracted features are duration independent and22\ntherefore the trained classifier is able to label events shorter than one second.23\n2420\n2.2.11 Rain: AED and Linear Classifier1\nAs with wind detection, we approached rain detection as a task to classify AED2\nevents. The selected rain events consisted of heavy canopy rain. In particular, they3\nexcluded light rain and drizzle. During canopy rain, broadband percussive effects4\narise from the striking of large rain drops on surfaces near the microphone (e.g.,5\nFigure 4(a)). The following three features were found to offer reasonable detection6\naccuracy. Two describe acoustic intensity and the third describes acoustic entropy.7\n8\nRain Feature 1: The selected canopy rain events included discernible raindrops which9\nleft a trace of about ten intensity peaks per second (Figure 4(b)). Feature 1 is the mean10\ninterval in seconds between the intensity peaks.11\n12\nRain Feature 2: The difference between the minimum and maximum raindrop interval13\nFigure 4(b). The difference is less for canopy rain events.14\n15\nRain Feature 3: Observation revealed that canopy rain events have higher entropy16\nvalues in the 8.5-10.5 kHz frequency bins than do non-rain events. Therefore feature 317\nis the mean entropy value of frequency bins in the range 8.5–10.5 kHz (Figure 4(c)).18\n19\nFor training and test data we extracted all events whose minimum and maximum20\nfrequencies were >1 kHz and >8.5 kHz respectively and whose bandwidth was greater21\nthan 2 kHz. Reliable tagging required events of duration longer than three seconds but22\nonce again, the features are duration independent (for events longer than 0.5 seconds)23\nand therefore the trained classifier can be used to label events shorter than 3 seconds.24\n2521\n3 Experiments1\nIn this section we demonstrate the above techniques on a variety of animal calls that2\nare representative of those of interest to the ecologists with whom we are working.3\nWe describe the hardware, data preparation and the recognition accuracy for the4\nvarious techniques.5\n6\n3.1 Hardware7\n3.1.1 Networked Sensors8\nThese devices provide a near real-time sensing in locations with 3G connectivity. Due9\nto bandwidth constraints and 3G transmission costs, they are configured to activate at10\nregular intervals (typically two to four minutes at half hourly intervals), upload data to11\na central repository and then deactivate until the next scheduled recording. These12\nnetworked sensors consist of a 3G smartphone (HTC), an electret-style external13\nmicrophone, pre-amplifier, DC-DC converter and an external power supply (solar14\npanel). The system is capable of operating continuously and autonomously for months15\nat a time with minimal maintenance. Uninterrupted, continuous deployments of 1816\nmonths have been achieved to date (Wimmer et al. 2010).17\n18\n3.1.2 Acoustic Loggers19\nAcoustic loggers are designed to provide a short term or ad hoc high resolution20\nacoustic sensing capability but can be configured to record continuously for extended21\nperiods (Wimmer et al. 2010). Recorded acoustic data is stored internally and the22\ndevice provides up to 14 days continuous recording. These sensors are highly portable2322\nand powered by batteries for ad hoc deployments or externally using a solar power1\nsupply for fixed deployments. As with the networked devices, these sensors are self-2\ncontained in an all-weather container.3\n4\n3.2 Experimental Design5\nAll recordings were obtained from various locations on the east coast of Queensland,6\nAustralia, using either networked sensors or data loggers. Most of the recordings were7\nobtained in the context of three research projects, one on koalas (FitzGibbon et al.8\n2009), another on quolls (Belcher et al. 2008) and another at a university field station9\n(Williamson et al. 2008).10\n11\nNetworked sensors returned recordings of two or four minutes depending on the12\nprotocol requested by the ecologist. Data loggers returned recordings of several hours13\nduration. These were split into lengths of two to four minutes.14\n15\nMuch consideration needs to be given to the reporting of recognizer accuracy. In the16\nabsence of generally accepted standardised datasets, recognition accuracy can be17\nmade arbitrarily high depending on the selection and prior cleaning of the recordings.18\nWe have endeavoured in these experiments to construct datasets that reflect the ‘real-19\nworld’ of sound which ecologists must process. Datasets were chosen according to20\ntheir expected ecological significance rather than to obtain clean recordings. For21\nexample, recordings containing wind and rain ‘contaminants’ were not removed.22\nSome of the recordings include the morning and evening chorus where there can be a23\ncacophony of sound. Others contain traffic noises, air-conditioners (in city24\nrecordings), human speech, dogs barking and airplanes. Finally the tagging of calls2523\nwas done by ecologists whose trained ears could detect faint distant calls that one1\nwould not expect to detect by automated means. Only in this last case did we exclude2\nfrom consideration calls whose intensity was less than 2 - 4 dB above the background3\nnoise level.4\n5\nAnother issue in the context of our data was whether to express accuracy in terms of6\ncorrectly identified calls or correctly identified recordings. We opted for the latter7\nbecause the principle cost of an error is the time spent online by an ecologist loading a8\nfile and accessing the predicted call. Consequently we use the following standard9\ndefinitions for recall and precision:10\nRecall = TP/(TP+FN),11\nand12\nPrecision = TP/(TP+FP),13\nbut define TP (true positives) as the number of short (2-4 minute) recordings correctly14\nidentified as containing one or more calls of interest; FN (false negatives) as the15\nnumber of files containing positive calls none of which were identified; and FP (false16\npositives) as the number of files incorrectly identified as containing one or more calls.17\nScoring on the basis of recordings has two effects on the reported accuracy. Where a18\nrecogniser detects a TP yet makes an FP or FN error in the same file, we label that file19\ncorrectly classified. On the other hand we observed many instances where multiple20\nTPs obtained in one recording were offset by a single error in another file. This21\nsituation arises because many bird calls appear in temporal clusters. In short, the22\naccuracy figures presented below must therefore be regarded as indications of23\noperational performance in a real problem as opposed to finely tuned estimates of2424\naccuracy. Note that all the experiments were designed to produce a binary output –1\ncall identified/not identified.2\n3\n3.3 Calls4\n3.3.1 Currawong (Strepera graculina)5\nThe currawong dataset consisted of 29 four-minute recordings taken at half-hourly6\nintervals from midnight to 7am and 5pm to midnight on the 2nd November 2009 at St7\nBees Island. This protocol was designed to encompass both sides of the morning and8\nevening chorus. The dominant calling species is the curlew. However 5 of the 299\nrecordings contained currawong calls and typically their calls are clustered suggesting10\nflocks of the birds making an appearance during the recording.11\n12\nWe compared two recognition techniques, HMMs trained on MFCC features and a13\nsimple binary template. As can be seen from the results in Table I, neither method14\nperformed well. However there are two points to note here: First much time was spent15\nobtaining training and validation data for the HMM method while the binary template16\nwas quickly prepared from only one representative call, manually edited by the user to17\nremove artefacts and echo. Second, the currawong calls were numerous but clustered18\ninto just five files. The binary template recognised the great majority of the calls and19\ntherefore on a call basis its recall would have far exceeded that of the HMM. The low20\nrecall of the HMM approach was due to the difficulty of training a suitable noise21\nmodel that covered the range of ambient noise situations. The low precision of the22\nbinary template was primarily due to false positive identifications of the more23\nnumerous curlew calls which sit in the same frequency band.2425\n3.3.2 Beach Stone-curlew (Esacus neglectus)1\nThe curlew dataset was the same as that used to test the currawong recognizer. 19 of2\nthe 29 recordings contain curlew calls, although 9 of them are of low intensity due to3\ndistance of the source. The curlew produces a variety of syllable types during one call4\nbut the syllable shown in Figure 1(b) is sufficiently well defined to warrant targeting.5\nWe employed a binary template and achieved an accuracy of 76% (Table I). The6\ndominant errors were false negatives due to the large proportion of low intensity calls.7\n8\n3.3.3 Male Koala (Phascolarctos cinereus)9\nThe male koala bellow consists of a sequence of loud inhalations and exhalations. The10\nexhalations have an oscillatory component which offers a suitable target for11\nrecognition. Training data (for tuning the OD parameters) consisted of 12 four-minute12\nrecordings, each containing a koala call. Test data, totalling 7 hours 40 mins (46013\nminutes), were split into 115 four-minute recordings, 12 of which contained koala14\ncalls. There were 18 calls, ranging from high to low intensity. This selection of15\nrecordings having low call density is representative of the real world situation.16\n17\nRecall was 75% with the three false negative files containing distant bellows of very18\nlow intensity. To measure precision, we took into account that koala bellows contain19\nmultiple oscillatory exhalations which will be detected in clusters. Consequently files20\ncontaining only a single hit are likely to be false positives. Ignoring recordings with21\nfewer than two hits resulted in a precision of 75%. False positives were mostly due to22\na bird (the Orange Footed Scrub Fowl) with a deep, chattering call, producing23\noscillations in a frequency band overlapping that of the male koala. Although2426\nprecision and recall give some indication of accuracy of the detection, these metrics1\nignore the large volume of data that was scanned to get the result. Based on total files2\ncorrectly classified, we obtain an accuracy of 95%. For the ecologist this is a large3\nsaving in time.4\n5\n3.3.4 Cane Toad (Bufo marinus)6\nCane toad data was collected in a suburban backyard (Brisbane, Queensland, January7\n2010) and in rural farmland (near Gympie, South East Queensland). Both locations8\nwere in the vicinity of permanent or semi-permanent water bodies with known cane9\ntoad populations. A total of 674 minutes of recording were split into 337 two-minute10\nfiles. The dataset contained 83 cane toad calls in 53 files. The suburban recordings11\nwere ‘contaminated’ with a wide variety of extraneous sounds including traffic, air-12\nconditioning, speech, dogs etc.13\n14\nThe OD recogniser achieved a recall and precision of 93% and 98% respectively, and15\nan accuracy of 99% over 337 files. These high accuracy rates are partly due to the fact16\nthat cane toads have very consistent call characteristics which make it possible to fine17\ntune the recogniser parameters. The false positives fall into two categories:18\nkookaburra calls that oscillate in same frequency range as cane toads and very short19\nhits due to background noise. The false negatives were all low intensity calls.20\nAdditionally, one call was masked by wind, an unwelcome contaminant that affects21\nthe accuracy of animal call recognition in all real world tasks.22\n2327\n3.3.5 Asian House Gecko (Hemidactylus frenatus)1\nAsian house gecko recordings were recorded in January and February 2010 just2\noutside a suburban house (Brisbane, Queensland). 540 minutes of recording were split3\ninto 270 two-minute files, 77 of which contained 84 calls. As shown in Figure 1(e),4\nthe gecko call consists of a succession of chirps which can be detected using the OD5\nalgorithm. Recall and precision were 91% and 90% respectively with an accuracy of6\n94% over the 270 files. The high accuracies confirm that geckos, like cane toads, have7\nvery consistent call characteristics that are readily detectible by the OD recogniser.8\nFalse negatives errors were due to missing calls of low intensity.9\n10\n3.3.6 Segmentation for OD Recognition Experiments11\nRecognition efficiency can be greatly increased by ignoring those parts of a recording12\nin which there is no acoustic activity in the bandwidth of interest. We repeated the13\nrecognition experiments for male koalas, cane toads and geckos using a segmentation14\nalgorithm to remove silence.15\n16\nTable II confirms that high accuracy rates can be retained when using the17\nsegmentation filter prior to OD recognition. In the case of the gecko, accuracy18\nactually increased due to the filtering out of false positives (‘noise’). However in the19\ncase of the cane toad data, segmentation removed some weak calls thus lowering20\nrecall. The main purpose of the segmentation filter however was to reduce processing21\ntime and in this it was successful - up to 87% time reduction in the case of the gecko22\ndata (Table II, bottom right cell).23\n2428\n3.3.7 Ground Parrot (Pezoporus wallicus)1\nGround parrot data was collected with data loggers placed in a nature reserve 100 km2\nnorth of Brisbane. We acquired a total of 6 hours and 45 minutes of recordings, which3\nwere broken into one-minute sections. An ecologist tagged the calls (see Figure 1(f)).4\nOf the 405 one-minute files, 32 contained calls. However nine of these were barely5\naudible. It should be noted that 1-2 dB is the minimum perceptible audible difference6\nbetween signal and background noise (Lüscher 1951). Consequently ground parrot7\ncalls whose maximum intensity was less than 2 dB above background noise were8\nignored for testing purposes.9\n10\nWe used EPR for detecting a specific type of ground parrot call, a stable pattern of11\nshort-duration, narrow-band, faint chirps that ascend in the frequency as shown in12\nFigure 1(e) and Figure 2. Note that ground parrots have other vocalisations that differ13\nfrom our call of interest.14\n15\nAED’s two default parameters were modified: the intensity threshold was reduced16\nfrom 9 to 3 dB (because many of the calls were faint) and the small area threshold was17\nlowered from 200 to 100 pixels (because the component syllables are small). The EPR18\nalgorithm overlays the ground parrot template (see Figure 2) on acoustic events19\ndetected by AED. To limit unnecessary computation, only events were considered20\nwhose centroid lay within the 3.5 - 4.5 kHz bandwidth of interest. For each location,21\nthe bottom-left vertex of the template was aligned to the bottom-left vertex of the22\nselected ‘test’ event. The precision and recall rates cited below were calculated using23\na threshold overlap of 27% determined on the total data. There was not enough data24\nfor a separate validation and test set.2529\nPrecision and recall for the 23 files with audible parrot calls were both 87%. Two of1\nthe three false negatives detected were faint calls (compared to the true positives), and2\nthe other was missed because of edge effects (i.e. only half a call was present at the3\nend of a recording). Three false positives resulted from incorrect detections due to4\nrain which presented in the spectrogram as a random distribution of acoustic events.5\nTotal accuracy was 99%. We believe this is a particularly effective technique for6\nmulti-syllable calls which have a characteristic distribution of events in the7\nspectrogram.8\n9\n3.3.8 Eastern Whipbird (Psophodes olivaceus)10\nThe whipbird has a two syllable call consisting of a whistle followed by a whip11\n(Figure 1(g)). It is a good candidate for the Syntactic Pattern Recognition approach.12\nThe dataset consists of 38 two minute recordings, 14 of which contain whipbird calls.13\nThe accuracy of 82% shown in Table I is somewhat misleading in that whip bird calls14\nare clustered and some of the recordings contained many true positive recognitions15\nthat counted only as one TP on a file basis.16\n17\n3.3.9 Torresian Crow (Corvus orru)18\nThe crow spectrogram has a set of stacked harmonics that extend up into the 8 kHz19\nrange depending on how close the bird is to the microphone. It is clearly a candidate20\nfor our HD algorithm. The crow dataset consisted of 20 four-minute recordings, 12 of21\nwhich contained crow calls. Recall was 100% and precision was 71% with an22\naccuracy of 75%. We have used the same algorithm (with different parameter values)2330\nfor recognition of female koala calls and the vowels in human speech but we do not1\npresent results due to lack of suitable data in our environmental recordings.2\n3\n3.3.10 Wind Events4\nA classifier was trained with 142 ‘wind’ and 142 ‘not-wind’ events using Matlab’s5\nclassify.m class. ‘Not-wind’ events included low frequency rumbling due to traffic6\nand aircraft. Best results on a validation set of 383 ‘wind’ events and 243 ‘not-wind’7\nevents were obtained with a diagonal-linear classifier. The training and validation8\nerror rates were 10% and 14% respectively.9\n10\nWe tested the diagonal-linear classifier on 1235 one-minute audio files (Table I),11\nacquired with a variety of sensors at different locations. The recordings included koala12\nbellows, ground parrot calls and many other kinds of acoustic events. The accuracy13\nrate was 96% on a one-minute recording basis (Table I). False positive detections14\nwere due to male koala bellows that have low frequency content. The FP-FN trade-off15\nfor the wind detector could be adjusted using a constant which shifts the decision16\nplane towards one class or the other. We used the default value of zero for this17\nconstant.18\n19\n3.3.11 Rain Events20\nCanopy rain events were extracted from recordings taken at five different locations in21\nQueensland, Australia. The events were classified as ‘rain’, ‘not-rain’ or ‘not-sure’;22\nevents needed to be longer than 3 seconds for reliable classification but even so, many23\nevents were classed as ‘not-sure’. The training and validation sets excluded ‘not-sure’2431\nevents and consequently there was less data available than for the wind classifier.1\nNon-rain events primarily consisted of percussive sounds resulting from construction2\nand human activity. A classifier was trained with 54 ‘rain’ and 52 ‘not-rain’ events3\nusing Matlab’s classify.m function. A validation set of 19 ‘rain’ and 14 ‘not-rain’4\nevents was used to determine that a Linear classifier provided best results. We tested5\nthe classifier on 247 one-minute files, 104 of which contained rain events and6\nachieved recall, precision and accuracy rates of 75%, 75% and 79% respectively7\n(Table I).8\n9\nIt is worth noting that rain detection is the more difficult than wind detection, both for10\nhumans and for the machine classifier. Indeed, the poorer performance of the rain11\nclassifier was probably in part due to the prior inaccurate tagging of events that were12\nused to train the classifier.13\n14\n4 Discussion and Conclusion15\nIn this work we have described a toolbox of call recognition techniques to detect16\nanimal calls in environmental recordings. Our objective was to report performance17\nfigures for experimental conditions that reflect the needs of ecologists having to18\nprocess many hours of recordings. The work reported here arose out of an early19\nrealisation that a one-recogniser-fits-all approach would not cope with the20\nunconstrained variety of acoustic events that appear in environmental recordings. In21\nparticular, the highly refined techniques of ASR using MFCCs and HMMs have been22\ntailored to a very constrained audio environment and, even on theoretical grounds,23\nthey are ill-suited to unconstrained audio environments. Of course ASR techniques2432\ncan be made to perform well on datasets of animal calls that have been clipped from1\nrecordings to make a balanced N-class problem. But this is not the real world of2\necology.3\n4\nOur recognisers have a number of features that reflect real world usage:5\n1. With the exception of the MFCC-HMM approach, our recognisers can be6\ntrained in the first instance with just one or very few training instances. The7\nrecognisers can be refined when new instances become available. This is8\nnecessary because most often large numbers of training calls are not available9\nand large training and tests sets are time consuming to curate.10\n2. The recognisers are constructed as binary classifiers, i.e. they detect presence11\nor absence. This is in contrast to an N-class classifier which must assign new12\ninstances to the most likely class. The difficulty with an N-class classifier is13\nhow to represent the null class when the audio content is unconstrained.14\n3. An additional difficulty with the single N-class classifier is that typically the15\ndiscovery of a new instance or class requires the retraining of the entire16\nclassifier. With N binary classifiers, only one of them needs retraining.17\n4. In our experience successful classification depends more on extracting the18\nappropriate features for a task than on the sophistication of the classifier. With19\nthe one-recogniser-fits-all approach the same feature set must be extracted for20\nall calls and consequently a powerful classifier is required because it is21\ndifficult to separate the classes in feature space. By contrast, given a set of22\nappropriate features, a linear classifier is often good enough. This was most23\nobviously demonstrated in the wind and rain classification tasks where a linear2433\nclassifier was the best (of those offered by Matlab) at separating the two1\nclasses.2\n5. For the most part our binary classifiers have tuning parameters whose function3\nis intuitively clear for the non-technical ecologist. For example, calls have a4\nminimum and a maximum duration and amplitude thresholds are represented5\nin decibels. The obvious exception is the MFCC-HMM approach whose many6\nparameters require a basic understanding of the theory.7\n8\nThere are two obvious extensions to our work. The first is to construct classifiers for9\nadditional call types. The second is more challenging. At the present time our10\nrecognisers return a score either in dB units or normalised in [0, 1]. Therefore we are11\nnot able to make a choice between two simultaneous ‘hits’ if the score is in different12\nunits. We intend to normalise all scores so that it is possible to disambiguate cases13\nwhere a single acoustic event returns positive to more than one classifier. This14\neffectively offers the possibility of using the N binary classifiers to imitate an N-class15\nnearest neighbour classifier.16\n17\nAcknowledgement18\nThe Microsoft QUT eResearch Centre is funded by the Queensland State Government19\nunder a Smart State Innovation Fund (National and International Research Alliances20\nProgram), Microsoft Research and QUT.2134\nTables\nTable I. Toolbox for Call Recognition\nGeneric Call Structure Call Features Extracted from\nSpectrograms\nClassification\nAlgorithm\nRecordings (Files in\nDatasets)\n# Files\nwith\nCalls\nRecall Precision Accuracy\nMFCC HMM 40% 100% 90%Currawong\nBinary template Template matching\n29 x 4-minutes 5\n100% 50% 83%\nFrequency-modulated whistle(s)\nCurlew Binary template Template matching 29 x 4-minutes 19 63% 100% 76%\nMale Koala 115 x 4-minutes 12 75% 75% 95%\nCane Toad 337 x 2-minutes 55 93% 98% 99%\nOscillatory components\nAsian House Gecko\nOscillation rate and\namplitude using DCT.\nThreshold classifier\n270 x 2-minutes 77 91% 90% 94%\nSyllable events having fixed\ndistribution in spectrogram\nGround Parrot AED Event Pattern\nRecognition\n405 x 1-minute 23 87% 87% 99%\nCombinations of whistles, chirps,\nwhips\nWhipbird Orientation of spectral\ntracks\nSyntactic Pattern\nRecognition\n38 x 2-mintues 14 100% 67% 82%\nStacked harmonics Crow Frequency spacing of\nparallel spectral tracks.\nThreshold classifier 20 x 4-minutes 15 100% 71% 75%\nWind AED/Image features Diag. linear classifier 1235 x 1-minute 792 99% 96% 96%Diffuse\nRain AED/Image features Linear classifier 247 x 1-minute 104 75% 75% 79%35\nTable II. OD Parameter Settings and Recognition Results\nDataset\nMale koala Cane toad Asian house\ngecko\nParameter\nFrequency band 0.1 – 1.0 kHz 0.5 – 1.0 kHz 1.5 – 3.0 kHz\nFrame overlap 75% 75% 0%\nDCT duration 0.3 sec 0.5 sec 1.0 sec\nOscillation bounds 20 – 50 Hz 10 – 20 Hz 3 – 7 Hz*\nMin DCT amplitude 0.6 0.6 0.5\nCall duration 0.5 – 2.5 sec 0.5 – 20.0 sec 1.0 – 6.0 sec\n% of freq. bins with oscillations 20% 40% 30%\nOD Recognition Results\nTotal Number of Files 115 337 270\nTrue Positives Files 9 49 70\nFalse Positives Files 3 1 8\nFalse Negatives Files 3 4 7\nRecall 75% 93% 91%\nPrecision 75% 98% 90%\nAccuracy 95% 99% 94%\nOD Recognition Results - With Segmentation\nRecall 75% 83% 91%\nPrecision 75% 98% 93%\nAccuracy 95% 97% 96%\nProcessing Time (% of the original\ntime w/o segmentation)\n34% 56% 13%\n*Asian house gecko oscillation bounds were derived based on behavioural studies described in\n(Marcellini 1974).36\nFigure Captions\nFigure 1. Examples of animal vocalisations and other sounds.\nFigure 2. Example Ground Parrot template.\nFigure 3. Wind image features.\nFigure 4. Rain image features.37\nReferences\nAcevedo, M. A., C. J. Corrada-Bravo, H. Corrada-Bravo, L. J. Villanueva-Rivera, and\nT. M. Aide. 2009. Automated classification of bird and amphibian calls using\nmachine learning: A comparison of methods. Ecological Informatics 4:206214.\nAgranat, I. 2009. Automatically Identifying Animal Species from their Vocalizations.\nFifth International Conference on Bio-Acoustics, Holywell Park.\nAkhtar, M., E. Ambikairajah, and J. Epps. 2007. GMM-Based Classification of\nGenomic Sequences. Pages 103 - 106. The International Conference on Digital\nSignal Processing.\nAnderson, S., A. Dave, and D. Margoliash. 1996. Template-based automatic\nrecognition of birdsong syllables from continuous recordings. Journal of the\nAcoustical Society of America 100:1209-1219.\nBelcher, C., M. Jones, and S. Burnett 2008. Spotted-tailed quoll, Dasyurus maculatus.\nNew Holland Publishers.\nBrandes, S. T. 2008. Automated sound recording and analysis techniques for bird\nsurveys and conservation. Bird Conservation International 18:S163-S173.\nBrandes, T., P. Naskrecki, and H. Figueroa. 2006. Using image processing to detect\nand classify narrow-band cricket and frog calls. The Journal of the Acoustical\nSociety of America 120:2950-2957.\nBrunelli, R. 2009. Template matching techniques in computer vision: theory and\npractice John Wiley & Sons, West Sussex.\nBunke, H., and A. Sanfeliu, editors. 1990. Syntactic and structural pattern recognition\n– theory and applications. World Scientific Publishing Co, Singapore.38\nCai, J., D. Ee, B. Pham, P. Roe, and J. Zhang. 2007. Sensor network for the\nmonitoring of ecosystem: Bird species recognition. Pages 293–298. Third\nInternational Conference on Intelligent Sensors, Sensor Networks and\nInformation Processing. Citeseer, Melbourne.\nCatchpole, C., and P. Slater 1995. Bird song: Biological themes and variations. Press\nSyndicate University of Cambridge, Cambridge.\nChen, Z., and R. Maher. 2006. Semi-automatic classification of bird vocalizations\nusing spectral peak tracks. The Journal of the Acoustical Society of America\n120:2974.\nCowling, M., and R. Sitte. 2003. Comparison of techniques for environmental sound\nrecognition. Pattern Recognition Letters 24:2895-2907.\nFitzGibbon, S., W. Ellis, and F. Carrick. 2009. Mines, farms, koalas and GPS-loggers:\nassessing the ecological value of riparian vegetation in central Queensland.\nPage Poster Presentation. The 10th International Congress of Ecology.\nJuang, C., and T. Chen. 2007. Birdsong recognition using prediction-based recurrent\nneural fuzzy networks. Neurocomputing 71:121-130.\nKwan, C., G. Mei, X. Zhao, Z. Ren, R. Xu, V. Stanford, C. Rochet, J. Aube, and K. C.\nHo. 2004. Bird classification algorithms: theory and experimental results.\nPages V-289-292 vol.285. IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, 2004. Proceedings. (ICASSP '04).\nLau, A., R. Mason, B. Pham, M. Richards, P. Roe, and J. Zhang. 2008. Monitoring\nthe environment through acoustics using smartphone-based sensors and 3G\nnetworking in K. Langendoen, editor. Proceedings of the Second International\nWorkshop on Wireless Sensor Network Deployments (WiDeploy08); 4th39\nIEEE International Conference on Distributed Computing in Sensor Systems,\nDCOSS 2008. IEEE.\nLüscher, E. 1951. The Difference Limen of Intensity Variations of Pure Tones and its\nDiagnostic Significance. The Journal of Laryngology & Otology 65:486-510.\nMarcellini, D. L. 1974. Acoustic Behavior of the Gekkonid Lizard, Hemidactylus\nfrenatus. Herpetologica 30:44-52.\nMcIlraith, A. L., and H. C. Card. 1997. Birdsong recognition using backpropagation\nand multivariate statistics. IEEE Transactions on Signal Processing 45:27402748.\nRickwood, P., and A. Taylor. 2008. Methods for automatically analyzing humpback\nsong units. Journal of the Acoustical Society of America 123:1763-1772.\nSomervuo, P., A. Harma, and S. Fagerlund. 2006. Parametric Representations of Bird\nSounds for Automatic Species Recognition. IEEE Transactions on Audio,\nSpeech, and Language Processing 14:2252-2263.\nTemko, A., R. Malkin, C. Zieger, D. Macho, C. Nadeu, and M. Omologo. 2006.\nAcoustic event detection and classification in smart-room environments:\nEvaluation of CHIL project systems. Cough 65:5-11.\nTowsey, M., and B. Planitz. 2010. Technical Report: Acoustic analysis of the\nnatural environment. Queensland University of Technology, Brisbane.\nTowsey, M., B. Planitz, J. Wimmer, and P. Roe. 2010. Animal Call Recognition\nUsing Oscillation Detection. Page (submitted). The 2nd Conference on\nEnvironmental Science and Information Application Technology (ISSNIP\n2010).40\nTrifa, V., A. Kirschel, C. Taylor, and E. Vallejo. 2008. Automated species recognition\nof antbirds in a Mexican rainforest using hidden Markov models. The Journal\nof the Acoustical Society of America 123:2424-2431.\nWilliamson, I., S. Fuller, and C. Marston. 2008. A Vertebrate Survey of the Samford\nEcological Research Facility. School of Natural Resource Sciences,\nQueensland University of Technology (QUT), Brisbane.\nWimmer, J., M. Towsey, B. Planitz, and P. Roe. 2010. Scaling Acoustic Data\nAnalysis through Collaboration and Automation. Page (submitted). IEEE\neScience 2010 Conference, Brisbane, Australia.\nYoung, S., G. Evermann, D. Kershaw, G. Moore, J. Odell, D. Ollason, V. Valtchev,\nand P. Woodland 2006. The HTK book (for HTK Version 3.4). Cambridge\nUniversity Engineering Dept., Cambridge.\nZhuang, X., Z. Xi, T. S. Huang, and M. Hasegawa-Johnson. 2008. Feature analysis\nand selection for acoustic event detection. Pages 17-20. Acoustics, Speech and\nSignal Processing, 2008. ICASSP 2008. IEEE International Conference on.",
    "affiliations": [
      {
        "country": "Australia",
        "discipline": "Other",
        "university": "Queensland University of Technology"
      }
    ],
    "species_categories": [
      "Bird",
      "Terrestrial Mammal",
      "Other"
    ],
    "specialized_species": [
      "Currawong",
      "Curlew",
      "Male Koala",
      "Cane Toad",
      "Asian House Gecko",
      "Ground Parrot",
      "Whipbird",
      "Crow"
    ],
    "computational_stages": [
      "Data Collection",
      "Meaning Identification"
    ],
    "linguistic_features": [
      "Semanticity",
      "Discreteness and Syntax"
    ],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886036",
    "updated_at": "2026-02-10T20:17:11.684378"
  },
  {
    "id": "707830aa-da06-4e6c-94b7-b2dac3f93de8",
    "title": "Automated bird acoustic event detection and robust species classification",
    "authors": [
      "Zhao, Zhao",
      "Zhang, Sai-hua",
      "Xu, Zhi-yong",
      "Bellisario, Kristen",
      "Dai, Nian-hua",
      "Omrani, Hichem",
      "Pijanowski, Bryan C"
    ],
    "year": "2017",
    "journal": "Ecological Informatics",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886040"
  },
  {
    "id": "7db5b47d-a9ab-412b-a58d-940549893bd0",
    "title": "Simultaneous segmentation and classification of bird song using CNN",
    "authors": [
      "Narasimhan, Revathy",
      "Fern, Xiaoli Z",
      "Raich, Raviv"
    ],
    "year": "2017",
    "journal": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886044"
  },
  {
    "id": "da316ee0-771d-498c-8341-8ebfb5c7a869",
    "title": "Automatic bird song and syllable segmentation with an open-source deep-learning object detection method--a case study in the Collared Flycatcher (Ficedula albicollis)",
    "authors": [
      "Zseb{\\H{o}}k, S{\\'a}ndor",
      "Nagy-Egri, M{\\'a}t{\\'e} Ferenc",
      "Barnaf{\\\"o}ldi, Gergely G{\\'a}bor",
      "Laczi, Mikl{\\'o}s",
      "Nagy, Gergely",
      "Vaskuti, Eva",
      "Garamszegi, L{\\'a}szl{\\'o} Zsolt"
    ],
    "year": "2019",
    "journal": "Ornis Hungarica",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886048"
  },
  {
    "id": "9109de91-b82c-4ed1-97e7-22315317ccc5",
    "title": "Multi-channel environmental sound segmentation utilizing sound source localization and separation U-Net",
    "authors": [
      "Sudo, Yui",
      "Itoyama, Katsutoshi",
      "Nishida, Kenji",
      "Nakadai, Kazuhiro"
    ],
    "year": "2021",
    "journal": "2021 IEEE/SICE International Symposium on System Integration (SII)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886052"
  },
  {
    "id": "47f7205d-8bef-46aa-bad1-d58bb74751a3",
    "title": "Towards lexical analysis of dog vocalizations via online videos",
    "authors": [
      "Wang, Yufei",
      "Zhang, Chunhao",
      "Huang, Jieyi",
      "Wu, Mengyue",
      "Zhu, Kenny"
    ],
    "year": "2023",
    "journal": "arXiv preprint arXiv:2309.13086",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886056"
  },
  {
    "id": "55fc3477-f50a-4282-b42e-28d557cf7a3a",
    "title": "Vocal labeling of others by nonhuman primates",
    "authors": [
      "Oren, Guy",
      "Shapira, Aner",
      "Lifshitz, Reuven",
      "Vinepinsky, Ehud",
      "Cohen, Roni",
      "Fried, Tomer",
      "Hadad, Guy P",
      "Omer, David"
    ],
    "year": "2024",
    "journal": "Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886110"
  },
  {
    "id": "79c4b30b-8728-4553-8693-21a0e6a67987",
    "title": "Species-independent analysis and identification of emotional animal vocalizations",
    "authors": [
      "Ntalampiras, Stavros"
    ],
    "year": "2025",
    "journal": "Scientific Reports",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886115"
  },
  {
    "id": "11986ad6-1d3c-4c5d-8693-b43d07a7c77b",
    "title": "Toward Automatic Discovery of a Canine Phonetic Alphabet",
    "authors": [
      "Wang, Theron S",
      "Li, Xingyuan",
      "Lekhak, Hridayesh",
      "Dang, Tuan Minh",
      "Wu, Mengyue",
      "Zhu, Kenny"
    ],
    "year": "2025",
    "journal": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886122"
  },
  {
    "id": "f4253770-2c6a-4a40-96b3-8620ccf31d3c",
    "title": "Speech perception by the chinchilla: Voiced-voiceless distinction in alveolar plosive consonants",
    "authors": [
      "Kuhl, Patricia K",
      "Miller, James D"
    ],
    "year": "1975",
    "journal": "Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886126"
  },
  {
    "id": "7d831f38-6548-4ece-b3e3-f14f2560cc36",
    "title": "On the evolution of language and generativity",
    "authors": [
      "Corballis, Michael C"
    ],
    "year": "1992",
    "journal": "Cognition",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886130"
  },
  {
    "id": "bde5f8c6-74e6-4678-a09e-5793e948e0fa",
    "title": "Partially observable Markov models inferred using statistical tests reveal context-dependent syllable transitions in Bengalese finch songs",
    "authors": [
      "Lu, Jiali",
      "Surendralal, Sumithra",
      "Bouchard, Kristofer E",
      "Jin, Dezhe Z"
    ],
    "year": "2025",
    "journal": "Journal of Neuroscience",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886133"
  },
  {
    "id": "71a42ce6-fdb7-4ab4-8720-9f02105ebde1",
    "title": "Bidirectional generative adversarial representation learning for natural stimulus synthesis",
    "authors": [
      "Reilly, Johnny",
      "Goodwin, John D",
      "Lu, Sihao",
      "Kozlov, Andriy S"
    ],
    "year": "2024",
    "journal": "Journal of Neurophysiology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886137"
  },
  {
    "id": "6ee6470b-e0b5-497e-9231-fb7cb6342d2b",
    "title": "ECOGEN: Bird sounds generation using deep learning",
    "authors": [
      "Guei, Axel-Christian",
      "Christin, Sylvain",
      "Lecomte, Nicolas",
      "Hervet, {\\'E}ric"
    ],
    "year": "2024",
    "journal": "Methods in Ecology and Evolution",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886143"
  },
  {
    "id": "b70e5eaa-854d-4509-98d9-0ff982a3a007",
    "title": "The faculty of language: what's special about it?",
    "authors": [
      "Pinker, Steven",
      "Jackendoff, Ray"
    ],
    "year": "2005",
    "journal": "Cognition",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886169"
  },
  {
    "id": "2829da25-028c-4b25-af48-66400a883d63",
    "title": "A forkhead-domain gene is mutated in a severe speech and language disorder",
    "authors": [
      "Lai, Cecilia SL",
      "Fisher, Simon E",
      "Hurst, Jane A",
      "Vargha-Khadem, Faraneh",
      "Monaco, Anthony P"
    ],
    "year": "2001",
    "journal": "Nature",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886175"
  },
  {
    "id": "851a07e1-7b50-4b0f-804b-de5a45b38781",
    "title": "Molecular evolution of FOXP2, a gene involved in speech and language",
    "authors": [
      "Enard, Wolfgang",
      "Przeworski, Molly",
      "Fisher, Simon E",
      "Lai, Cecilia SL",
      "Wiebe, Victor",
      "Kitano, Takashi",
      "Monaco, Anthony P",
      "P{\\\"a}{\\\"a}bo, Svante"
    ],
    "year": "2002",
    "journal": "Nature",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886181"
  },
  {
    "id": "36616282-6b46-4722-b2b6-1c2cf0e66e3e",
    "title": "The Watkins marine mammal sound database: an online, freely accessible resource",
    "authors": [
      "Sayigh, Laela",
      "Daher, Mary Ann",
      "Allen, Julie",
      "Gordon, Helen",
      "Joyce, Katherine",
      "Stuhlmann, Claire",
      "Tyack, Peter"
    ],
    "year": "2016",
    "journal": "Proceedings of Meetings on Acoustics",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886185"
  },
  {
    "id": "a5361ac8-8f91-471d-9679-4afa102c5005",
    "title": "The Sarasota Dolphin Whistle Database: A unique long-term resource for understanding dolphin communication",
    "authors": [
      "Sayigh, Laela S",
      "Janik, Vincent M",
      "Jensen, Frants H",
      "Scott, Michael D",
      "Tyack, Peter L",
      "Wells, Randall S"
    ],
    "year": "2022",
    "journal": "Frontiers in Marine Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886190"
  },
  {
    "id": "3ee0c246-6bd9-4976-9476-3fae85f09e9e",
    "title": "A Public Dataset of Annotated Orcinus orca Acoustic Signals for Detection and Ecotype Classification",
    "authors": [
      "Palmer, KJ",
      "Cummings, Emma",
      "Dowd, Michael G",
      "Frasier, Kait",
      "Frazao, Fabio",
      "Harris, Alex",
      "Houweling, April",
      "Kanes, Jasper",
      "Kirsebom, Oliver S",
      "Klinck, Holger",
      "others"
    ],
    "year": "2025",
    "journal": "Scientific Data",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886195"
  },
  {
    "id": "e3badccd-3025-47f6-8e72-f5927fae68fe",
    "title": "Barking in domestic dogs: context specificity and individual identification",
    "authors": [
      "Yin, Sophia",
      "McCowan, Brenda"
    ],
    "year": "2004",
    "journal": "Animal behaviour",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886199"
  },
  {
    "id": "608b4803-417a-4e10-ad53-c805a2fab658",
    "title": "The vocal repertoire of the ringtailed lemur (Lemur catta)",
    "authors": [
      "Macedonia, Joseph M"
    ],
    "year": "1993",
    "journal": "Folia primatologica",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886203"
  },
  {
    "id": "9a33f330-0c93-465f-8a8f-03181c1c5bba",
    "title": "The vocal repertoire of Barbary macaques: a quantitative analysis of a graded signal system",
    "authors": [
      "Hammerschmidt, Kurt",
      "Fischer, Julia"
    ],
    "year": "1998",
    "journal": "Ethology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886207"
  },
  {
    "id": "520c73af-efbe-4ef2-9fec-055bd1813ae0",
    "title": "ChiroVox: a public library of bat calls",
    "authors": [
      "G{\\\"o}rf{\\\"o}l, Tam{\\'a}s",
      "Huang, Joe Chun-Chia",
      "Csorba, G{\\'a}bor",
      "Gy{\\H{o}}r{\\\"o}ssy, Dorottya",
      "Est{\\'o}k, P{\\'e}ter",
      "Kingston, Tigga",
      "Szabadi, Kriszta Lilla",
      "McArthur, Ellen",
      "Senawi, Juliana",
      "Furey, Neil M",
      "others"
    ],
    "year": "2022",
    "journal": "PeerJ",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886211"
  },
  {
    "id": "72d94b01-5d67-45c4-9c1e-b31d244830b3",
    "title": "The audio-visual batvision dataset for research on sight and sound",
    "authors": [
      "Brunetto, Amandine",
      "Hornauer, Sascha",
      "Stella, X Yu",
      "Moutarde, Fabien"
    ],
    "year": "2023",
    "journal": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886219"
  },
  {
    "id": "5fc25f79-2485-47cd-a188-453d52c374d0",
    "title": "eBird: A citizen-based bird observation network in the biological sciences",
    "authors": [
      "Sullivan, Brian L",
      "Wood, Christopher L",
      "Iliff, Marshall J",
      "Bonney, Rick E",
      "Fink, Daniel",
      "Kelling, Steve"
    ],
    "year": "2009",
    "journal": "Biological conservation",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886224"
  },
  {
    "id": "c4342c38-3428-4abe-a4bb-c799f6e970e6",
    "title": "HumBugDB: a large-scale acoustic mosquito dataset",
    "authors": [
      "Kiskin, Ivan",
      "Sinka, Marianne",
      "Cobb, Adam D",
      "Rafique, Waqas",
      "Wang, Lawrence",
      "Zilli, Davide",
      "Gutteridge, Benjamin",
      "Dam, Rinita",
      "Marinos, Theodoros",
      "Li, Yunpeng",
      "others"
    ],
    "year": "2021",
    "journal": "arXiv preprint arXiv:2110.07607",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886228"
  },
  {
    "id": "a5eba97f-881c-48f5-9748-f1ce7529c539",
    "title": "Audio set: An ontology and human-labeled dataset for audio events",
    "authors": [
      "Gemmeke, Jort F",
      "Ellis, Daniel PW",
      "Freedman, Dylan",
      "Jansen, Aren",
      "Lawrence, Wade",
      "Moore, R Channing",
      "Plakal, Manoj",
      "Ritter, Marvin"
    ],
    "year": "2017",
    "journal": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886232"
  },
  {
    "id": "6166a752-bdae-49bd-9652-4ef2b94aa837",
    "title": "The animal sound archive at the Humboldt-University of Berlin: Current activities in conservation and improving access for bioacoustic research",
    "authors": [
      "Frommolt, Karl-Heinz",
      "Bardeli, Rolf",
      "Kurth, Frank",
      "Clausen, Michael"
    ],
    "year": "2006",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886236"
  },
  {
    "id": "a2542328-29da-4620-9179-b88aca6d53b5",
    "title": "Classification of dog barks: a machine learning approach",
    "authors": [
      "Moln{\\'a}r, Csaba",
      "Kaplan, Fr{\\'e}d{\\'e}ric",
      "Roy, Pierre",
      "Pachet, Fran{\\c{c}}ois",
      "Pongr{\\'a}cz, P{\\'e}ter",
      "D{\\'o}ka, Antal",
      "Mikl{\\'o}si, {\\'A}d{\\'a}m"
    ],
    "year": "2008",
    "journal": "Animal Cognition",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886240"
  },
  {
    "id": "a1bb8788-c8d7-4634-b002-dfb73e9b175e",
    "title": "Macaulay Library",
    "authors": [
      "{Cornell Lab of Ornithology}"
    ],
    "year": "2025",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886245"
  },
  {
    "id": "bb5616d4-a6b8-424d-87fa-0403f7c37267",
    "title": "An Annotated Set of Audio Recordings of Eastern North American Birds Containing Frequency",
    "authors": [
      "Chronister, Lauren M",
      "Rhinehart, Tessa A",
      "Place, Aidan",
      "Kitzes, Justin"
    ],
    "year": "2021",
    "journal": "Time, and Species Information",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886252"
  },
  {
    "id": "4f2d517c-e641-4c96-a0c4-8b9793e33ff9",
    "title": "AmphibiaWeb: Information on amphibian biology and conservation",
    "authors": [
      "AmphibiaWeb"
    ],
    "year": "2002",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886260"
  },
  {
    "id": "e4c8a121-26be-4c29-86bd-77ef8aa609fa",
    "title": "The animal diversity web",
    "authors": [
      "Myers, Phil",
      "Espinosa, Roger",
      "Parr, CS",
      "Jones, Tricia",
      "Hammond, GS",
      "Dewey, TA"
    ],
    "year": "2006",
    "journal": "Accessed October",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886267"
  },
  {
    "id": "c4529b9a-a73b-491b-91fe-318ad33bfa2a",
    "title": "Bug Bytes sound library: stored product insect pest sounds",
    "authors": [
      "Mankin, Richard"
    ],
    "year": "2019",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886271"
  },
  {
    "id": "13c50392-7777-47a4-8422-ba462b892271",
    "title": "Rainforest Connection Species Audio Detection",
    "authors": [
      "Bourhan Yassin",
      "inversion",
      "Mahreen Qazi",
      "Zephyr Gold"
    ],
    "year": "2020",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886277"
  },
  {
    "id": "2e6f8c7b-3a0b-4ed8-9b67-f911dc5f6661",
    "title": "DCASE Challenge 2024",
    "authors": [
      "{DCASE Community}"
    ],
    "year": "2024",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886281"
  },
  {
    "id": "ee830a8b-140d-454d-8c9d-b022761ef247",
    "title": "Animal Sound Archive",
    "authors": [
      "{Museum f{\\\"u}r Naturkunde Berlin}"
    ],
    "year": "2017",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886286"
  },
  {
    "id": "59c72d34-6a7a-408d-9333-525d81fbb3b4",
    "title": "North American Bat Monitoring Program (NABat)",
    "authors": [
      "{U.S. Geological Survey}"
    ],
    "year": "2023",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886308"
  },
  {
    "id": "108dc2f1-ac78-4fbd-b88a-26e0a7dbcc35",
    "title": "xeno-canto: now for Orthoptera sounds from around the world",
    "authors": [
      "Vellinga, Willem-Pier",
      "Planqu{\\'e}, Robert"
    ],
    "year": "2022",
    "journal": "Journal of Orthoptera Research",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886313"
  },
  {
    "id": "637ede05-ed3c-4f78-8fe4-c90f9bdfa28a",
    "title": "A library of easily downloadable datasets with animal vocalizations",
    "authors": [
      "{Earth Species Project}"
    ],
    "year": "2023",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886317"
  },
  {
    "id": "7eef193e-5f9f-4e1b-8b1e-8813589a9222",
    "title": "Website of Office of Laboratory Animal Welfare",
    "authors": [
      "{National Institutes of Health}"
    ],
    "year": "",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886321"
  },
  {
    "id": "b9a0037e-f685-4d94-b370-8dc7c8fc5515",
    "title": "University Research Compliance",
    "authors": [
      "{Oklahoma State University}"
    ],
    "year": "",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886325"
  },
  {
    "id": "62cf3690-8f30-4377-9204-5cf2ed6b132d",
    "title": "Mechanisms of American English vowel production in a grey parrot (Psittacus erithacus)",
    "authors": [
      "Warren, Denice K",
      "Patterson, Dianne K",
      "Pepperberg, Irene M"
    ],
    "year": "1996",
    "journal": "The Auk",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886329"
  },
  {
    "id": "ebecc2b0-a608-4962-a4ae-fc5c0bdff8b0",
    "title": "Primate vocal anatomy and physiology: Similarities and differences between humans and nonhuman primates",
    "authors": [
      "Nishimura, Takeshi"
    ],
    "year": "2020",
    "journal": "The origins of language revisited: differentiation from music and the emergence of neurodiversity and autism",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886333"
  },
  {
    "id": "2ef72a50-d952-4977-8927-e4cf0fad821a",
    "title": "Can macaques perceive place of articulation from formant transition information?",
    "authors": [
      "Sinnott, Joan M",
      "Williamson, Trina L"
    ],
    "year": "1999",
    "journal": "The Journal of the Acoustical Society of America",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886338"
  },
  {
    "id": "8c409159-e1c6-4da3-bd28-5cbea5711aeb",
    "title": "Explained: Generative AI’s environmental impact",
    "authors": [
      "Zewe, Adam"
    ],
    "year": "2025",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886342"
  },
  {
    "id": "01b9160a-7797-4080-86a2-dcdd9ab5af87",
    "title": "How AI infrastructure is driving a sharp rise in electricity bills",
    "authors": [
      "Bennet, Geoff"
    ],
    "year": "2025",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886345"
  },
  {
    "id": "999bb295-ea22-43a3-8a24-e2b93e5c44ec",
    "title": "Koe: Web-based software to classify acoustic units and analyse sequence structure in animal vocalizations",
    "authors": [
      "Fukuzawa, Yukio",
      "Webb, Wesley H",
      "Pawley, Matthew DM",
      "Roper, Michelle M",
      "Marsland, Stephen",
      "Brunton, Dianne H",
      "Gilman, Andrew"
    ],
    "year": "2020",
    "journal": "Methods in Ecology and Evolution",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886420"
  },
  {
    "id": "aad5e29a-b9ec-4676-9755-9e11c3873dea",
    "title": "Experimental evidence for compositional syntax in bird calls",
    "authors": [
      "Suzuki, Toshitaka N",
      "Wheatcroft, David",
      "Griesser, Michael"
    ],
    "year": "2016",
    "journal": "Nature communications",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886426"
  },
  {
    "id": "d82cfc91-e380-454f-a7e3-bf8eb5de2ead",
    "title": "Voice analysis in dogs with deep learning: development of a fully automatic voice analysis system for bioacoustics studies",
    "authors": [
      "Karaaslan, Mahmut",
      "Turkoglu, Bahaeddin",
      "Kaya, Ersin",
      "Asuroglu, Tunc"
    ],
    "year": "2024",
    "journal": "Sensors (Basel, Switzerland)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886430"
  },
  {
    "id": "610e69bd-b0ee-4102-8f72-181ee35939b9",
    "title": "Machine learning approach regarding the classification and prediction of dog sounds: a case study of South Indian breeds",
    "authors": [
      "Mohandas, Prabu",
      "Anni, Jerline Sheebha",
      "Hasikin, Khairunnisa",
      "Velauthapillai, Dhayalan",
      "Raj, Veena",
      "Murugathas, Thanihaichelvan",
      "Azizan, Muhammad Mokhzaini",
      "Thanasekaran, Rajkumar"
    ],
    "year": "2022",
    "journal": "Applied Sciences",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886434"
  },
  {
    "id": "475b9345-8a70-4dc5-8d64-5884d75c3123",
    "title": "Assessment of the Emotional State in Domestic Dogs using a Bi-dimensional Model of Emotions and a Machine Learning Approach for the Analysis of its Vocalizations",
    "authors": [
      "P{\\'e}rez-Espinosa, Humberto",
      "Reyes-Meza, Ver{\\'o}nica",
      "de Lourdes Arteaga-Castaneda, Mar{\\i}a",
      "Espinosa-Curiel, Ismael",
      "Bautista, Amando",
      "Mart{\\i}nez-Miranda, Juan"
    ],
    "year": "2017",
    "journal": "Advances on Language \\& Knowledge Engineering",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886466"
  },
  {
    "id": "f7d8e06c-9cca-4fdd-a844-817437283700",
    "title": "Whale song shows language-like statistical structure",
    "authors": [
      "Arnon, Inbal",
      "Kirby, Simon",
      "Allen, Jenny A",
      "Garrigue, Claire",
      "Carroll, Emma L",
      "Garland, Ellen C"
    ],
    "year": "2025",
    "journal": "Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886475"
  },
  {
    "id": "65f5c4e9-39e6-4865-8828-d33615cba4c0",
    "title": "Deep representation learning for orca call type classification",
    "authors": [
      "Bergler, Christian",
      "Schmitt, Manuel",
      "Cheng, Rachael Xi",
      "Schr{\\\"o}ter, Hendrik",
      "Maier, Andreas",
      "Barth, Volker",
      "Weber, Michael",
      "N{\\\"o}th, Elmar"
    ],
    "year": "2019",
    "journal": "International Conference on Text, Speech, and Dialogue",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886482"
  },
  {
    "id": "0909398d-3846-47df-b730-316f37a1c0fe",
    "title": "ORCA-WHISPER: An Automatic Killer Whale Sound Type Generation Toolkit Using Deep Learning.",
    "authors": [
      "Bergler, Christian",
      "Barnhill, Alexander",
      "Perrin, Dominik",
      "Schmitt, Manuel",
      "Maier, Andreas K",
      "N{\\\"o}th, Elmar"
    ],
    "year": "2022",
    "journal": "INTERSPEECH",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886503"
  },
  {
    "id": "888181dc-c32a-43cc-a35d-07af18ff7079",
    "title": "Open-source machine learning BANTER acoustic classification of beaked whale echolocation pulses",
    "authors": [
      "Rankin, Shannon",
      "Sakai, Taiki",
      "Archer, Frederick I",
      "Barlow, Jay",
      "Cholewiak, Danielle",
      "DeAngelis, Annamaria I",
      "McCullough, Jennifer LK",
      "Oleson, Erin M",
      "Simonis, Anne E",
      "Soldevilla, Melissa S",
      "others"
    ],
    "year": "2024",
    "journal": "Ecological Informatics",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886517"
  },
  {
    "id": "2f1f2848-7d13-44d3-94db-722a7b093622",
    "title": "An HMM-DNN-based system for the detection and classification of low-frequency acoustic signals from baleen whales, earthquakes, and air guns off Chile",
    "authors": [
      "Buchan, Susannah J",
      "Duran, Miguel",
      "Rojas, Constanza",
      "Wuth, Jorge",
      "Mahu, Rodrigo",
      "Stafford, Kathleen M",
      "Becerra Yoma, Nestor"
    ],
    "year": "2023",
    "journal": "Remote Sensing",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886522"
  },
  {
    "id": "6eeb3cac-73cb-40e1-b0e1-44191a697b7e",
    "title": "Automatic detection of unidentified fish sounds: A comparison of traditional machine learning with deep learning",
    "authors": [
      "Mouy, Xavier",
      "Archer, Stephanie K",
      "Dosso, Stan",
      "Dudas, Sarah",
      "English, Philina",
      "Foord, Colin",
      "Halliday, William",
      "Juanes, Francis",
      "Lancaster, Darienne",
      "Van Parijs, Sofie",
      "others"
    ],
    "year": "2024",
    "journal": "Frontiers in Remote Sensing",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886527"
  },
  {
    "id": "3de6cbdf-d679-4111-af11-604dd9451742",
    "title": "Fish Acoustic Detection Algorithm Research: a deep learning app for Caribbean grouper calls detection and call types classification",
    "authors": [
      "Ibrahim, Ali K",
      "Zhuang, Hanqi",
      "Sch{\\\"a}rer-Umpierre, Michelle",
      "Woodward, Caroline",
      "Erdol, Nurgun",
      "Cherubin, Laurent M"
    ],
    "year": "2024",
    "journal": "Frontiers in Marine Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886531"
  },
  {
    "id": "ef776fc4-b96d-471d-af9f-660092d2d6c3",
    "title": "Computer vision for bioacoustics: Detection of bearded seal vocalizations in the Chukchi shelf using YOLOV5",
    "authors": [
      "Escobar-Amado, Christian",
      "Badiey, Mohsen",
      "Wan, Lin"
    ],
    "year": "2023",
    "journal": "IEEE Journal of Oceanic Engineering",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886539"
  },
  {
    "id": "78a43c1d-a579-4479-972d-1f9cc6456ab0",
    "title": "Automatic detection and classification of bearded seal vocalizations in the northeastern Chukchi Sea using convolutional neural networks",
    "authors": [
      "Escobar-Amado, Christian",
      "Badiey, Mohsen",
      "Pecknold, Sean",
      "others"
    ],
    "year": "2022",
    "journal": "The Journal of the Acoustical Society of America",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886543"
  },
  {
    "id": "76db3f5d-aa38-46e3-9659-1e1451883efd",
    "title": "Deep learning and computer vision algorithms for detection and classification of bearded seal vocalizations in the Arctic Ocean",
    "authors": [
      "Escobar-Amado, Christian David"
    ],
    "year": "2022",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886547"
  },
  {
    "id": "fe491773-def7-4514-984d-25415da21701",
    "title": "Automatic parameter estimation and detection of Saimaa ringed seal knocking vocalizations",
    "authors": [
      "Solana, Adri{\\`a}",
      "Houegnigan, Ludwig",
      "Nadeu, Climent",
      "Young, Mairi",
      "Kunnasranta, Mervi"
    ],
    "year": "2024",
    "journal": "bioRxiv",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886552"
  },
  {
    "id": "b6dbd8e2-cf9b-43fb-9729-2fd81ea27d19",
    "title": "ORCA-SPY enables killer whale sound source simulation, detection, classification and localization using an integrated deep learning-based segmentation",
    "authors": [
      "Hauer, Christopher",
      "N{\\\"o}th, Elmar",
      "Barnhill, Alexander",
      "Maier, Andreas",
      "Guthunz, Julius",
      "Hofer, Heribert",
      "Cheng, Rachael Xi",
      "Barth, Volker",
      "Bergler, Christian"
    ],
    "year": "2023",
    "journal": "Scientific Reports",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886556"
  },
  {
    "id": "82555bdf-09a1-47ef-910e-9fcd4e94a41d",
    "title": "TweetyBERT: Automated parsing of birdsong through self-supervised machine learning",
    "authors": [
      "Vengrovski, George",
      "Hulsey-Vincent, Miranda R",
      "Bemrose, Melissa A",
      "Gardner, Timothy J"
    ],
    "year": "2025",
    "journal": "bioRxiv",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886560"
  },
  {
    "id": "cbb5fd92-af82-42ac-bed8-e7e42a6b86fa",
    "title": "FinchGPT: a Transformer based language model for birdsong analysis",
    "authors": [
      "Kobayashi, Kosei",
      "Matsuzaki, Kosuke",
      "Taniguchi, Masaya",
      "Sakaguchi, Keisuke",
      "Inui, Kentaro",
      "Abe, Kentaro"
    ],
    "year": "2025",
    "journal": "arXiv preprint arXiv:2502.00344",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886564"
  },
  {
    "id": "98c6f26e-5bf7-4e7e-a274-72f744198d2d",
    "title": "Measuring context dependency in birdsong using artificial neural networks",
    "authors": [
      "Morita, Takashi",
      "Koda, Hiroki",
      "Okanoya, Kazuo",
      "Tachibana, Ryosuke O"
    ],
    "year": "2021",
    "journal": "PLoS computational biology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886568"
  },
  {
    "id": "91b89d35-e491-4a74-9cf3-de5a7d41ca4a",
    "title": "Impact of transfer learning methods and dataset characteristics on generalization in birdsong classification",
    "authors": [
      "Ghani, Burooj",
      "Kalkman, Vincent J",
      "Planqu{\\'e}, Bob",
      "Vellinga, Willem-Pier",
      "Gill, Lisa",
      "Stowell, Dan"
    ],
    "year": "2025",
    "journal": "Scientific Reports",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886573"
  },
  {
    "id": "15c826a7-335d-4f53-82b4-734d7c5e4a26",
    "title": "Multi-Scale Deep Feature Fusion with Machine Learning Classifier for Birdsong Classification",
    "authors": [
      "Li, Wei",
      "Lv, Danju",
      "Yu, Yueyun",
      "Zhang, Yan",
      "Gu, Lianglian",
      "Wang, Ziqian",
      "Zhu, Zhicheng"
    ],
    "year": "2025",
    "journal": "Applied Sciences",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886580"
  },
  {
    "id": "720d23cb-b4b4-4b54-bcfd-6ac166a985f4",
    "title": "DuSAFNet: A Multi-Path Feature Fusion and Spectral--Temporal Attention-Based Model for Bird Audio Classification",
    "authors": [
      "Lu, Zhengyang",
      "Li, Huan",
      "Liu, Min",
      "Lin, Yibin",
      "Qin, Yao",
      "Wu, Xuanyu",
      "Xu, Nanbo",
      "Pu, Haibo"
    ],
    "year": "2025",
    "journal": "Animals",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886587"
  },
  {
    "id": "c0c32936-a09f-4a09-afda-daa9510cf2c4",
    "title": "Bird Vocalization Generation with Deep Learning",
    "authors": [
      "Shevtsov, Andrii"
    ],
    "year": "2025",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886594"
  },
  {
    "id": "7585cc88-ed9e-4c47-be1e-9d1d410fcf1f",
    "title": "Automated Annotation of Birdsong with a Neural Network That Segments Spectrograms",
    "authors": [
      "Cohen, Yarden",
      "Nicholson, David Aaron",
      "Sanchioni, Alexa",
      "Mallaber, Emily K",
      "Skidanova, Viktoriya",
      "Gardner, Timothy J"
    ],
    "year": "2022",
    "journal": "eLife",
    "abstract": "",
    "doi": "10.7554/eLife.63853",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886603"
  },
  {
    "id": "6f5fa0ca-dc50-4011-9406-0786067903a0",
    "title": "DISCO}}: {{A}} Deep Learning Ensemble for Uncertainty-Aware Segmentation of Acoustic Signals",
    "authors": [
      "Colligan, Thomas",
      "Irish, Kayla",
      "Emlen, Douglas J.",
      "Wheeler, Travis J."
    ],
    "year": "2023",
    "journal": "PLOS ONE",
    "abstract": "",
    "doi": "10.1371/journal.pone.0288172",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886611"
  },
  {
    "id": "9836d29b-1ce8-4579-9469-eaf991d699f1",
    "title": "Automated Segmentation of Linear Time-Frequency Representations of Marine-Mammal Sounds",
    "authors": [
      "Dadouchi, Florian",
      "Gervaise, Cedric",
      "Ioana, Cornel",
      "Huillery, Julien",
      "Mars, J{\\'e}r{\\^o}me I."
    ],
    "year": "2013",
    "journal": "The Journal of the Acoustical Society of America",
    "abstract": "",
    "doi": "10.1121/1.4816579",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886617"
  },
  {
    "id": "3fc1dc33-3435-4213-aa59-ecd9afe4797f",
    "title": "A Segmentation Algorithm for Zebra Finch Song at the Note Level",
    "authors": [
      "Du, Ping",
      "Troyer, Todd W."
    ],
    "year": "2006",
    "journal": "Neurocomputing",
    "abstract": "",
    "doi": "10.1016/j.neucom.2005.12.110",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886624"
  },
  {
    "id": "5479cb42-b949-4744-b2eb-0680932616ff",
    "title": "Consistent birdsong syllable segmentation using deep semi-supervised learning",
    "authors": [
      "Ghaffari Jadidi, Houtan",
      "Devos, Paul"
    ],
    "year": "2023",
    "journal": "Forum Acusticum 2023",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886630"
  },
  {
    "id": "60e5a44d-27eb-4cbf-9a07-d97466d40582",
    "title": "Acoustic Animal Identification Using Unsupervised Learning",
    "authors": [
      "Guerrero, Maria J.",
      "Bedoya, Carol L.",
      "L{\\'o}pez, Jos{\\'e} D.",
      "Daza, Juan M.",
      "Isaza, Claudia"
    ],
    "year": "2023",
    "journal": "Methods in Ecology and Evolution",
    "abstract": "",
    "doi": "10.1111/2041-210X.14103",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886637"
  },
  {
    "id": "7c5d92a6-71f0-45cf-bf7a-e27d6ab7d01a",
    "title": "Time-Frequency Segmentation of Bird Song in Noisy Acoustic Environments",
    "authors": [
      "Neal, Lawrence",
      "Briggs, Forrest",
      "Raich, Raviv",
      "Fern, Xiaoli Z."
    ],
    "year": "2011",
    "journal": "2011 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})",
    "abstract": "",
    "doi": "10.1109/ICASSP.2011.5946906",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886641"
  },
  {
    "id": "af8ddc99-2573-42ee-b8bb-38558a1bfdc0",
    "title": "Machine learning algorithms can predict emotional valence across ungulate vocalizations",
    "authors": [
      "Lef{\\`e}vre, Romain A",
      "Sypherd, Ciara CR",
      "Briefer, {\\'E}lodie F"
    ],
    "year": "2025",
    "journal": "iScience",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886645"
  },
  {
    "id": "a3a25322-7d78-4b05-8b26-74a12065393e",
    "title": "orcAI: A Machine Learning Tool to Detect and Classify Acoustic Signals of Killer Whales in Audio Recordings",
    "authors": [
      "Bonhoeffer, Sebastian",
      "Selbmann, Anna",
      "Angst, Daniel C",
      "Ochsner, Nicolas",
      "Miller, Patrick JO",
      "Samarra, Filipa IP",
      "Baumgartner, Ch{\\'e}rine D"
    ],
    "year": "2025",
    "journal": "Marine Mammal Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886652"
  },
  {
    "id": "7145c614-270c-486c-92fa-29cd2f789826",
    "title": "Machine learning to predict killer whale (Orcinus orca) behaviors using partially labeled vocalization data",
    "authors": [
      "Sandholm, Sophia"
    ],
    "year": "2025",
    "journal": "Frontiers in Marine Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886655"
  },
  {
    "id": "0f2e3d4f-154f-418e-ac5d-f385e49f4953",
    "title": "Multiclass cnn approach for automatic classification of dolphin vocalizations",
    "authors": [
      "Di Nardo, Francesco",
      "De Marco, Rocco",
      "Li Veli, Daniel",
      "Screpanti, Laura",
      "Castagna, Benedetta",
      "Lucchetti, Alessandro",
      "Scaradozzi, David"
    ],
    "year": "2025",
    "journal": "Sensors",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886660"
  },
  {
    "id": "c85d3eb2-8278-4f08-8a48-c8bc7f91960e",
    "title": "Elephant sound classification using deep learning optimization",
    "authors": [
      "Dewmini, Hiruni",
      "Meedeniya, Dulani",
      "Perera, Charith"
    ],
    "year": "2025",
    "journal": "Sensors",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886666"
  },
  {
    "id": "c991e0a2-16cc-4422-8f5b-8317a8931a43",
    "title": "A Comprehensive Meta-Analysis on Animal Identification Using Machine Learning and Deep Learning",
    "authors": [
      "Vijayalakshmi, A",
      "Shanmugavadivu, P",
      "Vijayalakshmi, S",
      "Sivaranjani, R"
    ],
    "year": "2024",
    "journal": "International Conference on Data Engineering and Communication Technology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886670"
  },
  {
    "id": "82d70522-701e-4807-8df3-782533699497",
    "title": "VoCallBase: A web-based platform for annotating and standardizing animal vocalizations",
    "authors": [
      "{Evolving Language NCCR}"
    ],
    "year": "2024",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886674"
  },
  {
    "id": "28d85d21-9c60-4bf5-bf09-9c334a215104",
    "title": "What can animal communication teach us about human language?",
    "authors": [
      "Fishbein, Adam R",
      "Fritz, Jonathan B",
      "Idsardi, William J",
      "Wilkinson, Gerald S"
    ],
    "year": "2020",
    "journal": "Philosophical Transactions of the Royal Society B",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886681"
  },
  {
    "id": "2bbfd134-ccce-4107-acad-d7a45372fcdb",
    "title": "Long-distance, low-frequency elephant communication",
    "authors": [
      "Garstang, Michael"
    ],
    "year": "2004",
    "journal": "Journal of Comparative Physiology A",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886684"
  },
  {
    "id": "e29a561d-3453-4fb6-bc1b-fbd22c453e39",
    "title": "Vocal communication in African elephants (Loxodonta africana)",
    "authors": [
      "Soltis, Joseph"
    ],
    "year": "2010",
    "journal": "Zoo Biology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886688"
  },
  {
    "id": "7de627ad-aec8-4764-a361-e097e7a7dbff",
    "title": "Neural mechanisms for lexical processing in dogs",
    "authors": [
      "Andics, Attila",
      "G{\\'a}bor, Anna",
      "G{\\'a}csi, M{\\'a}rta",
      "Farag{\\'o}, Tam{\\'a}s",
      "Szab{\\'o}, D{\\'o}ra",
      "Mikl{\\'o}si, Ad{\\'a}m"
    ],
    "year": "2016",
    "journal": "Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886695"
  },
  {
    "id": "b68d9bef-de49-4109-bbf5-06a29bc8fe6a",
    "title": "Speech naturalness detection and language representation in the dog brain",
    "authors": [
      "Cuaya, Laura V",
      "Hern{\\'a}ndez-P{\\'e}rez, Ra{\\'u}l",
      "Boros, Marianna",
      "Deme, Andrea",
      "Andics, Attila"
    ],
    "year": "2022",
    "journal": "Elsevier",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886699"
  },
  {
    "id": "89ec3935-bedf-4280-8332-dbc9d807ce78",
    "title": "Profile of Dorothy L. Cheney and Robert M. Seyfarth",
    "authors": [
      "Jennifer Viegas"
    ],
    "year": "2018",
    "journal": "Proceedings of the National Academy of Sciences of the United States of America",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886703"
  },
  {
    "id": "9c6d9b5b-1f3b-4dce-97f2-90e8ba4b900a",
    "title": "The Mystery of Language Evolution",
    "authors": [
      "Yang, C",
      "Hauser, MD",
      "Berwick, RC",
      "Tattersall, I",
      "Ryan, MJ",
      "Watumull, J",
      "Chomsky, N",
      "Lewontin, RC"
    ],
    "year": "2014",
    "journal": "Frontiers in psychology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886710"
  },
  {
    "id": "e7df76a6-f407-4664-ac77-605e258f3ce6",
    "title": "Ancestry of the AUTS2 family--A novel group of polycomb-complex proteins involved in human neurological disease",
    "authors": [
      "Sellers, Robert A",
      "Robertson, David L",
      "Tassabehji, May"
    ],
    "year": "2020",
    "journal": "PLoS One",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886714"
  },
  {
    "id": "cddab331-b37d-410f-a388-5b6b84325f9d",
    "title": "Towards a new taxonomy of primate vocal production learning",
    "authors": [
      "Fischer, Julia",
      "Hammerschmidt, Kurt"
    ],
    "year": "2020",
    "journal": "Philosophical Transactions of the Royal Society B",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886718"
  },
  {
    "id": "7d39f3a2-1bfa-49d9-a21f-5ff9f057d22a",
    "title": "Becoming human: human infants link language and cognition, but what about the other great apes?",
    "authors": [
      "Novack, Miriam A",
      "Waxman, Sandra"
    ],
    "year": "2020",
    "journal": "Philosophical Transactions of the Royal Society B",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886724"
  },
  {
    "id": "0659e82d-09e0-4cd4-aca1-6065952ac8a4",
    "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
    "authors": [
      "Tarvainen, Antti",
      "Valpola, Harri"
    ],
    "year": "2017",
    "journal": "Advances in neural information processing systems",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886728"
  },
  {
    "id": "61ac6f5a-29ad-4443-af4f-75183edec1f6",
    "title": "Songs of Humpback Whales: Humpbacks emit sounds in long, predictable patterns ranging over frequencies audible to humans.",
    "authors": [
      "Payne, Roger S",
      "McVay, Scott"
    ],
    "year": "1971",
    "journal": "Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886736"
  },
  {
    "id": "037fe8f2-8c97-4fcc-9e6e-471dd4159bc6",
    "title": "Hierarchical temporal structure in music, speech and animal vocalizations: jazz is like a conversation, humpbacks sing like hermit thrushes",
    "authors": [
      "Kello, Christopher T",
      "Bella, Simone Dalla",
      "M{\\'e}d{\\'e}, Butovens",
      "Balasubramaniam, Ramesh"
    ],
    "year": "2017",
    "journal": "Journal of the Royal Society Interface",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886743"
  },
  {
    "id": "c5419821-bf0d-4b7e-8651-30b03afc12b6",
    "title": "Compensating class imbalance for acoustic chimpanzee detection with convolutional recurrent neural networks",
    "authors": [
      "Anders, Franz",
      "Kalan, Ammie K",
      "K{\\\"u}hl, Hjalmar S",
      "Fuchs, Mirco"
    ],
    "year": "2021",
    "journal": "Ecological Informatics",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886747"
  },
  {
    "id": "fb2504fc-ae5f-44c5-92a0-8e1389217236",
    "title": "A densely sampled and richly annotated acoustic data set from a wild bird population",
    "authors": [
      "Recalde, Nilo Merino",
      "Estand{\\'\\i}a, Andrea",
      "Pichot, Loanne",
      "Vansse, Antoine",
      "Cole, Ella F",
      "Sheldon, Ben C"
    ],
    "year": "2024",
    "journal": "Animal Behaviour",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886751"
  },
  {
    "id": "9be119b5-c707-4d05-95b2-6a3b0bbf92de",
    "title": "Birds, bats and beyond: Evaluating generalization in bioacoustics models",
    "authors": [
      "Van Merri{\\\"e}nboer, Bart",
      "Hamer, Jenny",
      "Dumoulin, Vincent",
      "Triantafillou, Eleni",
      "Denton, Tom"
    ],
    "year": "2024",
    "journal": "Frontiers in Bird Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886758"
  },
  {
    "id": "122f1505-501e-4f4e-acba-c3dba3b0582b",
    "title": "On the opportunities and risks of foundation models",
    "authors": [
      "Bommasani, Rishi"
    ],
    "year": "2021",
    "journal": "arXiv preprint arXiv:2108.07258",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886762"
  },
  {
    "id": "a2ba456b-707c-4d42-8e15-8d8cce33e115",
    "title": "Foundation Models for Bioacoustics--a Comparative Review",
    "authors": [
      "Schwinger, Raphael",
      "Zadeh, Paria Vali",
      "Rauch, Lukas",
      "Kurz, Mats",
      "Hauschild, Tom",
      "Lapp, Sam",
      "Tomforde, Sven"
    ],
    "year": "2025",
    "journal": "arXiv preprint arXiv:2508.01277",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886766"
  },
  {
    "id": "0638f13a-4847-4bdd-9850-a14687e8ae77",
    "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
    "authors": [
      "Baevski, Alexei",
      "Zhou, Yuhao",
      "Mohamed, Abdelrahman",
      "Auli, Michael"
    ],
    "year": "2020",
    "journal": "Advances in neural information processing systems",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886772"
  },
  {
    "id": "0bb11b44-e15a-444b-82f1-d2aff597d82c",
    "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
    "authors": [
      "Hsu, Wei-Ning",
      "Bolte, Benjamin",
      "Tsai, Yao-Hung Hubert",
      "Lakhotia, Kushal",
      "Salakhutdinov, Ruslan",
      "Mohamed, Abdelrahman"
    ],
    "year": "2021",
    "journal": "IEEE/ACM transactions on audio, speech, and language processing",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886776"
  },
  {
    "id": "168a526d-9f4e-4945-9ed5-23b015720b0a",
    "title": "Robust speech recognition via large-scale weak supervision",
    "authors": [
      "Radford, Alec",
      "Kim, Jong Wook",
      "Xu, Tao",
      "Brockman, Greg",
      "McLeavey, Christine",
      "Sutskever, Ilya"
    ],
    "year": "2023",
    "journal": "International conference on machine learning",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886781"
  },
  {
    "id": "4dde6143-10b9-4065-a08d-7bc91e352770",
    "title": "animal2vec and MeerKAT: A self-supervised transformer for rare-event raw audio input and a large-scale reference dataset for bioacoustics",
    "authors": [
      "Sch{\\\"a}fer-Zimmermann, Julian C",
      "Demartsev, Vlad",
      "Averly, Baptiste",
      "Dhanjal-Adams, Kiran",
      "Duteil, Mathieu",
      "Gall, Gabriella",
      "Fai{\\ss}, Marius",
      "Johnson-Ulrich, Lily",
      "Stowell, Dan",
      "Manser, Marta B",
      "others"
    ],
    "year": "2024",
    "journal": "arXiv preprint arXiv:2406.01253",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886788"
  },
  {
    "id": "2cfad0fd-479c-459c-8095-0f9248d791fa",
    "title": "BovineTalk: machine learning for vocalization analysis of dairy cattle under the negative affective state of isolation",
    "authors": [
      "Gavojdian, Dinu",
      "Mincu, Madalina",
      "Lazebnik, Teddy",
      "Oren, Ariel",
      "Nicolae, Ioana",
      "Zamansky, Anna"
    ],
    "year": "2024",
    "journal": "Frontiers in Veterinary Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886792"
  },
  {
    "id": "f3a9cc8d-ef7b-473b-95f6-b34f20bb4ac7",
    "title": "You talkin’to me? Interactive playback is a powerful yet underused tool in animal communication research",
    "authors": [
      "King, Stephanie L"
    ],
    "year": "2015",
    "journal": "Biology Letters",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886796"
  },
  {
    "id": "eca9f458-2e28-4e5d-8e81-9f965448b707",
    "title": "Characterization and assessment of vocalization responses of cows to different physiological states",
    "authors": [
      "Yoshihara, Yu",
      "Oya, Kosei"
    ],
    "year": "2021",
    "journal": "Journal of Applied Animal Research",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886802"
  },
  {
    "id": "9b2d3896-b560-4b36-8e0d-7dc98ec37e23",
    "title": "Behavioral effects of auditory stimulation on kenneled dogs",
    "authors": [
      "Kogan, Lori R",
      "Schoenfeld-Tacher, Regina",
      "Simon, Allen A"
    ],
    "year": "2012",
    "journal": "Journal of Veterinary Behavior",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886806"
  },
  {
    "id": "38722340-a70d-49ce-ad4d-da680a8bcb07",
    "title": "Causal cognition in a non-human primate: field playback experiments with Diana monkeys",
    "authors": [
      "Zuberb{\\\"u}hler, Klaus"
    ],
    "year": "2000",
    "journal": "Cognition",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886809"
  },
  {
    "id": "e92bfa19-d75d-4a47-81af-efff9a24c195",
    "title": "Evaluating the function of the male harbour seal, Phoca vitulina, roar through playback experiments",
    "authors": [
      "Hayes, Sean A",
      "Kumar, Anurag",
      "Costa, Daniel P",
      "Mellinger, David K",
      "Harvey, James T",
      "Southall, Brandon L",
      "Le Boeuf, Burney J"
    ],
    "year": "2004",
    "journal": "Animal Behaviour",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886816"
  },
  {
    "id": "8d25fe31-bcf4-437c-bc1a-52508600520e",
    "title": "Wild chimpanzees (Pan troglodytes schweinfurthii) distinguish between different scream types: evidence from a playback study",
    "authors": [
      "Slocombe, Katie Elizabeth",
      "Townsend, Simon W",
      "Zuberb{\\\"u}hler, Klaus"
    ],
    "year": "2009",
    "journal": "Animal cognition",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886857"
  },
  {
    "id": "ec9d7d84-50d7-46ac-a27e-2fbde16adc02",
    "title": "Turning the tables: a tiny bird uses alarm calls and mimicry to deceive its nest predator",
    "authors": [
      "Ascah, Lauren",
      "Igic, Branislav",
      "Magrath, Robert"
    ],
    "year": "2025",
    "journal": "Biology Letters",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886861"
  },
  {
    "id": "6c4e2d56-450b-42bf-98b5-657c95e83f22",
    "title": "Feeling anxious? The mechanisms of vocal deception in tufted capuchin monkeys",
    "authors": [
      "Kean, Donna",
      "Tiddi, Barbara",
      "Fahy, Martin",
      "Heistermann, Michael",
      "Schino, Gabriele",
      "Wheeler, Brandon C"
    ],
    "year": "2017",
    "journal": "Animal Behaviour",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887332"
  },
  {
    "id": "05a20b7a-ad3b-45fb-a2e0-84a196674cd9",
    "title": "Neural decipherment via minimum-cost flow: From Ugaritic to Linear B",
    "authors": [
      "Luo, Jiaming",
      "Cao, Yuan",
      "Barzilay, Regina"
    ],
    "year": "2019",
    "journal": "arXiv preprint arXiv:1906.06718",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887340"
  },
  {
    "id": "2e30168b-bbc9-4d37-877f-692dcff47ac1",
    "title": "Deciphering undersegmented ancient scripts using phonetic prior",
    "authors": [
      "Luo, Jiaming",
      "Hartmann, Frederik",
      "Santus, Enrico",
      "Barzilay, Regina",
      "Cao, Yuan"
    ],
    "year": "2021",
    "journal": "Transactions of the Association for Computational Linguistics",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887345"
  },
  {
    "id": "aaab177d-3eae-49c4-b33b-480e11bf61f0",
    "title": "A fast and accurate zebra finch syllable detector",
    "authors": [
      "Pearre, Ben",
      "Perkins, L Nathan",
      "Markowitz, Jeffrey E",
      "Gardner, Timothy J"
    ],
    "year": "2017",
    "journal": "PloS one",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887349"
  },
  {
    "id": "b772f758-b412-4bd5-a403-a525159ab834",
    "title": "Acoustic analysis of cattle (Bos taurus) mother--offspring contact calls from a source--filter theory perspective",
    "authors": [
      "de la Torre, M{\\'o}nica Padilla",
      "Briefer, Elodie F",
      "Reader, Tom",
      "McElligott, Alan G"
    ],
    "year": "2015",
    "journal": "Applied Animal Behaviour Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887354"
  },
  {
    "id": "f46099dd-036c-4ffa-b567-0b10e1ea9ed5",
    "title": "Vocal individuality of Holstein-Friesian cattle is maintained across putatively positive and negative farming contexts",
    "authors": [
      "Green, Alexandra",
      "Clark, Cameron",
      "Favaro, Livio",
      "Lomax, Sabrina",
      "Reby, David"
    ],
    "year": "2019",
    "journal": "Scientific Reports",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887362"
  },
  {
    "id": "8f08cf31-4396-4e1c-9deb-111c85c1ad13",
    "title": "Vocalization and other behaviors as indicators of emotional valence: The case of cow-calf separation and reunion in beef cattle",
    "authors": [
      "Schnaider, Maria A",
      "Heidemann, Marina S",
      "Silva, Adelaide HP",
      "Taconeli, C{\\'e}sar A",
      "Molento, Carla FM"
    ],
    "year": "2022",
    "journal": "Journal of Veterinary Behavior",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887366"
  },
  {
    "id": "05ebddd9-33f4-4333-a5cc-3d72b7d455ad",
    "title": "Multi Modal Information Fusion of Acoustic and Linguistic Data for Decoding Dairy Cow Vocalizations in Animal Welfare Assessment",
    "authors": [
      "Jobarteh, Bubacarr",
      "Mincu, Madalina",
      "Dinu, Gavojdian",
      "Neethirajan, Suresh"
    ],
    "year": "2024",
    "journal": "arXiv preprint arXiv:2411.00477",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887370"
  },
  {
    "id": "355811f6-0cf2-455a-a6c2-3c4b31ae6e25",
    "title": "Sound analysis in dairy cattle vocalisation as a potential welfare monitor",
    "authors": [
      "Meen, GH",
      "Schellekens, MA",
      "Slegers, MHM",
      "Leenders, NLG",
      "van Erp-van der Kooij, Elaine",
      "Noldus, Lucas PJJ"
    ],
    "year": "2015",
    "journal": "Computers and Electronics in Agriculture",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887374"
  },
  {
    "id": "093bf7d1-5025-4e3d-8ea6-cf2aa779651e",
    "title": "A context-aware method-based cattle vocal classification for livestock monitoring in smart farm",
    "authors": [
      "Sattar, Farook"
    ],
    "year": "2022",
    "journal": "Chemistry Proceedings",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887391"
  },
  {
    "id": "1cec138f-453a-4a9b-8566-c5351f34921e",
    "title": "On-farm detection of claw lesions in dairy cows based on acoustic analyses and machine learning",
    "authors": [
      "Volkmann, N",
      "Kulig, B",
      "Hoppe, S",
      "Stracke, J",
      "Hensel, O",
      "Kemper, N"
    ],
    "year": "2021",
    "journal": "Journal of dairy science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887397"
  },
  {
    "id": "5f0d4e7e-821e-4bff-b2c9-07176845f8ef",
    "title": "Japanese Black cattle call patterns classification using multiple acoustic features and machine learning models",
    "authors": [
      "Peng, Yingqi",
      "Kondo, Naoshi",
      "Fujiura, Tateshi",
      "Suzuki, Tetsuhito",
      "Yoshioka, Hidetsugu",
      "Itoyama, Erina",
      "others"
    ],
    "year": "2023",
    "journal": "Computers and Electronics in Agriculture",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887402"
  },
  {
    "id": "cb18130b-617e-440e-bb6d-8dd18cd06173",
    "title": "Integrating diverse data sources to predict disease risk in dairy cattle—a machine learning approach",
    "authors": [
      "Lasser, Jana",
      "Matzhold, Caspar",
      "Egger-Danner, Christa",
      "Fuerst-Waltl, Birgit",
      "Steininger, Franz",
      "Wittek, Thomas",
      "Klimek, Peter"
    ],
    "year": "2021",
    "journal": "Journal of Animal Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887407"
  },
  {
    "id": "43c718ca-e96b-403c-b865-a7d709213398",
    "title": "Cattle disease prediction using artificial intelligence",
    "authors": [
      "Rao, A",
      "Monika, HR",
      "Rakshitha, BC",
      "Thaseen, Seham"
    ],
    "year": "2023",
    "journal": "International Journal for Research in Applied Science \\& Engineering Technology (IJRASET)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887415"
  },
  {
    "id": "fc5f4dca-8f91-430b-96b0-14d4df7eebee",
    "title": "Machine learning approaches to predict and detect early-onset of digital dermatitis in dairy cows using sensor data",
    "authors": [
      "Magana, Jennifer",
      "Gavojdian, Dinu",
      "Menahem, Yakir",
      "Lazebnik, Teddy",
      "Zamansky, Anna",
      "Adams-Progar, Amber"
    ],
    "year": "2023",
    "journal": "Frontiers in Veterinary Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887419"
  },
  {
    "id": "2547b98d-83d1-4cda-be46-08eefd4c8ffb",
    "title": "Calming Hungarian Grey cattle in headlocks using processed nasal vocalization of a mother cow",
    "authors": [
      "Lenner, {\\'A}d{\\'a}m",
      "Papp, Zolt{\\'a}n Lajos",
      "Szab{\\'o}, Csaba",
      "Koml{\\'o}si, Istv{\\'a}n"
    ],
    "year": "2023",
    "journal": "Animals",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887423"
  },
  {
    "id": "ec3931fa-f66e-40fd-8d7b-7da09435278f",
    "title": "Analysis of sounds made by Bos taurus and Bubalus bubalis dams to their calves",
    "authors": [
      "Lenner, {\\'A}d{\\'a}m",
      "Papp, Zolt{\\'a}n Lajos",
      "Koml{\\'o}si, Istv{\\'a}n"
    ],
    "year": "2025",
    "journal": "Frontiers in Veterinary Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887430"
  }
]   "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886430"
  },
  {
    "id": "610e69bd-b0ee-4102-8f72-181ee35939b9",
    "title": "Machine learning approach regarding the classification and prediction of dog sounds: a case study of South Indian breeds",
    "authors": [
      "Mohandas, Prabu",
      "Anni, Jerline Sheebha",
      "Hasikin, Khairunnisa",
      "Velauthapillai, Dhayalan",
      "Raj, Veena",
      "Murugathas, Thanihaichelvan",
      "Azizan, Muhammad Mokhzaini",
      "Thanasekaran, Rajkumar"
    ],
    "year": "2022",
    "journal": "Applied Sciences",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886434"
  },
  {
    "id": "475b9345-8a70-4dc5-8d64-5884d75c3123",
    "title": "Assessment of the Emotional State in Domestic Dogs using a Bi-dimensional Model of Emotions and a Machine Learning Approach for the Analysis of its Vocalizations",
    "authors": [
      "P{\\'e}rez-Espinosa, Humberto",
      "Reyes-Meza, Ver{\\'o}nica",
      "de Lourdes Arteaga-Castaneda, Mar{\\i}a",
      "Espinosa-Curiel, Ismael",
      "Bautista, Amando",
      "Mart{\\i}nez-Miranda, Juan"
    ],
    "year": "2017",
    "journal": "Advances on Language \\& Knowledge Engineering",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886466"
  },
  {
    "id": "f7d8e06c-9cca-4fdd-a844-817437283700",
    "title": "Whale song shows language-like statistical structure",
    "authors": [
      "Arnon, Inbal",
      "Kirby, Simon",
      "Allen, Jenny A",
      "Garrigue, Claire",
      "Carroll, Emma L",
      "Garland, Ellen C"
    ],
    "year": "2025",
    "journal": "Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886475"
  },
  {
    "id": "65f5c4e9-39e6-4865-8828-d33615cba4c0",
    "title": "Deep representation learning for orca call type classification",
    "authors": [
      "Bergler, Christian",
      "Schmitt, Manuel",
      "Cheng, Rachael Xi",
      "Schr{\\\"o}ter, Hendrik",
      "Maier, Andreas",
      "Barth, Volker",
      "Weber, Michael",
      "N{\\\"o}th, Elmar"
    ],
    "year": "2019",
    "journal": "International Conference on Text, Speech, and Dialogue",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886482"
  },
  {
    "id": "0909398d-3846-47df-b730-316f37a1c0fe",
    "title": "ORCA-WHISPER: An Automatic Killer Whale Sound Type Generation Toolkit Using Deep Learning.",
    "authors": [
      "Bergler, Christian",
      "Barnhill, Alexander",
      "Perrin, Dominik",
      "Schmitt, Manuel",
      "Maier, Andreas K",
      "N{\\\"o}th, Elmar"
    ],
    "year": "2022",
    "journal": "INTERSPEECH",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886503"
  },
  {
    "id": "888181dc-c32a-43cc-a35d-07af18ff7079",
    "title": "Open-source machine learning BANTER acoustic classification of beaked whale echolocation pulses",
    "authors": [
      "Rankin, Shannon",
      "Sakai, Taiki",
      "Archer, Frederick I",
      "Barlow, Jay",
      "Cholewiak, Danielle",
      "DeAngelis, Annamaria I",
      "McCullough, Jennifer LK",
      "Oleson, Erin M",
      "Simonis, Anne E",
      "Soldevilla, Melissa S",
      "others"
    ],
    "year": "2024",
    "journal": "Ecological Informatics",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886517"
  },
  {
    "id": "2f1f2848-7d13-44d3-94db-722a7b093622",
    "title": "An HMM-DNN-based system for the detection and classification of low-frequency acoustic signals from baleen whales, earthquakes, and air guns off Chile",
    "authors": [
      "Buchan, Susannah J",
      "Duran, Miguel",
      "Rojas, Constanza",
      "Wuth, Jorge",
      "Mahu, Rodrigo",
      "Stafford, Kathleen M",
      "Becerra Yoma, Nestor"
    ],
    "year": "2023",
    "journal": "Remote Sensing",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886522"
  },
  {
    "id": "6eeb3cac-73cb-40e1-b0e1-44191a697b7e",
    "title": "Automatic detection of unidentified fish sounds: A comparison of traditional machine learning with deep learning",
    "authors": [
      "Mouy, Xavier",
      "Archer, Stephanie K",
      "Dosso, Stan",
      "Dudas, Sarah",
      "English, Philina",
      "Foord, Colin",
      "Halliday, William",
      "Juanes, Francis",
      "Lancaster, Darienne",
      "Van Parijs, Sofie",
      "others"
    ],
    "year": "2024",
    "journal": "Frontiers in Remote Sensing",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886527"
  },
  {
    "id": "3de6cbdf-d679-4111-af11-604dd9451742",
    "title": "Fish Acoustic Detection Algorithm Research: a deep learning app for Caribbean grouper calls detection and call types classification",
    "authors": [
      "Ibrahim, Ali K",
      "Zhuang, Hanqi",
      "Sch{\\\"a}rer-Umpierre, Michelle",
      "Woodward, Caroline",
      "Erdol, Nurgun",
      "Cherubin, Laurent M"
    ],
    "year": "2024",
    "journal": "Frontiers in Marine Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886531"
  },
  {
    "id": "ef776fc4-b96d-471d-af9f-660092d2d6c3",
    "title": "Computer vision for bioacoustics: Detection of bearded seal vocalizations in the Chukchi shelf using YOLOV5",
    "authors": [
      "Escobar-Amado, Christian",
      "Badiey, Mohsen",
      "Wan, Lin"
    ],
    "year": "2023",
    "journal": "IEEE Journal of Oceanic Engineering",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886539"
  },
  {
    "id": "78a43c1d-a579-4479-972d-1f9cc6456ab0",
    "title": "Automatic detection and classification of bearded seal vocalizations in the northeastern Chukchi Sea using convolutional neural networks",
    "authors": [
      "Escobar-Amado, Christian",
      "Badiey, Mohsen",
      "Pecknold, Sean",
      "others"
    ],
    "year": "2022",
    "journal": "The Journal of the Acoustical Society of America",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886543"
  },
  {
    "id": "76db3f5d-aa38-46e3-9659-1e1451883efd",
    "title": "Deep learning and computer vision algorithms for detection and classification of bearded seal vocalizations in the Arctic Ocean",
    "authors": [
      "Escobar-Amado, Christian David"
    ],
    "year": "2022",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886547"
  },
  {
    "id": "fe491773-def7-4514-984d-25415da21701",
    "title": "Automatic parameter estimation and detection of Saimaa ringed seal knocking vocalizations",
    "authors": [
      "Solana, Adri{\\`a}",
      "Houegnigan, Ludwig",
      "Nadeu, Climent",
      "Young, Mairi",
      "Kunnasranta, Mervi"
    ],
    "year": "2024",
    "journal": "bioRxiv",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886552"
  },
  {
    "id": "b6dbd8e2-cf9b-43fb-9729-2fd81ea27d19",
    "title": "ORCA-SPY enables killer whale sound source simulation, detection, classification and localization using an integrated deep learning-based segmentation",
    "authors": [
      "Hauer, Christopher",
      "N{\\\"o}th, Elmar",
      "Barnhill, Alexander",
      "Maier, Andreas",
      "Guthunz, Julius",
      "Hofer, Heribert",
      "Cheng, Rachael Xi",
      "Barth, Volker",
      "Bergler, Christian"
    ],
    "year": "2023",
    "journal": "Scientific Reports",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886556"
  },
  {
    "id": "82555bdf-09a1-47ef-910e-9fcd4e94a41d",
    "title": "TweetyBERT: Automated parsing of birdsong through self-supervised machine learning",
    "authors": [
      "Vengrovski, George",
      "Hulsey-Vincent, Miranda R",
      "Bemrose, Melissa A",
      "Gardner, Timothy J"
    ],
    "year": "2025",
    "journal": "bioRxiv",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886560"
  },
  {
    "id": "cbb5fd92-af82-42ac-bed8-e7e42a6b86fa",
    "title": "FinchGPT: a Transformer based language model for birdsong analysis",
    "authors": [
      "Kobayashi, Kosei",
      "Matsuzaki, Kosuke",
      "Taniguchi, Masaya",
      "Sakaguchi, Keisuke",
      "Inui, Kentaro",
      "Abe, Kentaro"
    ],
    "year": "2025",
    "journal": "arXiv preprint arXiv:2502.00344",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886564"
  },
  {
    "id": "98c6f26e-5bf7-4e7e-a274-72f744198d2d",
    "title": "Measuring context dependency in birdsong using artificial neural networks",
    "authors": [
      "Morita, Takashi",
      "Koda, Hiroki",
      "Okanoya, Kazuo",
      "Tachibana, Ryosuke O"
    ],
    "year": "2021",
    "journal": "PLoS computational biology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886568"
  },
  {
    "id": "91b89d35-e491-4a74-9cf3-de5a7d41ca4a",
    "title": "Impact of transfer learning methods and dataset characteristics on generalization in birdsong classification",
    "authors": [
      "Ghani, Burooj",
      "Kalkman, Vincent J",
      "Planqu{\\'e}, Bob",
      "Vellinga, Willem-Pier",
      "Gill, Lisa",
      "Stowell, Dan"
    ],
    "year": "2025",
    "journal": "Scientific Reports",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886573"
  },
  {
    "id": "15c826a7-335d-4f53-82b4-734d7c5e4a26",
    "title": "Multi-Scale Deep Feature Fusion with Machine Learning Classifier for Birdsong Classification",
    "authors": [
      "Li, Wei",
      "Lv, Danju",
      "Yu, Yueyun",
      "Zhang, Yan",
      "Gu, Lianglian",
      "Wang, Ziqian",
      "Zhu, Zhicheng"
    ],
    "year": "2025",
    "journal": "Applied Sciences",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886580"
  },
  {
    "id": "720d23cb-b4b4-4b54-bcfd-6ac166a985f4",
    "title": "DuSAFNet: A Multi-Path Feature Fusion and Spectral--Temporal Attention-Based Model for Bird Audio Classification",
    "authors": [
      "Lu, Zhengyang",
      "Li, Huan",
      "Liu, Min",
      "Lin, Yibin",
      "Qin, Yao",
      "Wu, Xuanyu",
      "Xu, Nanbo",
      "Pu, Haibo"
    ],
    "year": "2025",
    "journal": "Animals",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886587"
  },
  {
    "id": "c0c32936-a09f-4a09-afda-daa9510cf2c4",
    "title": "Bird Vocalization Generation with Deep Learning",
    "authors": [
      "Shevtsov, Andrii"
    ],
    "year": "2025",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886594"
  },
  {
    "id": "7585cc88-ed9e-4c47-be1e-9d1d410fcf1f",
    "title": "Automated Annotation of Birdsong with a Neural Network That Segments Spectrograms",
    "authors": [
      "Cohen, Yarden",
      "Nicholson, David Aaron",
      "Sanchioni, Alexa",
      "Mallaber, Emily K",
      "Skidanova, Viktoriya",
      "Gardner, Timothy J"
    ],
    "year": "2022",
    "journal": "eLife",
    "abstract": "",
    "doi": "10.7554/eLife.63853",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886603"
  },
  {
    "id": "6f5fa0ca-dc50-4011-9406-0786067903a0",
    "title": "DISCO}}: {{A}} Deep Learning Ensemble for Uncertainty-Aware Segmentation of Acoustic Signals",
    "authors": [
      "Colligan, Thomas",
      "Irish, Kayla",
      "Emlen, Douglas J.",
      "Wheeler, Travis J."
    ],
    "year": "2023",
    "journal": "PLOS ONE",
    "abstract": "",
    "doi": "10.1371/journal.pone.0288172",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886611"
  },
  {
    "id": "9836d29b-1ce8-4579-9469-eaf991d699f1",
    "title": "Automated Segmentation of Linear Time-Frequency Representations of Marine-Mammal Sounds",
    "authors": [
      "Dadouchi, Florian",
      "Gervaise, Cedric",
      "Ioana, Cornel",
      "Huillery, Julien",
      "Mars, J{\\'e}r{\\^o}me I."
    ],
    "year": "2013",
    "journal": "The Journal of the Acoustical Society of America",
    "abstract": "",
    "doi": "10.1121/1.4816579",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886617"
  },
  {
    "id": "3fc1dc33-3435-4213-aa59-ecd9afe4797f",
    "title": "A Segmentation Algorithm for Zebra Finch Song at the Note Level",
    "authors": [
      "Du, Ping",
      "Troyer, Todd W."
    ],
    "year": "2006",
    "journal": "Neurocomputing",
    "abstract": "",
    "doi": "10.1016/j.neucom.2005.12.110",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886624"
  },
  {
    "id": "5479cb42-b949-4744-b2eb-0680932616ff",
    "title": "Consistent birdsong syllable segmentation using deep semi-supervised learning",
    "authors": [
      "Ghaffari Jadidi, Houtan",
      "Devos, Paul"
    ],
    "year": "2023",
    "journal": "Forum Acusticum 2023",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886630"
  },
  {
    "id": "60e5a44d-27eb-4cbf-9a07-d97466d40582",
    "title": "Acoustic Animal Identification Using Unsupervised Learning",
    "authors": [
      "Guerrero, Maria J.",
      "Bedoya, Carol L.",
      "L{\\'o}pez, Jos{\\'e} D.",
      "Daza, Juan M.",
      "Isaza, Claudia"
    ],
    "year": "2023",
    "journal": "Methods in Ecology and Evolution",
    "abstract": "",
    "doi": "10.1111/2041-210X.14103",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886637"
  },
  {
    "id": "7c5d92a6-71f0-45cf-bf7a-e27d6ab7d01a",
    "title": "Time-Frequency Segmentation of Bird Song in Noisy Acoustic Environments",
    "authors": [
      "Neal, Lawrence",
      "Briggs, Forrest",
      "Raich, Raviv",
      "Fern, Xiaoli Z."
    ],
    "year": "2011",
    "journal": "2011 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})",
    "abstract": "",
    "doi": "10.1109/ICASSP.2011.5946906",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886641"
  },
  {
    "id": "af8ddc99-2573-42ee-b8bb-38558a1bfdc0",
    "title": "Machine learning algorithms can predict emotional valence across ungulate vocalizations",
    "authors": [
      "Lef{\\`e}vre, Romain A",
      "Sypherd, Ciara CR",
      "Briefer, {\\'E}lodie F"
    ],
    "year": "2025",
    "journal": "iScience",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886645"
  },
  {
    "id": "a3a25322-7d78-4b05-8b26-74a12065393e",
    "title": "orcAI: A Machine Learning Tool to Detect and Classify Acoustic Signals of Killer Whales in Audio Recordings",
    "authors": [
      "Bonhoeffer, Sebastian",
      "Selbmann, Anna",
      "Angst, Daniel C",
      "Ochsner, Nicolas",
      "Miller, Patrick JO",
      "Samarra, Filipa IP",
      "Baumgartner, Ch{\\'e}rine D"
    ],
    "year": "2025",
    "journal": "Marine Mammal Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886652"
  },
  {
    "id": "7145c614-270c-486c-92fa-29cd2f789826",
    "title": "Machine learning to predict killer whale (Orcinus orca) behaviors using partially labeled vocalization data",
    "authors": [
      "Sandholm, Sophia"
    ],
    "year": "2025",
    "journal": "Frontiers in Marine Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886655"
  },
  {
    "id": "0f2e3d4f-154f-418e-ac5d-f385e49f4953",
    "title": "Multiclass cnn approach for automatic classification of dolphin vocalizations",
    "authors": [
      "Di Nardo, Francesco",
      "De Marco, Rocco",
      "Li Veli, Daniel",
      "Screpanti, Laura",
      "Castagna, Benedetta",
      "Lucchetti, Alessandro",
      "Scaradozzi, David"
    ],
    "year": "2025",
    "journal": "Sensors",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886660"
  },
  {
    "id": "c85d3eb2-8278-4f08-8a48-c8bc7f91960e",
    "title": "Elephant sound classification using deep learning optimization",
    "authors": [
      "Dewmini, Hiruni",
      "Meedeniya, Dulani",
      "Perera, Charith"
    ],
    "year": "2025",
    "journal": "Sensors",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886666"
  },
  {
    "id": "c991e0a2-16cc-4422-8f5b-8317a8931a43",
    "title": "A Comprehensive Meta-Analysis on Animal Identification Using Machine Learning and Deep Learning",
    "authors": [
      "Vijayalakshmi, A",
      "Shanmugavadivu, P",
      "Vijayalakshmi, S",
      "Sivaranjani, R"
    ],
    "year": "2024",
    "journal": "International Conference on Data Engineering and Communication Technology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886670"
  },
  {
    "id": "82d70522-701e-4807-8df3-782533699497",
    "title": "VoCallBase: A web-based platform for annotating and standardizing animal vocalizations",
    "authors": [
      "{Evolving Language NCCR}"
    ],
    "year": "2024",
    "journal": "",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886674"
  },
  {
    "id": "28d85d21-9c60-4bf5-bf09-9c334a215104",
    "title": "What can animal communication teach us about human language?",
    "authors": [
      "Fishbein, Adam R",
      "Fritz, Jonathan B",
      "Idsardi, William J",
      "Wilkinson, Gerald S"
    ],
    "year": "2020",
    "journal": "Philosophical Transactions of the Royal Society B",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886681"
  },
  {
    "id": "2bbfd134-ccce-4107-acad-d7a45372fcdb",
    "title": "Long-distance, low-frequency elephant communication",
    "authors": [
      "Garstang, Michael"
    ],
    "year": "2004",
    "journal": "Journal of Comparative Physiology A",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886684"
  },
  {
    "id": "e29a561d-3453-4fb6-bc1b-fbd22c453e39",
    "title": "Vocal communication in African elephants (Loxodonta africana)",
    "authors": [
      "Soltis, Joseph"
    ],
    "year": "2010",
    "journal": "Zoo Biology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886688"
  },
  {
    "id": "7de627ad-aec8-4764-a361-e097e7a7dbff",
    "title": "Neural mechanisms for lexical processing in dogs",
    "authors": [
      "Andics, Attila",
      "G{\\'a}bor, Anna",
      "G{\\'a}csi, M{\\'a}rta",
      "Farag{\\'o}, Tam{\\'a}s",
      "Szab{\\'o}, D{\\'o}ra",
      "Mikl{\\'o}si, Ad{\\'a}m"
    ],
    "year": "2016",
    "journal": "Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886695"
  },
  {
    "id": "b68d9bef-de49-4109-bbf5-06a29bc8fe6a",
    "title": "Speech naturalness detection and language representation in the dog brain",
    "authors": [
      "Cuaya, Laura V",
      "Hern{\\'a}ndez-P{\\'e}rez, Ra{\\'u}l",
      "Boros, Marianna",
      "Deme, Andrea",
      "Andics, Attila"
    ],
    "year": "2022",
    "journal": "Elsevier",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886699"
  },
  {
    "id": "89ec3935-bedf-4280-8332-dbc9d807ce78",
    "title": "Profile of Dorothy L. Cheney and Robert M. Seyfarth",
    "authors": [
      "Jennifer Viegas"
    ],
    "year": "2018",
    "journal": "Proceedings of the National Academy of Sciences of the United States of America",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886703"
  },
  {
    "id": "9c6d9b5b-1f3b-4dce-97f2-90e8ba4b900a",
    "title": "The Mystery of Language Evolution",
    "authors": [
      "Yang, C",
      "Hauser, MD",
      "Berwick, RC",
      "Tattersall, I",
      "Ryan, MJ",
      "Watumull, J",
      "Chomsky, N",
      "Lewontin, RC"
    ],
    "year": "2014",
    "journal": "Frontiers in psychology",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886710"
  },
  {
    "id": "e7df76a6-f407-4664-ac77-605e258f3ce6",
    "title": "Ancestry of the AUTS2 family--A novel group of polycomb-complex proteins involved in human neurological disease",
    "authors": [
      "Sellers, Robert A",
      "Robertson, David L",
      "Tassabehji, May"
    ],
    "year": "2020",
    "journal": "PLoS One",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886714"
  },
  {
    "id": "cddab331-b37d-410f-a388-5b6b84325f9d",
    "title": "Towards a new taxonomy of primate vocal production learning",
    "authors": [
      "Fischer, Julia",
      "Hammerschmidt, Kurt"
    ],
    "year": "2020",
    "journal": "Philosophical Transactions of the Royal Society B",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886718"
  },
  {
    "id": "7d39f3a2-1bfa-49d9-a21f-5ff9f057d22a",
    "title": "Becoming human: human infants link language and cognition, but what about the other great apes?",
    "authors": [
      "Novack, Miriam A",
      "Waxman, Sandra"
    ],
    "year": "2020",
    "journal": "Philosophical Transactions of the Royal Society B",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886724"
  },
  {
    "id": "0659e82d-09e0-4cd4-aca1-6065952ac8a4",
    "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
    "authors": [
      "Tarvainen, Antti",
      "Valpola, Harri"
    ],
    "year": "2017",
    "journal": "Advances in neural information processing systems",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886728"
  },
  {
    "id": "61ac6f5a-29ad-4443-af4f-75183edec1f6",
    "title": "Songs of Humpback Whales: Humpbacks emit sounds in long, predictable patterns ranging over frequencies audible to humans.",
    "authors": [
      "Payne, Roger S",
      "McVay, Scott"
    ],
    "year": "1971",
    "journal": "Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886736"
  },
  {
    "id": "037fe8f2-8c97-4fcc-9e6e-471dd4159bc6",
    "title": "Hierarchical temporal structure in music, speech and animal vocalizations: jazz is like a conversation, humpbacks sing like hermit thrushes",
    "authors": [
      "Kello, Christopher T",
      "Bella, Simone Dalla",
      "M{\\'e}d{\\'e}, Butovens",
      "Balasubramaniam, Ramesh"
    ],
    "year": "2017",
    "journal": "Journal of the Royal Society Interface",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886743"
  },
  {
    "id": "c5419821-bf0d-4b7e-8651-30b03afc12b6",
    "title": "Compensating class imbalance for acoustic chimpanzee detection with convolutional recurrent neural networks",
    "authors": [
      "Anders, Franz",
      "Kalan, Ammie K",
      "K{\\\"u}hl, Hjalmar S",
      "Fuchs, Mirco"
    ],
    "year": "2021",
    "journal": "Ecological Informatics",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886747"
  },
  {
    "id": "fb2504fc-ae5f-44c5-92a0-8e1389217236",
    "title": "A densely sampled and richly annotated acoustic data set from a wild bird population",
    "authors": [
      "Recalde, Nilo Merino",
      "Estand{\\'\\i}a, Andrea",
      "Pichot, Loanne",
      "Vansse, Antoine",
      "Cole, Ella F",
      "Sheldon, Ben C"
    ],
    "year": "2024",
    "journal": "Animal Behaviour",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886751"
  },
  {
    "id": "9be119b5-c707-4d05-95b2-6a3b0bbf92de",
    "title": "Birds, bats and beyond: Evaluating generalization in bioacoustics models",
    "authors": [
      "Van Merri{\\\"e}nboer, Bart",
      "Hamer, Jenny",
      "Dumoulin, Vincent",
      "Triantafillou, Eleni",
      "Denton, Tom"
    ],
    "year": "2024",
    "journal": "Frontiers in Bird Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886758"
  },
  {
    "id": "122f1505-501e-4f4e-acba-c3dba3b0582b",
    "title": "On the opportunities and risks of foundation models",
    "authors": [
      "Bommasani, Rishi"
    ],
    "year": "2021",
    "journal": "arXiv preprint arXiv:2108.07258",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886762"
  },
  {
    "id": "a2ba456b-707c-4d42-8e15-8d8cce33e115",
    "title": "Foundation Models for Bioacoustics--a Comparative Review",
    "authors": [
      "Schwinger, Raphael",
      "Zadeh, Paria Vali",
      "Rauch, Lukas",
      "Kurz, Mats",
      "Hauschild, Tom",
      "Lapp, Sam",
      "Tomforde, Sven"
    ],
    "year": "2025",
    "journal": "arXiv preprint arXiv:2508.01277",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886766"
  },
  {
    "id": "0638f13a-4847-4bdd-9850-a14687e8ae77",
    "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
    "authors": [
      "Baevski, Alexei",
      "Zhou, Yuhao",
      "Mohamed, Abdelrahman",
      "Auli, Michael"
    ],
    "year": "2020",
    "journal": "Advances in neural information processing systems",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886772"
  },
  {
    "id": "0bb11b44-e15a-444b-82f1-d2aff597d82c",
    "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
    "authors": [
      "Hsu, Wei-Ning",
      "Bolte, Benjamin",
      "Tsai, Yao-Hung Hubert",
      "Lakhotia, Kushal",
      "Salakhutdinov, Ruslan",
      "Mohamed, Abdelrahman"
    ],
    "year": "2021",
    "journal": "IEEE/ACM transactions on audio, speech, and language processing",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886776"
  },
  {
    "id": "168a526d-9f4e-4945-9ed5-23b015720b0a",
    "title": "Robust speech recognition via large-scale weak supervision",
    "authors": [
      "Radford, Alec",
      "Kim, Jong Wook",
      "Xu, Tao",
      "Brockman, Greg",
      "McLeavey, Christine",
      "Sutskever, Ilya"
    ],
    "year": "2023",
    "journal": "International conference on machine learning",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886781"
  },
  {
    "id": "4dde6143-10b9-4065-a08d-7bc91e352770",
    "title": "animal2vec and MeerKAT: A self-supervised transformer for rare-event raw audio input and a large-scale reference dataset for bioacoustics",
    "authors": [
      "Sch{\\\"a}fer-Zimmermann, Julian C",
      "Demartsev, Vlad",
      "Averly, Baptiste",
      "Dhanjal-Adams, Kiran",
      "Duteil, Mathieu",
      "Gall, Gabriella",
      "Fai{\\ss}, Marius",
      "Johnson-Ulrich, Lily",
      "Stowell, Dan",
      "Manser, Marta B",
      "others"
    ],
    "year": "2024",
    "journal": "arXiv preprint arXiv:2406.01253",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886788"
  },
  {
    "id": "2cfad0fd-479c-459c-8095-0f9248d791fa",
    "title": "BovineTalk: machine learning for vocalization analysis of dairy cattle under the negative affective state of isolation",
    "authors": [
      "Gavojdian, Dinu",
      "Mincu, Madalina",
      "Lazebnik, Teddy",
      "Oren, Ariel",
      "Nicolae, Ioana",
      "Zamansky, Anna"
    ],
    "year": "2024",
    "journal": "Frontiers in Veterinary Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886792"
  },
  {
    "id": "f3a9cc8d-ef7b-473b-95f6-b34f20bb4ac7",
    "title": "You talkin’to me? Interactive playback is a powerful yet underused tool in animal communication research",
    "authors": [
      "King, Stephanie L"
    ],
    "year": "2015",
    "journal": "Biology Letters",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886796"
  },
  {
    "id": "eca9f458-2e28-4e5d-8e81-9f965448b707",
    "title": "Characterization and assessment of vocalization responses of cows to different physiological states",
    "authors": [
      "Yoshihara, Yu",
      "Oya, Kosei"
    ],
    "year": "2021",
    "journal": "Journal of Applied Animal Research",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886802"
  },
  {
    "id": "9b2d3896-b560-4b36-8e0d-7dc98ec37e23",
    "title": "Behavioral effects of auditory stimulation on kenneled dogs",
    "authors": [
      "Kogan, Lori R",
      "Schoenfeld-Tacher, Regina",
      "Simon, Allen A"
    ],
    "year": "2012",
    "journal": "Journal of Veterinary Behavior",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886806"
  },
  {
    "id": "38722340-a70d-49ce-ad4d-da680a8bcb07",
    "title": "Causal cognition in a non-human primate: field playback experiments with Diana monkeys",
    "authors": [
      "Zuberb{\\\"u}hler, Klaus"
    ],
    "year": "2000",
    "journal": "Cognition",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886809"
  },
  {
    "id": "e92bfa19-d75d-4a47-81af-efff9a24c195",
    "title": "Evaluating the function of the male harbour seal, Phoca vitulina, roar through playback experiments",
    "authors": [
      "Hayes, Sean A",
      "Kumar, Anurag",
      "Costa, Daniel P",
      "Mellinger, David K",
      "Harvey, James T",
      "Southall, Brandon L",
      "Le Boeuf, Burney J"
    ],
    "year": "2004",
    "journal": "Animal Behaviour",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886816"
  },
  {
    "id": "8d25fe31-bcf4-437c-bc1a-52508600520e",
    "title": "Wild chimpanzees (Pan troglodytes schweinfurthii) distinguish between different scream types: evidence from a playback study",
    "authors": [
      "Slocombe, Katie Elizabeth",
      "Townsend, Simon W",
      "Zuberb{\\\"u}hler, Klaus"
    ],
    "year": "2009",
    "journal": "Animal cognition",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886857"
  },
  {
    "id": "ec9d7d84-50d7-46ac-a27e-2fbde16adc02",
    "title": "Turning the tables: a tiny bird uses alarm calls and mimicry to deceive its nest predator",
    "authors": [
      "Ascah, Lauren",
      "Igic, Branislav",
      "Magrath, Robert"
    ],
    "year": "2025",
    "journal": "Biology Letters",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.886861"
  },
  {
    "id": "6c4e2d56-450b-42bf-98b5-657c95e83f22",
    "title": "Feeling anxious? The mechanisms of vocal deception in tufted capuchin monkeys",
    "authors": [
      "Kean, Donna",
      "Tiddi, Barbara",
      "Fahy, Martin",
      "Heistermann, Michael",
      "Schino, Gabriele",
      "Wheeler, Brandon C"
    ],
    "year": "2017",
    "journal": "Animal Behaviour",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887332"
  },
  {
    "id": "05a20b7a-ad3b-45fb-a2e0-84a196674cd9",
    "title": "Neural decipherment via minimum-cost flow: From Ugaritic to Linear B",
    "authors": [
      "Luo, Jiaming",
      "Cao, Yuan",
      "Barzilay, Regina"
    ],
    "year": "2019",
    "journal": "arXiv preprint arXiv:1906.06718",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887340"
  },
  {
    "id": "2e30168b-bbc9-4d37-877f-692dcff47ac1",
    "title": "Deciphering undersegmented ancient scripts using phonetic prior",
    "authors": [
      "Luo, Jiaming",
      "Hartmann, Frederik",
      "Santus, Enrico",
      "Barzilay, Regina",
      "Cao, Yuan"
    ],
    "year": "2021",
    "journal": "Transactions of the Association for Computational Linguistics",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887345"
  },
  {
    "id": "aaab177d-3eae-49c4-b33b-480e11bf61f0",
    "title": "A fast and accurate zebra finch syllable detector",
    "authors": [
      "Pearre, Ben",
      "Perkins, L Nathan",
      "Markowitz, Jeffrey E",
      "Gardner, Timothy J"
    ],
    "year": "2017",
    "journal": "PloS one",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887349"
  },
  {
    "id": "b772f758-b412-4bd5-a403-a525159ab834",
    "title": "Acoustic analysis of cattle (Bos taurus) mother--offspring contact calls from a source--filter theory perspective",
    "authors": [
      "de la Torre, M{\\'o}nica Padilla",
      "Briefer, Elodie F",
      "Reader, Tom",
      "McElligott, Alan G"
    ],
    "year": "2015",
    "journal": "Applied Animal Behaviour Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887354"
  },
  {
    "id": "f46099dd-036c-4ffa-b567-0b10e1ea9ed5",
    "title": "Vocal individuality of Holstein-Friesian cattle is maintained across putatively positive and negative farming contexts",
    "authors": [
      "Green, Alexandra",
      "Clark, Cameron",
      "Favaro, Livio",
      "Lomax, Sabrina",
      "Reby, David"
    ],
    "year": "2019",
    "journal": "Scientific Reports",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887362"
  },
  {
    "id": "8f08cf31-4396-4e1c-9deb-111c85c1ad13",
    "title": "Vocalization and other behaviors as indicators of emotional valence: The case of cow-calf separation and reunion in beef cattle",
    "authors": [
      "Schnaider, Maria A",
      "Heidemann, Marina S",
      "Silva, Adelaide HP",
      "Taconeli, C{\\'e}sar A",
      "Molento, Carla FM"
    ],
    "year": "2022",
    "journal": "Journal of Veterinary Behavior",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887366"
  },
  {
    "id": "05ebddd9-33f4-4333-a5cc-3d72b7d455ad",
    "title": "Multi Modal Information Fusion of Acoustic and Linguistic Data for Decoding Dairy Cow Vocalizations in Animal Welfare Assessment",
    "authors": [
      "Jobarteh, Bubacarr",
      "Mincu, Madalina",
      "Dinu, Gavojdian",
      "Neethirajan, Suresh"
    ],
    "year": "2024",
    "journal": "arXiv preprint arXiv:2411.00477",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887370"
  },
  {
    "id": "355811f6-0cf2-455a-a6c2-3c4b31ae6e25",
    "title": "Sound analysis in dairy cattle vocalisation as a potential welfare monitor",
    "authors": [
      "Meen, GH",
      "Schellekens, MA",
      "Slegers, MHM",
      "Leenders, NLG",
      "van Erp-van der Kooij, Elaine",
      "Noldus, Lucas PJJ"
    ],
    "year": "2015",
    "journal": "Computers and Electronics in Agriculture",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887374"
  },
  {
    "id": "093bf7d1-5025-4e3d-8ea6-cf2aa779651e",
    "title": "A context-aware method-based cattle vocal classification for livestock monitoring in smart farm",
    "authors": [
      "Sattar, Farook"
    ],
    "year": "2022",
    "journal": "Chemistry Proceedings",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887391"
  },
  {
    "id": "1cec138f-453a-4a9b-8566-c5351f34921e",
    "title": "On-farm detection of claw lesions in dairy cows based on acoustic analyses and machine learning",
    "authors": [
      "Volkmann, N",
      "Kulig, B",
      "Hoppe, S",
      "Stracke, J",
      "Hensel, O",
      "Kemper, N"
    ],
    "year": "2021",
    "journal": "Journal of dairy science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887397"
  },
  {
    "id": "5f0d4e7e-821e-4bff-b2c9-07176845f8ef",
    "title": "Japanese Black cattle call patterns classification using multiple acoustic features and machine learning models",
    "authors": [
      "Peng, Yingqi",
      "Kondo, Naoshi",
      "Fujiura, Tateshi",
      "Suzuki, Tetsuhito",
      "Yoshioka, Hidetsugu",
      "Itoyama, Erina",
      "others"
    ],
    "year": "2023",
    "journal": "Computers and Electronics in Agriculture",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887402"
  },
  {
    "id": "cb18130b-617e-440e-bb6d-8dd18cd06173",
    "title": "Integrating diverse data sources to predict disease risk in dairy cattle—a machine learning approach",
    "authors": [
      "Lasser, Jana",
      "Matzhold, Caspar",
      "Egger-Danner, Christa",
      "Fuerst-Waltl, Birgit",
      "Steininger, Franz",
      "Wittek, Thomas",
      "Klimek, Peter"
    ],
    "year": "2021",
    "journal": "Journal of Animal Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887407"
  },
  {
    "id": "43c718ca-e96b-403c-b865-a7d709213398",
    "title": "Cattle disease prediction using artificial intelligence",
    "authors": [
      "Rao, A",
      "Monika, HR",
      "Rakshitha, BC",
      "Thaseen, Seham"
    ],
    "year": "2023",
    "journal": "International Journal for Research in Applied Science \\& Engineering Technology (IJRASET)",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887415"
  },
  {
    "id": "fc5f4dca-8f91-430b-96b0-14d4df7eebee",
    "title": "Machine learning approaches to predict and detect early-onset of digital dermatitis in dairy cows using sensor data",
    "authors": [
      "Magana, Jennifer",
      "Gavojdian, Dinu",
      "Menahem, Yakir",
      "Lazebnik, Teddy",
      "Zamansky, Anna",
      "Adams-Progar, Amber"
    ],
    "year": "2023",
    "journal": "Frontiers in Veterinary Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887419"
  },
  {
    "id": "2547b98d-83d1-4cda-be46-08eefd4c8ffb",
    "title": "Calming Hungarian Grey cattle in headlocks using processed nasal vocalization of a mother cow",
    "authors": [
      "Lenner, {\\'A}d{\\'a}m",
      "Papp, Zolt{\\'a}n Lajos",
      "Szab{\\'o}, Csaba",
      "Koml{\\'o}si, Istv{\\'a}n"
    ],
    "year": "2023",
    "journal": "Animals",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887423"
  },
  {
    "id": "ec3931fa-f66e-40fd-8d7b-7da09435278f",
    "title": "Analysis of sounds made by Bos taurus and Bubalus bubalis dams to their calves",
    "authors": [
      "Lenner, {\\'A}d{\\'a}m",
      "Papp, Zolt{\\'a}n Lajos",
      "Koml{\\'o}si, Istv{\\'a}n"
    ],
    "year": "2025",
    "journal": "Frontiers in Veterinary Science",
    "abstract": "",
    "doi": "",
    "analysis_notes": "",
    "affiliations": [],
    "species_categories": [],
    "specialized_species": [],
    "computational_stages": [],
    "linguistic_features": [],
    "status": "pending",
    "created_at": "2026-01-13T12:49:59.887430"
  }
]