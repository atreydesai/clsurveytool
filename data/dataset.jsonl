{"id": "1eea6165-8eec-442c-9982-3c1b45f0fb6c", "title": "The Origin of Speech", "authors": ["Hockett,  Charles F."], "year": "1960", "journal": "Scientific American", "abstract": "", "doi": "10.1038/scientificamerican0960-88", "analysis_notes": "The Origin of Speech\nAuthor(s): Charles F. Hockett and Charles D. Hockett\nSource:\nScientific American , Vol. 203, No. 3 (September 1960), pp. 88-97\nPublished by: Scientific American, a division of Nature America, Inc.\nStable URL: https://www.jstor.org/stable/10.2307/24940617\nJSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide\nrange of content in a trusted digital archive. We use information technology and tools to increase productivity and\nfacilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.\nYour use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at\nhttps://about.jstor.org/terms\nScientific American, a division of Nature America, Inc. is collaborating with JSTOR to digitize,\npreserve and extend access to\nScientific American\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/termsTHREAT POSTURE of male stickleback is example of nonvocal\ncommunication in lower animals. In this picture, made by N. Tin·\nbergen of the University of Oxford, the fish is responding to it\"\nmirror image by indicating readiness to figllt \"intruding\" male,© 1960 SCIENTIFIC AMERICAN, INC\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/termsThe Origin of Speech\nMan lS the only animal that can communicate by nleans of abstract\nsYlnbols. Yet this ability shares lnany features with conlmunication\nIn other animals, and has arisen fr01n these lnore prilniti(Je syste71IS\nXout 50 years ago the Linguistic\nSociety of Paris established a\nstanding rule barring from its\nsessions papers on the origin of language.\nThis action was a symptom of the times.\nSpeculation about the origin of language\nhad been common throughout the 19th\ncentury, but had reached no conclusive\nresults. The whole enterprise in conse\nquence had come to be frowned upon\nas futile or crackpot-in respectable\nlinguistic and philological circles. Yet\namidst the speculations there were two\nwell-reasoned empirical plans that de\nserve mention even though their results\nwere negative.\nA century ago there were still many\ncorners of the world that had not been\nvisited by European travelers. It was\nreasonable for the European scholar to\nsuspect that beyond the farthest fron\ntiers there might lurk half-men or man\napes who would be \"living fossils\"\nattesting to earlier stages of human\nevolution. The speech (or quasi-speech)\nof these men (or quasi-men) might\nthen similarly attest to earlier stages in\nthe evolution of language. The search\nwas vain. Nowhere in the world has\nthere been discovered a language that\ncan validly and meaningfully be called\n\"primitive.\" Edward Sapir wrote in\n1921: \"There is no more striking gen\neral fact about language than its uni\nversality. One may argue as to whether\na particular tribe engages in activities\nthat are worthy of the name of religion\nor of art, but we know of no people that\nis not possessed of a fully developed\nlanguage. The lowliest South African\nBushman speaks in the forms of a rich\nsymbolic system that is in essence per\nfectly comparable to the speech of the\ncultivated Frenchman.\"\nThe other empirical hope in the 19th\ncentury rested on the comparative meth-\nby Charlcs F. Hockett\nod of historical linguistics, the discovery\nof which was one of the triumphs of the\nperiod. Between two languages the re\nsemblances are sometimes so extensive\nand orderly that they cannot be attrib\nuted to chance or to parallel develop\nment. The alternative explanation is that\nthe two are divergent descendants of a\nsingle earlier language. English, Dutch,\nGerman and the Scandinavian languages\nare related in just this way. The com\nparative method makes it possible to ex\namine such a group of related languages\nand to construct, often in surprising de\ntail, a portrayal of the common ancestor,\nin this case the proto-Germanic lan\nguage. Direct documentary evidence of\nproto-Germanic does not exist, yet un\nderstanding of its workings exceeds that\nof many languages spoken today. .\nThere was at first some hope that the\ncomparative method might help deter\nmine the origin of language. This hope\nwas rational in a day when it was\nthought that language might be only a\nfew thousands or tens of thousands of\nyears old, and when it was repeatedly\nbeing demonstrated that languages that\nhad been thought to be unrelated were\nin fact related. By applying the com\nparative method to all the languages of\nthe world, some earliest reconstructable\nhorizon would be reached. This might\nnot date back so early as the origin of\nlanguage, but it might bear certain ear\nmarks of primitiveness, and thus it would\nenable investigators to extrapolate to\nward the origin. This hope also proved\nvain. The earliest reconstructable stage\nfor any language family shows all the\ncomplexities and ftexibilities of the lan\nguages of today.\nT hese points had become clear a half\ncentury ago, by the time of the Paris\nruling. Scholars cannot really approve of\nsuch a prohibition. But in this instance\nit had the useful result of channeling the\nenergies of investigators toward the\ngathering of more and better information\nabout languages as they are today. The\nsubsequent progress in understanding\nthe workings of language has been truly\nremarkable. Various related fields have\nalso made vast strides in the last half\ncentury: zoologists know more about the\nevolutionary process, anthropologists\nknow more about the nature of culture,\nand so on. In the light of these develop\nments there need be no apology for re\nopening the issue of the origins of hu\nman speech.\nAlthough the comparative method of\nlinguistics, as has been shown, throws no\nlight on the origin of language, the in\nvestigation may be furthered by a com\npat'ative method modeled on that of the\nzoologist. The frame of reference must\nbe such that all languages look alike\nwhen viewed through it, but such that\nwithin it human language as a whole can\nbe compared with the communicative\nsystems of other animals, especially the\nother hominoids, man's closest living\nrelatives, the gibbons and great apes.\nThe useful items for this sort of com\nparison cannot be things such as the\nword for \"sky\"; languages have such\nwords, but gibbon calls do not involve\nwords at all. Nor can they be even the\nsignal for \"danger,\" which gibbons do\nhave. Rather, they must be the basic\nfeatures of design that can be present\nor absent in any communicative system,\nwhether it be a communicative system\nof humans, of animals or of machines.\nWith this sort of comparative method\nit may be possible to reconstruct the\ncommunicative habits of the remote an\ncestors of the hominoid line, which may\nbe called the protohominoids. The task,\nthen, is to work out the sequence by\n89© 1960 SCIENTIFIC AMERICAN, INC\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/termsA set of 13 design-features is presented in the illustration on the op\nposite page. There is solid empirical jus\ntification for the belief that all the lan\nguages of the world share every one of\nthem. At first sight some appear so trivial\nthat no one looking just at language\nwould bother to note them. They become\nworthy of mention only when it is real\nized that certain animal systems-and\ncertain human systems other than lan\nguage-lack them.\nThe first design-feature-the \"vocal\nauditory channel\"-is perhaps the most\nobvious. There are systems of communi\ncation that use other channels; for exam\nple, gesture, the dancing of bees or the\ncourtship ritual of the stickleback. The\nvocal-auditory channel has the advan\ntage-at least for primates;-that it leaves\nmuch of the body free for other activities\nthat can be carried on at the same time.\nThe next two design-features-\"rapid\nfading\" and \"broadcast transmission and\ndirectional reception,\" stemming from\nthe physics of sound-are almost un\navoidable consequences of the first. A\nlinguistic signal can be heard by any\nauditory system within earshot, and the\nsource can normally be localized by bin\naural direction-finding. The rapid fad\ning of such a signal means that it does\nnot linger for reception at the hearer's\nconvenience. Animal tracks and spoors,\non the other hand, persist for a while; so\nof course do written records, a product\nof man's extremely recent cultural ev\nlution.\nThe significance of \"interchangeabil\nity\" and \"total feedback\" for language\nbecomes clear upon comparison with\nother systems. In general a speaker of a\nlanguage can reproduce any linguistic\nmessag� he can understand, whereas the\ncharacteristic courtship motipns of the\nmale and female stickleback are differ\nent, and neither can act out those ap\npropriate to the other. For that matter\nin the communication of a human moth\ner and infant neither is apt to transmit\nthe characteristic signals or to manifest\nthe typical responses of the other. Again,\nthe speaker of a language hears, by total\nfeedback, everything of linguistic rele\nvance in what he himself says. In con\ntrast, the male stickleback does not see\nthe colors of his own eye and belly that\nare crucial in stimulating the fe\nmale. Feedback is important, since it\nmakes possible the so-called internali\nzation of Communicative behavior that\n90\ntion,\" refers to the fact that the bodily\neffort and spreading sound waves of\nspeech serve no function except as sig\nnals. A dog, panting with his tongue\nhanging out, is performing a biologically\nessential activity, since this is how dogs\ncool themselves off and maintain the\nproper body temperature. The panting\ndog incidentally produces sound, and\nthereby may inform other dogs (or Im\nmans) as to where he is and how he\nfeels. But this transmission of informa\ntion is strictly a side effect. Nor does the\ndog's panting exhibit the design-feature\nof \"semanticity.\" It is not a signal mean\ning that the dog is hot; it is part of being\nhot. In language, however, a message\ntriggers the particular result it does be\ncause there are relatively fixed associa\ntions between elements in messages\n(e.g., words) and recurrent features or\nsituations of the world around us. For\nexample, the English word \"salt\" means\nsalt, not sugar or pepper. The calls of\ngibbons also possess semanticity. The\ngibbon has a danger call, for example,\nand it does not in principle matter that\nthe meaning of the call is a great deal\nbroader and more vague than, say, the\ncry of \"Fire!\"\nIn a semantic communicative system\nthe ties between meaningful message\nelements and their meanings can be ar\nbitrary or nonarbitrary. In language the\nties are arbitrary. The word \"salt\" is not\nsalty nor granular; \"dog\" is not \"canine\";\n\"whale\" is a small word for a large ob\nject; \"microorganism\" is the reverse. A\npicture, on the other hand, looks like\nwhat it is a picture of. A bee dances\nfaster if the source of nectar she is re\nporting is closer, and slower if it is far\nther away. The design-feature of \"arbi\ntrariness\" has the disadvantage of being\narbitrary, but the great advantage that\nthere is no limit to what can be com\nmunicated about.\nHuman vocal organs can produce a\nhuge variety of sound. But in any one\nlanguage only a relatively small set of\nranges of sound is used, and the differ\nences between these ranges are function\nally absolute. The English words \"pin\"\nand \"bin\" are different to the ear only at\none point. If a speaker produces a syl\nlable that deviates from the normal pro\nnunciation of \"pin\" in the direction of\nthat of \"bin,\" he is not producing still a\nthird word, but just saying \"pin\" (@r\nperhaps \"bin\") in a noisy way. The\nhearer compensates if he can, on the\nbasis of context, or else fails to under-\neffects by way of vocal gesture. There is\nan effectively continuous scale of de\ngrees to which one may raise his voice\nas in anger, or lower it to signal confi\ndentiality. Bee-dancing also is continu\nous rather than discrete. ,\nMan is apparently almost unique in\nbeing able to talk about things that are\nremote in space or time (or both) from\nwhere the talking goes on. This feature\n\"displacement\" -seems to be definitely\nlacking in the vocal signaling of man's\nclosest relatives, though it does occur in\nbee-dancing.\nOne of the most important design\nfeatures of language is \"productivity\";\nthat is, the capacity to say things that\nhave never been said or heard before\nand yet to be understood by other speak\ners of the language. If a gibbon makes\nany vocal sound at all, it is one or an\nother of a small finite repertory of fa\nmiliar calls. The gibbon call system can\nbe characterized as closed. Language is\nopen, or \"productive,\" in the sense that\none can coin new utterances by putting\ntogether pieces familiar from old utter\nances, assembling them by patterns of\narrangement also familiar in old utter\nances.\nHuman genes carry the capacity to\nacquire a language, and probably also\na strong drive toward such acquisition,\nbut the detailed conventions of any one\nlanguage are transmitted extragenetical\nly by learning and teaching. To what\nextent such \"traditional transmission\"\nplays a part in gibbon calls or for other\nmammalian systems of vocal Signals is\nnot known, though in some instances the\nuniformity of the sounds made by a spe\ncies, wherever the species is found over\nthe world, is so great that genetics must\nbe responsible.\nThe meaningful elements in any lan\nguage-\"words\" in everyday parlance,\n\"morphemes\" to the linguist-constitute\nan enormous stock. Yet they are repre\nsented by small arrangements of a rela\ntively very small stock of distinguishable\nsounds which are in themselves wholly\nmeaningless. This \"duality of pattern\ning\" is illustrated by the English words\nTHIRTEEN DESIGN·FEATURES of ani.\nmal communication, discussed in detail in\nthe text of this article, are symbolized on\nopposite page. The patterns of the words\n\"pin,\" \"bin,\" \"teanl\" and \"nleal\" were\nrecorded at Bell Telephone Laboratories.© 1960 SCIENTIFIC AMERICAN, INC\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/terms4 INTE RCHANGE A BILI TY\n7 SEMANTICITY\n'')///./�\n, J\nl\n/\"\n\",\n5\n\"1\n� PASS THE SAL T\n'-\n10 DISPLACEMENT\nI �1\nu.j\\!iI,J ..,\n,\"\nSHADES OF JULIUS CAESAR\nk)', TO TAL FEEDBACK\n8 A RBI TRA RINESS\nWHALE\nMICROORGANISMS\nSHE HAS GREEN HAIR\n6 SPECIALIZATION\n9\nD\nISCRE TEN�SS\n-- �1'\\\\ �MWN�I,¥/lAAhMlM WII�\nPIN\n12 TRADITIONAL TRANSMISSION\n13 DUAUT. OF � �\n�\n.\nT . . . . ,., ............ E A. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . , ..........................M\n9 1© 1960 SCIENTIFIC AMERICAN, INC\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/termsNORWEGIAN\nSWEDISH\nNORTH\nGERMAN\nDIALECTS\nPROTO-GERMANIC\nPROTO- INDO-EUROPEAN\nDIALECTS\nORIGIN OF MODERN GERMANIC LANGUAGES, as indicated by this \"family tree,\"\nwas proto-Germanic, spoken some 2,700 years ago. Comparison of present·day languages\nhas provided detailed knowledge of proto·Germanic, although no direct documentary evi·\ndence for the language exists. It grew, in turn, from the proto·lndo-European of 5000 B.C.\nHistorical studies cannot, however, trace origins of language hack much further in time.\n92\nsounds in different permutations. Few\nanimal communicative systems share this\ndesign-feature of language-none among\nthe other hominoids, and perhaps none\nat all.\nI t should be noted that some of these\n13 design-features are not independ\nent. In particular, a system cannot be\neither arbitrary or nonarbitrary unless it\nis semantic, and it cannot have duality\nof patterning unless it is semantic. It\nshould also be noted that the listing does\nnot attempt to include all the features\nthat might be discovered in the commu\nnicative behavior of this or that species,\nbut only those that are clearly important\nfor language.\nIt is probably safe to assume that nine\nof the 13 features were already present\nin the vocal-auditory communication of\nthe protohominoids-just the nine that\nare securely attested for the gibbons and\nhumans of today. That is, there were a\ndozen or so distinct calls, each the ap\npropriate vocal response (or vocal part\nof the whole response) to a recurrent\nand biologically important type of situ\nation: the discovery of food, the detec\ntion of a predator, sexual interest, need\nfor maternal care, and so on. The prob\nlem of the origin of human speech, then,\nis that of trying to determine how such a\nsystem could have developed the four\nadditional properties of displacement,\nproductivity and full-blown traditional\ntransmission. Of course the full story in\nvolves a great deal more than communi\ncative behavior alone. The development\nmust be visualized as occurring in the\ncontext of the evolution of the primate\nhorde into the primitive society of food\ngatherers and huntel s, an integral part,\nbut a part, of the total evolution of be\nhavior.\nIt is possible to imagine a closed sys\ntem developing some degree of produc\ntivity, even in the absence of the other\nthree features. Human speech exhibits a\nphenomenon that could have this effect,\nthe phenomenon of \"blending.\" Some\ntimes a speaker will hesitate between\ntwo words or phrases, both reasonably\nappropriate for the situation in which he\nis speaking, and actually say something\nthat is neither wholly one nor wholly the\nother, but a combination of parts of\neach. Hesitating between \"Don't shout\nso loud\" and \"Don't yell so loud,\" he\nmight come out with \"Don't shell so\nloud.\" Blending is almost always in\nvolved in slips of the tongue, but it may© 1960 SCIENTIFIC AMERICAN, INC\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/terms-------_ .._.. __.\nREPTILES\n� � r�\nVERTEBRATES\nCHORDATES\nr\nP R ODUCTI VITY\nDUALITY OF PATT ERNING\nDISCR E TE NESS\nTRADITI ONAL TRANSMI SSION\nSPEC I ALIZATION\nSEMANTICITY\nA R B I T R ARINESS\nBROAD C AST T R ANSMISSI ON\nAND DIRECTI ONAL RE C EPTI ON\nINTE RCHA NGEABILI T Y\nRAPI D F A DING TOTAL FEEDBACK\nVOCAL - AUD I TO R Y CHANNel\nTOOL-MAKING AND CARRYING\nLARYNX AND SOFT PALATE SEPARATED\nHUMOR VOWEL COLOR MUSIC\nBIPEDAL LOCOMOTION, NOT UPRIGHT\nOCCASIONAl.TOOL USING\nHANDS HAND-EYE COORDINATION\nBINOCULAR VISION\nMOBILE FACIAL MUSCLES\nOMNIVOROUS?\nSOCIAL BEHAVIOR \"PLAY\"\"\nWARM BLOODEDNESS\nLAND EGG\nBREATHING WITH THORACIC MUSCLES\nLEGS\nSLEEPING VERSUS WAKING\nEXTERNAL EAR\nVISION\nHEARING (INTERNAL EAR)\nMOTILITY BILATERAL SYMMETRY\nFRONT AND REAR ENDS\nEVOLUTION OF LANGUAGE and some related characteristics\nare suggested by this classification of chordates. The lowest form\nof animal in each classification exhibits the features listed at the\nright of the class. Brackets indicate that each group possesses or has\nevolved beyond the characteristics exhibited by all the groups\nbelow. The 13 design-features of language appear in the colored\nrectangle. Some but by no means all of the characteristics asso\nciated with communication are presented in the column at right_\n93© 1960 SCIENTIFIC AMERICAN, INC\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/termsthat he has not said before. Anything a\nspeaker says must be either an exact\nrepetition of an utterance he has heard\nbefore, or else some blended product of\ntwo or more such familiar utterances.\nThus even such a smooth and normal\nsentence as \"I tried to get there, but the\ncar broke down\" might be produced as\na blend, say, of \"I tried to get there but\ncouldn't\" and \"While 1 was driving down\nMain Street the car broke down.\"\nChildren acquiring the language of\ntheir community pass through a stage\nthat is closed in just the way gibbon calls\n1 THE VOCAL·AUDITORY CHANNEl\n2 BROADCAST TRANSMISSION\nAND DIRECTIONAL RECEPTION\n3 RAPID FADING (TRANSITORINESS)\n4 INTERCHANGEABILITY\n0 TOTAL FEEDBACK\n6 SPECIALIZATION\n7 SEMANTICITY\n8 ARBITRARINESS\n9 DISCRETENESS\n10 DISPLACEMENT\n11 PRODUCTIVITY\n12 TRADITIONAL TRANSMISSION\n13 DUALITY OF PATTERNING\nin adult terms, has an internal structure,\nand yet for the child each may be an\nindivisible whole. He may also learn\nnew whole utterances from surrounding\nadults. The child takes the crucial step,\nhowever, when he first' says something\nthat he has not learned from others. The\nonly way in which the child can possibly\ndo this is by blending two of the whole\nutterances that he already knows.\nI n the case of the closed call-system\nof the gibbons or the protohominoids,\nthere is no source for the addition of new\nA\nSOME GRYLLIDAE\nAND TETTIGONIIDAE\nAUDITORY,\nNOT VOCAL\nYES\nYES, REPEATED\nLIMITED\nYES\nYES ?\n?\nYES?\n? ( T RIVIAL)\nB\nBEE DANCING\nYES, ALWAYS\nYES\nPROBABLY NOT\nand cries of other species. Even this\nwould not render the system productive,\nbut would merely enlarge it. But blend\ning might occur. Let AB represent the\nfood call and CD the danger call, each\na fairly complex phonetic pattern. Sup\npose a protohominoid encountered food\nand caught sight of a predator at the\nsame time. If the two stimuli were bal\nanced just right, he might emit the calls\nABCD or CDAB in quick sequence, or\nmight even produce AD or CB. Any of\nthese would be a blend. AD, for example,\nwould mean \"both food and danger.\" By\nc\nSTICKLEBACK\nCOURTSHIP\nD\nWESTERN\nMEADOWLARK\nSONG\nYES\nYES\nYES\n?\nYES\nYES?\nIN PART?\nIF SEMANTIC, YES\n?\n?\n?\n?\n?\nEIGHT SYSTEMS OF COMMUNICATION possess in varying de·\ngrees the 13 design.features of language. Column A refers to\nmembers of the cricket family. Column H concerns only Western\nmusic since the time of Bach. A question mark means that it is\n94© 1960 SCIENTIFIC AMERICAN, INC\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/termsout danger\" and \"danger without food.\"\nAnd all three of these calls-AB, CD and\nAD-would now be composite rather\nthan unitary, built out of smaller ele\nments with their own individual mean\ning�,\n: A ,;:ould m��n \"food\" ;,\nB, \"no �an\nger ; C, no food ; and D, danger.\nBut this is only part of the story. The\ngeneration of a blend can have no effect\nunless it is understood. Human beings\nare so good at understanding blends that\nit is hard to tell a blend from a rote repe\ntition, except in the case of slips of the\ntongue and some of the earliest and most\nE\nGIBBON CALLS\nF\nPARALINGUISTIC\nPHENOMENA\nYES\nYES\nYES\nLARGElY YES\nYES\nYES?\nYES?\nIN PART\nLARGElY NO\nIN PART\nascribed to man's prehuman ancestors. It\nmust be supposed, therefore, that occa\nsional blends occurred over many tens\nof thousands of years (perhaps, indeed,\nthey still may occur from time to time\namong gibbons or the great apes) , with\nrarely any appropriate communicative\nimpact on hearers, before the under\nstanding of blends became speedy\nenough to reinforce their production.\nHowever, once that did happen, the\nearlier closed system had become open\nand productive.\nIt is also possible to see how faint\nG\nLANGUAGE\nYES\nYES\nYES\nYES\nYES\nYES\nYES\nYES\nYES\nYES, OFTEN\nYES\nYES\nYES\nH\nINSTRUMENTAL\nMUSIC\nAUD I TORY,\nNOT VOCAL\nYES\nYES\n?\nYES\nYES\nNO ( IN GENERAL)\nIN PART\nYES\nYES\ndoubtful or not known if the system has the particular feature. A blank space indicates\nthat feature cannot be determined because another feature is lacking or is indefinite.\nductivity, duality and thoroughgoing\ntraditional transmission. Suppose an\nearly hominid, a man-ape say, caught\nsight of a predator without himself be\ning seen. Suppose that for whatever rea\nson-perhaps through fear-he sneaked\nsilently back toward others of his band\nand only a bit later gave forth the dan\nger call. This might give the whole band\na better chance to escape the predator,\nthus bestowing at least slight survival\nvalue on whatever factor was responsi\nble for the delay.\nSomething akin to communicative dis\nplacement is involved in lugging a stick\nor a stone around-it is like talking today\nabout what one should do tomorrow. Of\ncourse it is not to be supposed that the\nfirst tool-carrying was purposeful, any\nmore than that the Rrst displaced com\nmunication was a discussion of plans.\nCaught in a cul-de-sac by a predator,\nhowever, the early hominid might strike\nout in terror with his stick or stone and\nby chance disable or drive off his enemy.\nIn other words, the Rrst tool-carrying\nhad a consequence but not a purpose.\nBecause the outcome was fortunate, it\ntended to reinforce whatever factor,\ngenetic or traditional, prompted the be\nhavior and made the outcome possible.\nIn the end such events do lead to pur\nposive behavior.\nAlthough elements of displacement\nmight arise in this fashion, on the whole\nit seems likely that some degree of pro\nductivity preceded any great prolifera\ntion of communicative displacement as\nwell as any signiRcant capacity for tra\nditional transmission. A productive sys\ntem requires the young to catch on to\nthe ways in which whole signals are\nbuilt out of smaller meaningful elements,\nsome of which may never occur as whole\nsignals in isolation. The young can do\nthis only in the way that human children\nlearn their language: by learning some\nutterances as whole units, in due time\ntesting various blends based on that\nrepertory, and Rnally adjusting their pat\nterns of blending until the bulk of what\nthey say matches what adults would say\nand is therefore understood. Part of this\n!,earning process is bound to take place\naway from the precise situations for\nwhich the responses are baSically appro\npriate, and this means the promotion of\ndisplacement. Learning and teaching,\nmoreover, call on any capacity for tradi\ntional transmission that the band may\nhave. Insofar as the communicative sys\ntem itself has survival value, all this be\nstows survival value also on the capacity\n95© 1960 SCIENTIFIC AMERICAN, INC\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/termstem. A child can be taught how to avoid\ncertain dangers before he actually en\ncounters them.\nl' hese developments are also necessarily related to the appearance of\nlarge and convoluted brains, which are\nbetter storage units for the conventions\nof a complex communicative system and\nfor other traditionally transmitted skills\nand practices. Hence the adaptative\nvalue of the behavior serves to select\ngenetically for the change in structure.\nA lengthened period of childhood help\nlessness is also a longer period of plastic\nity for learning. There is therefore selec\ntion for prolonged childhood and, with\nit, later maturity and longer life. With\nmore for the young to learn, and with\nmale as well as female tasks to be taught,\nfathers become more domesticated. The\nincrease of displacement promotes reat the moment hunger for her.\nThere is excellent reason to believe\nthat duality of patterning was the last\nproperty to be developed, because one\ncan find little if any reason why a com\nmunicative system should have this\nproperty unless it is highly complicated.\nIf a vocal-auditory system comes to have\na larger and larger number of distinct\nmeaningful elements, those elements in\nevitably come to be more and more sim\nilar to one another in sound. There is a\npractical limit, for any species or any\nmachine, to the number of distinct stim\nuli that can be discriminated, especially\nwhen the discriminations typically have\nto be made in noisy conditions. Suppose\nthat Samuel F. B. Morse, in devising his\ntelegraph code, had proposed a signal\n.1 second long for \"A,\" .2 second long\nfor \"B,\" and so on up to 2.6 seconds for\n\"Z.\" Operators would have enormous\nSUBHUMAN PRIMATE CALLS are represented here hy sound spectrograms of the roar\n(top) and bark (bottom) of the howler monkey. Frequencies are shown vertically; time,\nhorizontally. Roaring, the most prominent howler vocalization, regnlates interactions and\nmovements of gronps of monkeys, and has both defensive and offensive fnnctions. Barking\nhas similar meanings hnt occnrs when the monkeys are not quite so excited. Spectrograms\nwere produced at Bell Telephone Laboratories from recordings made by Charles Southwick\nof the University of Southern Ohio during an expedition to Barro Colorado Island in the\nCanal Zone. The expedition was directed by C. R. Carpenter of Pennsylvania State University.\n96\npatterning. The telegraph operator has\nto learn to discriminate, in the first in\nstance, only two lengths of pulse and\nabout three lengths of pause. Each letter\nis' coded into a different arrangement of\nthese elementary meaningless units. Th�\narrangements are easily kept apart be\ncause the few meaningless units are\nplainly distinguishable.\nThe analogy explains why it was ad\nvantageous for the forerunner of lan\nguage, as it was becoming increasingly\ncomplex, to acquire duality of pattern\ning. However it occurred, this was a\nmajor breakthrough; without it language\ncould not possibly have achieved the\nefficiency and flexibility it has.\nOne of the basic principles of evolu\ntionary theory holds that the initial sur\nvival value of any innovation is con\nservative in that it makes possible the\nmaintenance of a largely traditional way\nof life in the face of changed circum\nstances. There was nothing in the make\nup of the protohominoids that destined\ntheir descendants to become human.\nSome of them, indeed, did not. They\nmade their way to ecological niches\nwhere food was plentiful and predators\nsufficiently avoidable, and where the de\nvelopment of primitive varieties of lan\nguage and culture would have bestowed\nno advantage. They survive still, with\nvarious sorts of specialization, as the\ngibbons and the great apes.\nMan's own remote ancestors, then,\nmust have come to live in circum\nstances where a slightly more flexible\nsystem of communication, the incipient\ncarrying and shaping of tools, and a\nslight increase in the capacity for tradi\ntional transmission made jqst the differ\nence between surviving-largely, be it\nnoted, by the good old protohominoid\nway of life-and dying out. There are\nvarious possibilities. If predators become\nmore numerous and dangerous, any\nnonce use of a tool as a weapon, any\nco-operative mode of escape or attack\nmight restore the balance. If food be\ncame scarcer, any technique for crack\ning harder nuts, for foraging over a\nwider territory, for sharing food so gath\nered or storing it when it was plentiful\nmight promote survival of the band.\nOnly after a very long period of such\nsmall adjustments to tiny changes of liv\ning conditions could the factors involved\n-incipient language, incipient tool-car\nrying and toolmaking, incipient culture\nhave started leading the way to a new\npattern of life, of the kind called human.© 1960 SCIENTIFIC AMERICAN, INC\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/termsA human talent\nThe June issue of this magazine contained an article full of learned speculation\non the neurological mechanism by which lines, straight and curved, are perceived.\nWhatever the mechanism, the nervous system is very good at seeing a line from\nexceedingly faint physical stimuli. We had been thinking about ways this talent\ncould help solve the nasty signal-to-noise problem that keeps cropping up on such\noccasions as when defense from submarine attack is considered. Today's almost\ninstantly available photography makes a fine bridge from an electronic system to a\nhuman nervous system. For example:\n1. Instead of an ordinary A-scope trace like � let's modulate intensity and sweep over mavthis... ing film with much overlap...\n2. so that even when the significant pulse � photographic summing-up finds it rather easily;\nstands out from the noise no more than this...\n3. and even when the A-scope shows only � the weak but non-random blip holds position\nthis... and builds up from all the sweeps to where\nthe marvelous combination of photography\nand the human perceptive mechanism says,\n\"There!\"\nOrganizations active in military developmenls who wish to know more about this work\nshould communicate with Eastman Kodak Company, Apparatus and Optical Division,\nRochester 4, N. Y.\nCreamed butyrate\nIn this nation of do-it-yourselfers and\nof housewives capable of taking the\nbit in their own teeth when occasion\ndemands, do you think there would be\na market for a cream that can be spread\nover bare wood with cheesecloth to\ndeposit in seconds a surface chemically\nand physically identical to a coat of\nhighest quality lacquer?\nWe have made such a cream-a\nstable, freeze-and-thaw-resistant water\nemulsion of the same kind of cellulose\nacetate butyrate on which the best\ngrades of conventional lacquers are\nbased.\nThe cream eliminates separate fill\ners, sealers and wash coats, long dry\ning periods, excessive sanding opera\ntions, and spraying equipment. With\none, two, or three coats a range of\neffects can be produced from a flat\n\"natural\" surface to a rich, semi\nglossy, \"rubbed\" surface. The fast\nfilm formation permits application of\nsuccessive coats within minutes and\neliminates the problem of surface im\nperfections from dust in the air. Gentle\nrubbing as the film forms fills the ir\nregularities in the wood and smooths\nout the top of the lacquer. Though\nwater-based, the cream does not raise\ngrain. After drying, the film has good\nresistance to water. It adheres well to\nthe wood, seals it well, prevents pene\ntration of subsequently applied con\nventional finishes (if they are desired)\nbut holds them tenaciously.\nThe product itself is almost water\nwhite, with the color stability to sun-\n• light for which all cellulose acetate\nbutyrate coatings have been esteemed.\nIt neither darkens wood nor is itself\ndarkened with the passage of time.\nAll these interesting properties we have\ndemonstrated to our own satisfaction. The\nilltricacies of marketing stich a produci\nthrough paint stores, stlpermarkels, five\nalld-dimes, or similarly formidable retail\nchannels fill us with dismay. Therefore we\nthought we would here ask around what\ncompanies are interested in trying to make\nhay wilh this lovely development. if indeed\nIhere are any such companies, Eastman\nChemical Producls inc., Kingsport, Tenll.\n(Subsidiary of Eastman Kodak Company)\nwill tell them all about emulsified butyrate.\nAmylose and culture\nSpaghetti and macaroni are basic.\nThe idea of making wheat flour up\ninto a paste and drying it for future\nuse must have come very early. Enter\nesthetics. The human spirit must be\nnourished along with the human body.\nFor reasons apparently unrelated to\nbiological metabolism, the paste must\nbe dried in certain shapes, and the in\ntegrity of these shapes must be preserved\nright to the pearly portal of the alimen\ntary tract. This principle is ancient: the\nancient Romans ate spaghetti with\ncheese; the ancient Japanese ate maca\nroni pressed from a paste of cooked\nrice.\nWhen spaghetti or macaroni is\ncooked for too long or allowed to\nstand cooked, the human spirit is\noffended. The morsels of pasta revert\nto a sticky paste, millenia of cultural\nadvance undone because amylose has\ngone into solution and then has loosely\nhydrogen-bonded itself into a net of\nslime. But for this unfortunate tend\nency, the world's food supply would be\nless dependent on specialized durum\nwheats. Without them, the spaghetti\nand macaroni would get even stickier.\nThe problem now appears to be as\nsoluble as the amylose itself.\nFirst fruits of the victory can already\nbe tasted. Try any of the up-to-date\ndehydrated potato-flake brands. Com\npare with home-whipped potato.\nWhatever the future holds for spaghetti\nand macaroni, the reason the instant-potato\nthing works out so well is that the processors\nadd a very small percentage of pure mono\nglyceride. it complexes the dissolved\namylose so securely that even the familiar\niodine-blue test can scarcely find it.\nThese Myverol Distilled Monoglycer\nides we prepare by glycerolysis of familial'\nvegetable and animal food fats. They are\nofficially recognized as safe. investigators\nwho would like samples of them with which\nto try remedying stickiness ill\nany starchy foods are invited\nto write Distillation Producls\nIndustries, Rochester 3,\nN. Y.(Division ofEast\nman Kodak CompaIlY).\nThis is another advertisement where Eastman Kodak Company probes at random for mutual interests\nand occasionally a IiHle revenue from those whose work has something to do with science© 1960 SCIENTIFIC AMERICAN, INC\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n74.111.45.22 on Sun, 28 Oct 2018 19:32:23 UTC��������������\nAll use subject to https://about.jstor.org/terms", "affiliations": [{"country": "United States", "discipline": "Linguistics", "university": "Cornell University"}], "species_categories": ["Fish", "Bird", "Primate", "Other"], "specialized_species": ["stickleback", "gibbon", "howler monkey", "meadowlark"], "computational_stages": ["Data Collection"], "linguistic_features": ["Vocal Auditory Channel and Turn-taking", "Semanticity", "Arbitrariness and Duality of Patterns", "Discreteness and Syntax", "Recursion", "Productivity", "Traditional and Cultural Transmission", "Learnability"], "status": "saved", "created_at": "2026-01-13T12:49:59.882639", "updated_at": "2026-01-13T13:11:57.615106", "committed_at": "2026-01-13T13:07:29.650118"}
{"id": "6161760c-d002-497b-9782-7f1da871009a", "title": "A Critical Companion to Zoosemiotics", "authors": ["Martinelli,  Dario"], "year": "2010", "journal": "Biosemiotics", "abstract": "", "doi": "10.1007/978-90-481-9249-6", "analysis_notes": "A critical companion of zoosemiotics is the first attempt to systematise the study of animal communication and signification through its most important and/or problematic terms and concepts, and its most representative scholars. It is a companion, in that it attempts to cover the entire range of key terms in the field, and it's critical, in that it aims not only to describe, but also to discuss, problematise and, in some cases, resolve, these terms.\n", "affiliations": [{"university": "University of Helsinki", "country": "Finland", "discipline": "Other"}], "species_categories": ["Primate", "Marine Mammal", "Terrestrial Mammal", "Bird", "Insect"], "specialized_species": ["Chimpanzee", "Gorilla", "Bonobo", "Orangutan", "Bottlenose Dolphin", "Humpback Whale", "African Grey Parrot", "Vervet Monkey", "Greater Spot-nosed Monkey", "Honeybee", "Wolf", "Lyrebird"], "computational_stages": ["Data Collection", "Sequence Representation", "Meaning Identification", "Generation"], "linguistic_features": ["Vocal Auditory Channel and Turn-taking", "Broadcast and Direct Reception", "Reference and Displacement", "Specialization", "Arbitrariness and Duality of Patterns", "Discreteness and Syntax", "Recursion", "Semanticity", "Prevarication", "Openness", "Tradition and Cultural Transmission", "Learnability"], "status": "saved", "created_at": "2026-01-13T12:49:59.882657", "updated_at": "2026-01-13T13:18:02.450736", "committed_at": "2026-01-13T13:18:17.657152"}
{"id": "2955d7b2-5dc8-4e76-8426-e9f10c023de5", "title": "Derived vocalizations of geladas (Theropithecus gelada) and the evolution of vocal complexity in primates", "authors": ["Gustison,  Morgan L.", "le Roux,  Aliza", "Bergman,  Thore J."], "year": "2012", "journal": "Philosophical Transactions of the Royal Society B: Biological Sciences", "abstract": "", "doi": "10.1098/rstb.2011.0218", "analysis_notes": "Research\nDerived vocalizations of geladas\n(Theropithecus gelada) and the evolution of\nvocal complexity in primates\nMorgan L. Gustison1,*, Aliza le Roux1,3 and Thore J. Bergman1,2\n1Department of Psychology, and 2Department of Ecology and Evolutionary Biology, University of\nMichigan, Ann Arbor, MI 48109, USA\n3Department of Zoology and Entomology, University of the Free State, Qwaqwa 9866, South Africa\nPrimates are intensely social and exhibit extreme variation in social structure, making them particularly well suited for uncovering evolutionary connections between sociality and vocal complexity.\nAlthough comparative studies find a correlation between social and vocal complexity, the function\nof large vocal repertoires in more complex societies remains unclear. We compared the vocal complexity found in primates to both mammals in general and human language in particular and found\nthat non-human primates are not unusual in the complexity of their vocal repertoires. To better\nunderstand the function of vocal complexity within primates, we compared two closely related primates (chacma baboons and geladas) that differ in their ecology and social structures. A key\ndifference is that gelada males form long-term bonds with the 2– 12 females in their harem-like\nreproductive unit, while chacma males primarily form temporary consortships with females.\nWe identified homologous and non-homologous calls and related the use of the derived nonhomologous calls to specific social situations. We found that the socially complex (but ecologically\nsimple) geladas have larger vocal repertoires. Derived vocalizations of geladas were primarily used\nby leader males in affiliative interactions with ‘their’ females. The derived calls were frequently\nused following fights within the unit suggesting that maintaining cross-sex bonds within a reproductive unit contributed to this instance of evolved vocal complexity. Thus, our comparison highlights\nthe utility of using closely related species to better understand the function of vocal complexity.\nKeywords: derived vocalizations; group size; homologous vocalizations; social complexity;\nvocal complexity; vocal repertoire\n1. INTRODUCTION\nThe complexity of vocal communication varies enormously across species, from humans with an endless\nrepertoire of sound combinations, to species of mongoose\nthat produce only three different sounds [1]. As we continue to document the diversity that exists in nature, we\nare increasingly able to use comparative studies to identify\nthe selective pressures responsible for increasing vocal\ncomplexity. One of the most salient findings that has\nemerged is that high levels of sociality are found in combination with a high degree of vocal complexity [2–5].\nFor example, ground-dwelling sciurid species with\nsocially complex groups (i.e. many age/sex classes) produce more acoustically distinct alarm calls than species\nwith fewer age/sex classes [2]. Although non-primate\ntaxa can be excellent study subjects for investigating the\nevolution of vocal complexity in general (e.g. rodents:\n[6]; bats [5]) primates, as our closest relatives, can provide insight into the evolution of the most complex\nvocal system—our own [7,8]. Moreover, primates exhibit\nextreme variation in social structure, making them\nparticularly well suited for uncovering evolutionary\nconnections between sociality and vocal complexity.\nFacets of primates’ sociality distinguish them from\nmost other mammals. First, primates exhibit an unusual\ndegree of sociality that some have proposed has resulted\nin a kind of ‘Machiavellian intelligence’ [9,10] in that\nindividuals are capable of forming coalitions [11],\ndeceiving others [12] and maintaining strong, longterm social bonds with both kin and non-kin [13–15].\nSecond, primates are unusual among mammals in that\nthe size of their groups is positively associated with\nsome aspects of brain size [10,13]. One intriguing explanation for this relationship is that primates require\nsophisticated cognitive abilities for keeping track of and\nmaintaining complex networks of social relationships\n[9,16,17]—particularly considering recent findings\nthat social networks actually enhance individuals’ fitness\n[18,19]. Offering further support, the strong positive\nrelationship between group size, time spent grooming\nand diversity of vocal repertoire in primates [20] suggest\nthat more vocalizations may indeed be necessary for\nnavigating the complex network of social relationships\nin primate societies.\n* Author for correspondence (gustison@umich.edu).\nOne contribution of 13 to a Theme Issue ‘The social network and\ncommunicative complexity in animals’.\nPhil. Trans. R. Soc. B (2012) 367, 1847–1859\ndoi:10.1098/rstb.2011.0218\n1847 This journal is q 2012 The Royal SocietyIn this review, we first focus on the evolution of\nvocal complexity in primates, and then propose a\nnovel approach for studying the function of vocal complexity. Although a species’ repertoire size provides\none useful comparative metric, it is a composite\nmeasure with no information about the function of\nthe individual calls that comprise it. Here, we propose\nthat the function of vocal complexity can be understood by comparing vocalizations among closely\nrelated species with differing repertoire sizes to identify\nspecies-specific derived calls. In such cases, the function of greater vocal complexity matches the function\nof the derived calls. As an example of how this\napproach can provide insights about the evolution of\ncomplexity, we compare the vocalizations of geladas\n(Theropithecus gelada) and chacma baboons (Papio\nursinus)—two closely related Old World monkeys\nwith overlapping vocal repertoires but very different\necological and social structures.\n2. VOCAL REPERTOIRES OF PRIMATES\nAND OTHER MAMMALS\n(a) Repertoire size as a measure of complexity\nMammalian vocal communication is typically\ndescribed as being made up of discrete, functional\nunits, or ‘calls’ [21 – 23]. Based on these functional\nunits, vocal complexity is quantified in terms of (i)\nnumber of discrete vocalizations in the repertoire\n(repertoire size) [2,20,24,25], or less commonly, (ii)\ndegree of individuality within discrete calls [5,6,26].\nOther ways of assessing vocal complexity include quantifying syllable complexity, amount of information\ncontained within a call [5], or the number of calls\nwithin a specific category of vocalizations (e.g. alarm\ncalls [2]).\nSeveral mammalian species produce call variants, or\ngraded calls, which vary slightly in acoustic properties\n[27– 31], such as fundamental frequency [28], ‘pitch’\n[29] or duration [24] and, as a result, have different\nmeanings to receivers [29,32– 34]. For species with\nsmall, fixed vocal repertoires, these subtle alterations\nmay help to extend the flexibility of an otherwise limited repertoire [7]. However, identifying graded calls\nrequires detailed acoustic and behavioural analyses\nand data of comparable detail are rarely available for\nmultiple species. Therefore, as has become the convention in vocal studies [2,20,24,25], we refer to\nrepertoire size as the number of discrete calls that\nanimals in a population or species produce.\n(b) Vocalization types\nVocalizations are produced in many different contexts.\nSome are produced in response to external stimuli such\nas predators and food. We call these ‘allospecific’ vocalizations (table 1) and include alarm calls and food calls.\nAlarm calls can communicate the degree of risk\ninvolved [2,24], indicate predator type (i.e. aerial or\nterrestrial) [53,54], or combine information on risk\nand predator type [51]. Primates, in particular, are\nknown to produce alarm vocalizations specific to predator type, eliciting appropriate responses in receivers\n[55– 57]. Notably, the complexity of primate alarm\ncalls is generally attributed to a complex physical\n(rather than social) environment [58]. If different predators have different modes of hunting, primate prey\nshould have evolved different predator responses to\neach. By contrast, the complexity of alarm calls in a\nsmall social carnivore, the meerkat (Suricata suricatta),\nhas been attributed to the need for social coordination\n[4]. Relative to a sympatric-living herbivore species like\nCape ground squirrels (Xerus inauris), meerkats travel\nfarther from underground shelters in their open habitat\nto find living prey, and they depend on ‘sentinels’ to\nemit referential alarm calls that vary acoustically\nbased on predator type [4]. This strategy allows individual meerkats to decrease time spent being vigilant\nand increase foraging efficiency.\nThe other allospecific vocalizations, food calls, are\nless variable than alarm calls. Only a handful of studies\nhave demonstrated that variation in calls is related to\nthe quantity or quality of the food source [59]; but\nmost studies report a lack of variation [60]. Although\nalarm and food calls differ substantially in degree of\ncomplexity, they share two features: (i) they are elicited\nby non-conspecifics, and (ii) they are the only two contexts where proto-syntax (i.e. the combination of call\nelements to form new meanings) has been reported,\nspecifically in primates (food calls [61]; alarm calls [62]).\nThe vast majority of mammalian vocalizations are\nemitted during social interactions with conspecifics,\nunder conditions of varying motivational states (e.g.\nmating, aggression, fear). We call these ‘social’ vocalizations and divide them into two main classes—calls\nthat function over a long distance (‘loud calls’) and\ncalls produced in close-range social interactions\n(‘close-range calls’, table 1). Loud calls may function\nto attract or defend mating partners [63], defend a\nterritory or food source through maintenance of intergroup spacing [42,64], or re-establish contact with\ngroup members that are out of sight (‘separation\ncalls’) [38].\nClose-range calls are produced in agonistic, neutral\nor affiliative contexts. Calls produced in agonistic\ncontexts may function to assess or warn rivals, such as\ncontest calls that advertise fitness [42] or threat calls\nthat maintain a dominance hierarchy [46,65]. Harassed\nindividuals, on the other hand, may produce distress\ncalls, which probably function to appease the aggressor\nand attract coalition partners [50]. The specific function of some close-range calls made in strictly neutral\nor affiliative social situations has been more difficult\nto ascertain and little is known about them besides\nthe contexts in which they are produced [7]. Copulation calls, for example, do not appear to have the\nsame function across different species and may even\nserve no function in some instances [66,67]. Other\nclose-range calls are ascribed to an affiliative function,\noften described as ‘contact calls’ (e.g. in raccoons (Procyon lotor) [65]; capuchins (Cebus capucinus) [68]).\nContact calls can be produced in various ‘friendly’ contexts, such as during post-conflict reconciliation\ninteractions [69] and prior to friendly behaviours like\nallogrooming [50,70,71]. These close-range contact\ncalls are also produced in more ‘neutral’ behavioural\nstates like foraging and resting, and therefore could be\ninvolved in the maintenance of group cohesion and\ninter-individual spacing [72].\n1848 M. L. Gustison et al. Primate vocal complexity\nPhil. Trans. R. Soc. B (2012)Primate vocal repertoires are similar to those of\nother terrestrial mammals (table 1). Although primate\nrepertoires may be slightly larger (on average), there is\nconsiderable overlap between primates and other taxa,\nboth in total repertoire size and within each category of\ncalls. Species with large repertoires relative to other\nspecies in their order generally produce a large proportion of calls in just one or two categories of calls\n(e.g. long distance and competitive calls—Callicebus\nmoloch [38]; distress and contact calls—Pan troglodytes\n[46]; allospecific and contact calls—Suricata suricatta\n[51,52]). This suggests that specific needs related to\none domain (e.g. competition or affiliation) might\ndrive the development of large repertoires, rather\nthan an overall increase in repertoire across all categories. Within primates, no clear taxonomic pattern\nhas emerged with respect to repertoire size. Each\nfamily of primates (including great apes) contains\nspecies with large and small repertoires. Surprisingly,\ndespite the social complexity of primates, there is no\nconsistent trend for primates to have more social calls\nthan other mammals, which suggests that simple comparisons of numbers of calls are of limited utility.\n(c) Function of larger repertoires\nOne of the primary hypotheses put forward to explain\nlarge, complex vocal repertoires is that social complexity creates the need for more vocalizations [21,73– 76].\nComparative studies have found a positive relationship between social complexity and communicative\ncomplexity, providing support for this hypothesis\n[2,3,5,20]. In sciurids, the alarm call repertoire size\nincreases with the number of demographic ‘roles’\n[2]. Additionally, in primates, an increase in total\nvocal repertoire size was associated with both larger\ngroups and increases in time spent grooming—a\nmeasure of social cohesion [20]. These studies have\nbeen important for pinpointing aspects of sociality\nTable 1. Vocal repertoire size for exemplar species from Primata, Rodentia, and Carnivora broken down into six categories:\nAllospecific (alarm calls and food calls), long distance (separation calls, intergroup spacing calls), contact (short-range soft\ncalls), competitive (threat and display calls), distress calls (fear calls during agonism) and other (contexts unknown or made\nin several different contexts). Sources from Primata are drawn from the repertoire analysis made by McComb & Semple\n[20], excluding captive studies. Total repertoire sizes in this paper are slightly different because we did not count sequences\nof discrete call units as separate calls if the units were produced singly. Sounds that are not strictly ‘vocalizations’, such as\nsneezes, coughs and teeth chattering, are excluded from the table. For comparison, we focus on exemplar species from\nRodentia and Carnivora because of similarities in social and vocal behaviour.\nspecies name allospecific long distance contact competitive distress other total size citations\nOrder Primata\nAlouatta palliata — 1 — 8 2 1 12 [35]\nArctocebus calabarensis — 1 — 1 1 — 3 [36]\nCallicebus moloch 1 3 1 1 3 1 10 [37]\nCallimico goeldii 4 7 3 6 4 1 25 [38]\nCebus olivaceus — — 4 4 2 1 11 [39]\nCercocebus torquatus 4 1 3 5 1 — 14 [40]\nCercopithecus aethiops 5 — 3 5 3 3 19 [41]\nEuoticus elegantulus 2 1 2 — 1 — 6 [36]\nGalago alleni 1 — 2 1 1 — 5 [36]\nG. demidovii 1 1 3 1 1 1 8 [36]\nMacaca fascicularis 2 2 1 1 4 5 15 [42]\nM. radiata 1 1 3 7 4 3 19 [43]\nM. silenus 1 2 3 5 2 1 14 [43]\nMandrillus sphinx 1 3 1 — 4 — 9 [44]\nPan paniscus — — 4 — 1 4 9 [45]\nP. troglodytes 3 1 8 4 8 3 27 [46]\nPerodicticus potto — 1 2 1 1 5 [36]\nPongo pygmaeus 1 1 1 2 2 1 8 [47]\nPresbytis entellus 3 1 2 3 3 2 14 [43]\nP. johnii 2 1 3 4 3 1 14 [43]\nProcolobus badius 2 — — 5 2 2 11 [48]\nOrder Rodentia\nNotomys alexis — — 1 1 1 2 5 [23]\nN. cervinus — — — 1 1 2 4 [23]\nN. mitchellii — — 1 1 1 2 5 [23]\nN. fuscus — — 1 1 1 2 5 [23]\nOctodon degus 2 — 4 5 2 — 13 [49]\nOrder Carnivora\nLycaon pictus 3 2 7 5 2 6 25 [50]\nSuricata suricatta 11 — 7 2 2 3 25 [51,52]\nCynictis penicillata 3 — 2 2 — 1 8 [24]\nSpeothos venaticus — 1 3 2 1 — 7 [27]\nCerdocyon thous 2 1 2 1 — — 6 [27]\nChrysocyon brachyurus 1 2 2 2 1 — 8 [27]\nPrimate vocal complexity M. L. Gustison et al. 1849\nPhil. Trans. R. Soc. B (2012)(i.e. large sociable groups, various demographic roles)\nthat may drive the evolution of large repertoires. However, vocal repertoires of different species may be\n‘large’ for different reasons (table 1), and more work\nis clearly needed to understand the selective pressures\nunderlying expansions in repertoires.\n3. VOCAL COMPLEXITY OF HUMANS AND\nOTHER PRIMATES\nRelative to humans, non-human primates (henceforth\nreferred to as ‘primates’) exhibit surprisingly simplistic\nvocal production [77,78]. (Note that a focus on vocal\nproduction ignores the more sophisticated language-like\nabilities that primates exhibit in terms of vocal perception [79]). According to the ‘source-filter-theory’ first\ndeveloped to describe human speech [80,81], vocal\nproduction entails two components: the ‘source’ of a\nvocalization (i.e. lungs and the vocal folds) and the\nmeans by which a vocalization is shaped, or ‘filtered’,\nin the vocal cavities (i.e. vocal tract). Speech relies\nheavily on the control of ‘formants’ or vocal resonances (a product of vocal tract morphology) to\nproduce distinct syllables and hence encode information [81]. Primates also produce formants but the\nformant structure (i.e. distance between sound frequency ‘peaks and valleys’ [81]) mainly encodes\nlimited information such as individual identity\n[82,83] and body size [84 – 86]. Even more elaborate\nare humans’ filtering tools, the descended larynx and\ntongue [81]. In most primates and other mammals,\nthe tongue remains flat inside the mouth. By\ncontrast, humans have remarkable control over the\nlocation and shape of the tongue [81,87], giving\nhumans unmatched plasticity in sound invention\n[88]. This unique vocal plasticity allows us to imitate\ncomplex sounds and invent novel sounds, a feat\nshared with some birds and cetaceans [89,90] but\nnot with other primates [77,78].\nDespite having a limited ability to imitate and create\nnew sounds, there are some features of primate vocal\nproduction that show similarities to human language.\nFor instance, some primates exhibit vocal dialects—\ngeographical variation in the acoustic structure of\ncertain vocalizations [91– 93]. Calls are recognizably\nhomologous between different populations of the\nsame species, but show acoustic distinctions related to\nvariation in habitat and the duration of isolation, similar\nto patterns in human linguistic diversity (e.g. [94]).\nAdditionally, primates such as chimpanzees (Pan\ntroglodytes) [95,96], blue monkeys (Cercopithecus mitis)\n[97] and capuchins (Cebus apella) [98,99] produce or\nsuppress vocalizations depending on the composition\nof the conspecific audience.\nPrimate communication also resembles the semantic\ncontent of human language. Several primates exhibit\npotentially ‘referential’ allospecific calls that are elicited\nby external stimuli (table 1). In some cases, the referential nature of these calls has been supported with\nplayback experiments. For example, each vervet\nmonkey (Chlorocebus pygerythrus) alarm call ‘refers’ to\na different type of predator (leopards, eagles and\nsnakes). Experimental playbacks in the wild indicate\nthat these different alarm calls produce different\npredator-appropriate responses in the absence of a\npredator [56]. In further support of the functionally\nreferential nature of primate vocalizations, habituation – dishabituation experiments on Diana monkeys\n(Cercopithecus aethiops) demonstrated that playbacks of\nleopard alarm calls or leopard growls resulted in predator-appropriate responses. These results suggest that\nDiana monkey responses are based on the underlying\nreferent (the predator) rather than any differences in\nthe calls’ acoustic properties [100].\nDespite some language-like properties of primate\ncommunication, humans exhibit unrivaled flexibility\nin mixing and matching different sounds to create\nnew meanings through syntax [77,78,101]. Very few\nmammalian species use combinations of calls, and\neven those that do are unlikely to use these combinations to generate new meanings. There are only a\nfew known cases where primates combine calls in\nways that change the meaning of the call elements\n(red-capped mangabeys (Cercocebus torquatus) [102];\nCampbell’s monkeys (Cercopithecus campbelli) [61];\nDiana monkeys (Cercopithecus diana) [57]). Importantly, these semantic combinations of sounds only\ncomprise a few specific elements and are highly constrained [102,103]. Non-human primate vocal\n‘productivity’ [104] is therefore far simpler than\nhuman communication and may, at best, be labelled\nas ‘proto-syntax’—a term that refers to rule-governed,\nrather than random, combinations of discrete sounds\nthat lack the sophistication of human grammar [61].\n4. FUNCTION OF DERIVED VOCALIZATIONS\nAlthough previous studies have been pivotal for identifying aspects of sociality that drive vocal complexity,\nwe still know relatively little about how large vocal\nrepertoires function in complex societies. One reason\nfor this is that comparisons of repertoire size alone\nfail to identify the specific calls that may have evolved\nin association with social complexity. With no knowledge about which calls are derived, we can say\nnothing about how those calls function. Another\nreason is that comparisons of group size alone fail to\nidentify the specific features about group life that\nrequire an increase in vocal complexity. Thus, several\nquestions remain unanswered: first, what specific\naspects of sociality create a need for vocal complexity?\nIs it the number of relationships, the nature of relationships or something else? Second, can we identify the\nderived components of the vocal repertoire that\nrelate to the demands of increased sociality? That is,\nif more social species have more calls, how are they\nusing these ‘extra’ calls?\nTo help answer these questions, we propose a systematic investigation of closely related species that\nmake detailed comparisons of the functions of ‘homologous’ (shared between species) and ‘derived’\n(unique to a species) vocalizations. Note that, although\na vocalization may be unique to a species because it was\npresent in the common ancestor and lost in the other\nspecies, we call them derived calls for simplicity,\nalthough the direction of the change (gain or loss)\nremains a hypothesis that can be examined by comparison to an outgroup. Previous studies have used\n1850 M. L. Gustison et al. Primate vocal complexity\nPhil. Trans. R. Soc. B (2012)comparisons among closely related species to understand vocal evolution [71] although not with the goal\nof understanding vocal complexity per se. In the\nprimate literature, several researchers have made comparisons of vocal behaviour between related species\n[36,43,105,106]. These studies often include general\nsimilarities and differences of call categories [36],\nacoustic structure [105] and/or contextual use [43]. In\none case [43], there was a clear attempt to identify\nhomologous and unique calls in two species of macaque\n(bonnet macaque (Macaca radiata) and lion-tailed\nmacaque (M. silenus)) and two species of langurs (Nilgiri\nlangur, Presbytis johnii ) and common langur (P. entellus));\nhowever, much of the ensuing analyses focused on the\ndifferential use of the homologous calls rather than\nexplaining the function of unique calls. The only study\nto date to focus on unique calls [44] compared the\nvocal behaviour of the forest-dwelling mandrill (Mandrillus sphinx) to published accounts of savannah-living\nbaboons (Papio spp.) and geladas (Theropithecus\ngelada). Kudo reported that mandrills produced two\nunique long-distance contact calls instead of the various\nshort-range calls made by baboons and geladas. Kudo\nproposed that this difference was likely due to ecological\npressures, as low-amplitude vocalizations do not travel\nwell in a forested environment where visual contact is\nalso limited [44].\nIdentifying homologous and derived vocalizations is\ncritical for identifying the specific social or ecological\nfactors that may account for complex vocal repertoires.\nHere, we use a comparison of the vocal complexity in\ngeladas with that in chacma baboons to demonstrate\nhow this homologous-derived vocalization strategy\nmay be implemented. By analysing calls from both\nspecies (all obtained from wild populations under\nnatural conditions), we control for variability in how\ncalls are classified which may drive some of the variation in overall repertoire size found in meta-analyses.\n5. GELADAS AND BABOONS—A CASE STUDY\nEarly researchers were struck by the intricate vocal behaviour of geladas as well as their unusually complex\nsocial groups [107 – 109]. Although some have proposed a causal connection between these factors\n[107], little progress has been made towards understanding why geladas, the only extant Theropithecus\nspecies, have elaborate vocal communication compared\nwith other primates. Thus, a comparison between the\nvocal behaviour of geladas and Papio baboons serves\ntwo purposes. First, these two taxa split relatively\nrecently (about 4 Myr ago), and Theropithecus and\nPapio are probably sister genera [110]. To human observers, they appear to produce similar calls in similar\ncontexts (e.g. affiliative grunts, threat grunts and\nalarm calls). It is therefore relatively straightforward\nto identify homologous calls, and simultaneously, to\npinpoint the unique call types that result in differences\nin vocal repertoire size. We can then assess how these\nunique calls are used to highlight the selective pressures\nthat may have favoured greater vocal complexity.\nSecond, the differences in the social system and\necology of geladas and baboons make the comparison\nparticularly useful for testing contrasting predictions\nabout the evolution of behavioural differences [111].\nBoth species live in matrilineal groups in which males\ndisperse [108,112,113]. Geladas aggregate into a\nmulti-level, fission – fusion society (forming groups as\nlarge as 1100 individuals) [114,115] and within this\ngroup they only recognize and primarily interact with\na small subset of the individuals within ‘harem-like’\nreproductive units of 2– 15 individuals [108,114 –\n117]. In these reproductive units, ‘leader’ males must\nmaintain social relationships with several females, and\nit is thought that maintaining close social bonds with\nhis unit females may serve to decrease the likelihood\nthat he will be out-competed by a non-unit, ‘bachelor’\nmale [118,119]. In contrast to their complex social\nsystem, gelada diets are simple and specialized, with\ngrass as the primary food item [120 –122].\nUnlike geladas, many baboons (e.g. chacma baboons\n(Papio ursinus)) have a single-level, multimale –\nmultifemale society with no discrete reproductive\nunits (20– 120 animals, [123 – 125]). Baboons maintain\ndifferentiated relationships based on kinship and dominance with all members of their group, but cross-sex\nrelationships consist mainly of temporary consortships\n[123 – 126]. In terms of ecology, baboons are extremely\ncomplex; they live in a range of habitat types and consume anything from fruits and seeds to insects and\nvertebrates [127 – 130].\nGiven that geladas and baboons differ in their sociality and ecology, we predict corresponding differences\nin the call types comprising their vocal repertoires. For\ninstance, geladas—specifically males—may produce\nmore types of calls that are used in affiliative situations.\nOn the other hand, baboons may use proportionally\nmore allospecific calls to communicate about general\nfeatures of the environment such as food items. To\ntest our predictions, we compared the vocal behaviour\nof geladas with one representative of the Papio genus—\nthe chacma baboon [44]—to identify derived call\ntypes. While we recognize that vocalizations from a\nsingle population may obscure variation within the\ngenus, both the literature and our experience with\nmultiple types of Papio baboons suggests that repertoire variation within Papio is minimal [44] and that\nthe types of vocalizations used by chacma baboons\nare very similar to even the socially-divergent Papio\nspecies, P. hamadryas [44,131]. Furthermore, our\ndescriptions of gelada vocalizations closely match\nthose from captive geladas [132] suggesting that such\nvocalizations are not unique to one population. For\nany derived vocalizations, we then conducted intraspecific analyses to determine their possible functions.\n(a) Study subjects\nData for this study come from 14 units within three\ndifferent bands in one community of wild geladas\n(about 1200 individuals) living in the Sankaber area\nof the Simien Mountains National Park, Ethiopia\n(2008 –2010) [113,114] and a single group of\nchacma baboons (group C) living in the Moremi\nGame Reserve in the Okavango Delta of Botswana\n(2001 –2002). The gelada units comprised one\nleader male, 0– 3 follower males, and 1– 11 females\nand their immature offspring. The gelada habitat\nPrimate vocal complexity M. L. Gustison et al. 1851\nPhil. Trans. R. Soc. B (2012)consisted of high-elevation open grassland and adjacent escarpments (sleeping sites). The chacma\nbaboon group ranged from 82 to 91 individuals,\nincluding 9 – 11 adult males, 29 – 31 adult females,\nand their immature offspring. The baboon habitat\nwas patchy scrub forest interspersed with seasonally\nflooded grasslands.\n(b) Comparison of gelada and chacma\nvocal repertoires\n(i) Data analysis\nWe opportunistically recorded vocalizations from 81\nadult geladas (males ¼ 36; Feb 2008–Apr 2010) and\n32 adult chacma baboons (males ¼ 11; Apr 2001–\nMay 2002) with a Sennheiser ME66 directional microphone connected to a digital stereo recorder (Marantz\nPMD 660 Digital Recorder for geladas; Sony VW-D6\nProfessional Walkman for chacma baboons). The call\ntypes and contexts of all vocalizations were described\nat the time of recording. Our analyses focus on\ncommon calls that occurred repeatedly during focal\nsampling and we do not attempt to describe all vocalizations produced in each species. The inter-observer\nreliability (between assignments made in the field and\nassignments that were blind to previous designations\nand based on isolated calls in the absence of contextual\ninformation) of a subset of these calls (five exemplars/\ncall type/species/sex class) was 96 per cent. We used\nAVISOFT (v. 5.1.12, R. Specht, Berlin) to generate\nspectrograms with a fast Fourier transformation size\nof 1024 points. Focusing on spectrograms with high\nsignal-to-noise ratio, we categorized call types by ear,\nvisual inspection of the spectrograms and the contexts\nin which they occurred (chacma females ¼ 50 calls;\nchacma males ¼ 32 calls; gelada females ¼ 72 calls;\ngelada males ¼ 92 calls). There were an equal\nnumber of calls per individual (within species/sex\nclass) for each call type (n ¼ 1 –3 call replicates per\nindividual, 6 –12 total calls per call type). We optimized\nthe frequency range of different call types (11 or\n22 kHz) where appropriate (time resolution of\n2.667–2.903 ms and a 100% frame).\nWe used Avisoft to quantify eight temporal and spectrum-based acoustic parameters in the spectrograms:\nduration, mean bandwidth, frequency under which\n25 per cent of the call’s energy lies (start, maximum\nand mean), number of harmonic peaks under 20 dB\n(maximum and mean), maximum peak frequency.\nThen, to determine the probability of correctly assigning each vocalization to a pre-categorized call type, we\nperformed stepwise discriminant function analyses\n(DFAs) with a subsequent leave-on-out cross-validation procedure for each of the four species/sex\nclasses separately [133]. We used multivariate analyses\nof variance (MANOVAs) to verify the significance of\nthe final DFA parameters. Finally, we identified homologous calls between species based on both acoustic\nand contextual similarity.\n(ii) Results\nMale and female geladas and chacma baboons produced a range of allospecific and social calls used in\nboth affiliative contexts (e.g. grooming and\ncopulation) and non-affiliative contexts (e.g. challenge\ndisplays and dominance interactions), with geladas\nproducing a greater number of call types (table 2).\nOf the 14 call types we identified, eight were found\nin both geladas and chacmas and six were unique to\ngeladas. The derived gelada calls occurred primarily\nin short-range affiliative contexts (table 2). Extant literature and our own observations indicated that most\nof the homologous call types are produced in a similar\nmorphological way—a vocalized exhale—while geladas\nproduce both inhaled and exhaled versions of calls that\nare acoustically distinct (we only separate inhaled and\nexhaled grunts here because they are the most\ncommon, but they also produce inhaled ‘moans’ and\n‘wobbles’, table 2).\nWe performed further analyses on 12 vocalization\ntypes, of which only seven were found in chacmas\n(figures 1 and 2). Other call types were excluded from\nfurther DFA analyses because they were rarely produced without overlapping vocalizations, and hence,\nthere were too few high-quality recordings to analyse\n(gelada female: display calls, moans, inhaled grunts,\nwobbles and yawns; gelada male: how barks, nasal\ninhaled grunts and alarm calls; chacma female: alarm\ncalls; chacma male: fear grunt, alarm calls and copulation calls). We were able to discriminate between\nall call types for each age-sex class, using DFAs;\nbased on eight acoustic parameters, we classified call\ntypes at a higher rate (range: 67.4 –93.8%; leave-oneout classification range: 50 –90.6%) than expected\nby random classification (range: 10 –33.3%). A\nMANOVA test carried out for each of the four\nspecies/sex classes showed that pre-categorized call\ntypes were significantly different from each other\nbased on variation in at least four of the chosen acoustic\nparameters (p , 0.003).\nIn sum, acoustic analysis shows that geladas share\na number of vocalization types with chacma baboons.\nWhile chacma baboons did not appear to have any\nunique calls, the analysis allowed identification of at\nleast five derived vocalization types in geladas:\ninhaled grunts, moans, pre-copulation calls, wobble\ncalls and yawns. We then carried out intraspecific\nanalysis to determine how these calls function in\ngelada society.\n(c) Intraspecific analysis of derived gelada\nvocalizations\n(i) Comparison of derived call use in gelada\nmales and females\nTo determine the function of the derived gelada vocalizations identified above, we first examined potential\nsex differences in the use of these calls. By definition,\npre-copulation calls were produced only by females\nin very straightforward contexts (i.e. produced prior\nto copulation). Thus, we focused here on the use of\ninhaled grunts, moans, wobbles and yawns. Behavioural data on adult male and female geladas were\nobtained between January 2009 and December 2010\nduring repeated 15-min focal follows of 53 females\n(mean + s.d.: 6.55 + 2.59 h per female; 348.50 h in\ntotal) and 13 leader males (6.60 + 1.86 h per male,\n85.75 h in total). During these focal follows we\n1852 M. L. Gustison et al. Primate vocal complexity\nPhil. Trans. R. Soc. B (2012)noted all vocalizations uttered by the focal animal, as\nwell as all social behaviour (e.g. approaches and\ngrooming interactions) involving the focal individual.\nNext, we determined sex differences in the use of\nderived vocalizations by carrying out a general linear\nmodel (GLM) with sex and average reproductive\nunit size (over the entire study period) as fixed factors.\nWe found that gelada males produced four of the\nderived calls (i.e. inhaled grunts, moans, wobbles\nand yawns) at a higher mean rate (14.13 calls per h)\nthan did gelada females (0.39 calls per h) (F1,63 ¼\n708.144, p , 0.001), and reproductive unit size did\nnot come out as a significant covariate (F1,63 ¼\n0.942, p ¼ 0.336). Thus, males appeared to be the\nsex using derived vocalizations. We next explored\nwhether these unique calls were used in contexts that\nare unique to males in gelada society.\n(ii) Functionality of derived gelada male calls\nFirst, we tested the hypothesis that derived social calls\nare used by males to maintain social relationships\nwith the unit females by examining vocal behaviour\nin the context of conflict resolution. Using all adult\nfemale focal data, we identified every fight (both as\nactor and receiver) in which the focal female was\ninvolved. These fights (n ¼ 107 events) were characterized by loud screams from the focal female (n ¼\n48 events), or direct, physical attacks from the focal\nfemale that included biting and slapping (n ¼ 59\nevents). We deliberately excluded any inconspicuous\nagonistic interactions (such as soft threat calls or\nvisual threats) that may have gone unnoticed by\nother group members. For each fight event we counted\nall derived vocalizations directed at the focal animal by\nmales in the 2 min preceding the event and the 2 min\nfollowing the event and compared these values with\nbinomial tests of proportions.\nSecond, we tested the hypothesis that derived social\ncalls were used by males in association with the presence\nof non-unit, ‘bachelor’ males that pose a threat to the\nleader males (all leader males are eventually ousted by\nbachelor males). We used all adult leader male focal\nTable 2. Descriptions of call types used by geladas and chacma baboons in short-range and in long-distance situations,\nincluding the way in which they are physically produced and the contexts in which they occur. CF, chacma female; CM,\nchacma male; GF, gelada female; GM, gelada male. Asterisks denote vocalizations that were not used in discriminant\nfunction analyses due to low sample size.\ncall type mode of production context\nI. Shared vocalizations in chacma baboons and geladas\naffiliative grunt (CF, CM,\nGF, GM)\nexhale a soft tonal contact call used during approaching, grooming,\nand infant-handling, as well as while moving and foraging\n[30,109,132,134,135]\ncopulation call (CF, CM*,\nGF, GM)\nexhale loud grunts given before and during mating [132]\nfear bark (CF, CM*, GF) exhale with retracted lips a ‘cough-like’ vocalization [136] given by subordinate\nindividuals to high-ranking animals [132]\nthreat grunt (CF, CM, GF,\nGM)\nexhale a staccato-like vocalization uttered by the dominant individual\nin an aggressive encounter [132,137,138]\nalarm call (CF*, CM*, GF) exhale noisy, harsh calls used in response to predators and other\nenvironmental threats [109,123]\ndisplay call or ‘wahoo’\n(CM, GF*, GM)\ninhale and exhale loud calls typically uttered during competitive displays\n[132,136]; chacma and gelada males, in particular, make a\n‘roar’ that often comes before these wahoo calls\nlost call\n(CM*,CF*,GF*,GM*)\nlong exhale a noisy vocalization that rises in pitch towards the end of the\ncall and associated with separation from the group or\nparticular individuals\nscream (CF, GF, GM) long exhale with\nretracted lips\na noisy drawn-out defensive call, usually given by subordinates\nwhen attacked by a higher-ranking individual [109,132,134]\nII. Derived gelada vocalizations\ninhaled grunt (GF*, GM) vocal inhale vocalized inhales often part of an affiliative grunt calling bout\n[135]; sometimes, inhaled grunts can have an audibly ‘nasal’\nsound, produced by the withdrawn lip obscuring nasal\npassages [135]\nmoan (GF*, GM) long exhale (sometimes\ninhaled)\nlong drawn-out affiliative grunt, often given by leader males\nto their unit’s females ([109,132,139]; this study)\nwobble (GF*, GM) vocal inhale or exhale with\nlip or tongue-flicking\nsoft, undulating calls usually given by males to their unit\nfemales, often following ‘anxiety-producing’ situations (this\nstudy)\nyawn (GF*, GM) inhale a vocalized yawn given in social contexts, often involving\ngrooming sessions and also after mating or in competitive\nsituations ([140]; this study)\npre-copulation call (GF) short exhale calls given by oestrous females while presenting their\ngenitals to males\nhow barks (GM*) exhale high-pitched barks/whinnies given by non-leader males\ngiving chase to other males in competitive displays\nPrimate vocal complexity M. L. Gustison et al. 1853\nPhil. Trans. R. Soc. B (2012)data for which the location of bachelors was stable\nthroughout the entire 15-min focal sample. In other\nwords, bachelor groups were either close to the focal\nmale (within 20 m: n ¼ 16 focals), far (more than 20 m;\nn ¼ 24 focals), or out of sight (n ¼ 26 focals). We carried\nout two GLMs with male identity as a random factor,\nbachelor distance as a fixed factor, and the rate of derived\ncalls as the dependent variable (first model: close versus\nfar; second model: close versus out of sight).\nWe found evidence that males used non-homologous\nderived calls to maintain cross-sex social relationships\nwith females in his units. Specifically, we found that\nmales directed derived non-homologous calls at\nfemales after fights happened (14 occurrences), and\nthey never used them before a fight (binomial test of\nproportions:\nx2\n1 ¼ 12:916, p , 0.001). On the other\nhand, we did not find any evidence that males used\nthe derived calls in response to the presence of bachelors. Leader males did not produce derived calls at a\nhigh rate when bachelor groups were close (3.23 calls\nper h) compared with when they were far away\n(2.04 calls per h) (F1,8 ¼ 0.394, p ¼ 0.548). Similarly,\nleader males did not produce derived calls at a higher\nrate when bachelor groups were close compared to\nwhen they were out of sight (2.51 calls per h) (F1,11 ¼\n0.078, p ¼ 0.785).\n(d) General discussion\nGeladas have an elaborate, almost ‘choral’ vocal repertoire [109] and live in a complex society with social\ngroups of varying sizes, making geladas an important\nmodel system for addressing hypotheses about\nvocal evolution. Identifying homologous vocalizations\nshared with Papio baboons allowed us to study the function of derived gelada vocalizations. It did not appear\nthat interacting with many individuals [20,141] was\nnecessarily an important factor in the use of derived\ncalls, as their production was not correlated with the\nsize of the reproductive unit. Rather, the need to maintain long-term bonds within the unit seemed most\nimportant; leader males used these derived vocalizations after fights broke out within their units. Thus,\nthe gelada-specific vocalizations may have evolved as\nan adaptation to simultaneously maintaining relationships both with and among multiple females—leader\nmales that are better able to ‘keep the peace’ of\ntheir reproductive units may, in turn, have higher\naffiliative\ngrunt\n100\n1 1 1 1 1 1 1\n1111\n11\n1 1 1 1\n1 111\n11\n100 100 100 100 100 100\n100\n100 100 100 100\n100 100100100100100\n100 100 100 100 100 100\ngelada\nmale\ngelada\nfemale\nchacma\nmale\nchacma\nfemale\nfrequency\n(kHz)\ntime (ms)\ncopulation\ncall\nthreat\ngrunt\nfear\ngrunt\nalarm\ncall scream\ndisplay\ncall\nFigure 1. Spectrograms of homologous calls shared by geladas and chacma baboons. Dashes represent calls that were not\nproduced or produced at a very low rate.\n1854 M. L. Gustison et al. Primate vocal complexity\nPhil. Trans. R. Soc. B (2012)reproductive success [118]. It remains to be determined why the cross-sex bonds seen in geladas seem\nmore tightly linked to vocal complexity than the\nwithin-sex bonds found in both species.\nOur results suggest that future studies should examine whether hamadryas baboons (Papio anubis), a Papio\nspecies that also has a ‘harem-like’ structure [142],\nhave any evidence of greater elaboration of affiliative\ncall use by males. This comparison is particularly\nimportant for uncovering how vocalizations relate to\nspecific aspects of long-term bonds because hamadryas\nmales form long-term bonds with females but the\nrelationship is more coercive than in geladas and\nthere does not appear to be a need to ‘keep the\npeace’. In geladas, investigations of how females\nrespond to derived vocalizations and the subsequent\nbenefits to leader male fitness is an exciting direction\nfor future research. It may be the case, for instance,\nthat these derived vocalizations reduce female anxiety,\nsimilarly to the proposed anxiolytic effects of grooming\n[143 – 145].\nOne puzzling aspect of our findings is that the\nderived calls used by males are all used in similar contexts. Further work is needed to tease apart any\npotential differences between the derived gelada\ncalls but this redundancy suggests an additional\nhypothesis. Perhaps the extremely large groups of\ngeladas (herds can number up to 1000) and high\nrates of vocalizations (mean + s.d.: chacmas: 8.84 +\n4.49 calls per h, geladas: 16.95 + 8.51 calls per h)\ncreate ‘vocal clutter’ that the geladas have overcome\nby diversifying their most common call types—affiliative vocalizations. Thus the need to maintain bonds\nwithin a noisy backdrop of conspecific vocalizations\nmay favour greater vocal complexity, possibly explaining some of the group size effects seen in other\nstudies [20].\n6. CONCLUSIONS\nComparisons of repertoire size and components\nsuggest that primates are broadly similar to other\nmammals, despite primates having greater social complexity. However, our comparison of baboons and\ngeladas highlights the utility of making detailed comparisons among closely related species to understand\nvocal evolution. We were able to examine the function\nof recently evolved calls in detail and examine the\nspecific social implications of increased repertoires\nby focusing on specific call types, addressing sexual\ndifferences, and using behavioural measures to\ndescribe social complexity. We found that the larger\nvocal repertoire of geladas is linked to the maintenance of cross-sex bonds within the reproductive unit.\nBroadly focused theoretical and comparative analyses\n[2,3,5,20] are vital to drive the investigation of\ncommunicative complexity. We argue that there is\nalso a need for more focused analyses among carefully\nchosen taxa using directly comparable measures in the\nstudy of vocal complexity.\nWe are grateful to the Ethiopian Wildlife Conservation\nDepartment, the Amhara National Regional State Parks\nDevelopment and Protection Authority, and the wardens\nand staff of the Simien Mountains National Park for\ngranting us the permission to conduct this research in\nEthiopia. We thank the Office of the President and the\nDepartment of Wildlife and National Parks of the Republic\nof Botswana for permission to conduct research in the\nMoremi Reserve. We would like to thank Esheti Jejaw,\nDavid Pappano, Eila Roberts, Noah Snyder-Mackler and\nVanessa Wilson for their assistance with collecting\nbehavioural data in the field. Tiffany Fritzler and Chelsea\nMiller helped us with the acoustic analysis and Ken Guire\nprovided expert statistical advice. Research in Botswana\nwas supported by NIH grant MH62249, an NRSA\nfellowship, the Leakey Foundation and the University of\nPennsylvania. Research in Ethiopia was supported by the\nWildlife Conservation Society (SSF grant no. 67250, the\nwobble yawn\ninhaled\ngrunt moan\npre-copulation\ncall\ngelada\nmale\ngelada\nfemale\n1\n100\n1 1 1\n1s 100 1s\n1\n100 100\ntime (ms)\nfrequency (kHz)\nFigure 2. Spectrograms of derived call types produced only by geladas. Dashes represent calls that were not produced or\nproduced at a very low rate.\nPrimate vocal complexity M. L. Gustison et al. 1855\nPhil. Trans. R. Soc. B (2012)National Geographic Society (grant no. 8100–06), the\nLeakey Foundation, the National Science Foundation\n(grant no. BCS-0715179), and the University of Michigan.\nWe thank Dr Robin Dunbar, Dr Jacinta Beehner and two\nanonymous reviewers for helpful comments on a previous\nversion of this manuscript.\nREFERENCES\n1 Baker, C. 1988 Vocalization of captive water mongooses, Atilax paludinosus. Z. Saugetierkd 53, 83–91.\n2 Blumstein, D. T. & Armitage, K. B. 1997 Does sociality\ndrive the evolution of communicative complexity?\nA comparative test with ground-dwelling sciurid alarm\ncalls. Am. Nat. 150, 179 –200. (doi:10.1086/286062)\n3 Freeberg, T. M. 2006 Social complexity can drive vocal\ncomplexity: group size influences vocal information\nin Carolina chickadees. Psychol. Sci. 17, 557 –561.\n(doi:10.1111/j.1467-9280.2006.01743.x)\n4 Furrer, R. D. & Manser, M. B. 2009 The evolution of\nurgency-based and functionally referential alarm calls\nin ground-dwelling species. Am. Nat. 173, 400 –410.\n(doi:10.1086/596541)\n5 Wilkinson, G. S. 2003 Social and vocal complexity in\nbats. In Animal social complexity: intelligence, culture,\nand individualized societies (eds F. B. M. de Waal &\nP. L. Tyack), pp. 322– 341. Cambridge, MA: Harvard\nUniversity Press.\n6 Pollard, K. A. & Blumstein, D. T. 2012 Evolving communicative complexity: insights from rodents and\nbeyond. Phil. Trans. R. Soc. B 367, 1869–1878.\n(doi:10.1098/rstb.2011.0221)\n7 Fedurek, P. & Slocombe, K. E. 2011 Primate vocal\ncommunication: a useful tool for understanding\nhuman speech and language evolution? Hum. Biol. 83,\n153– 173. (doi:10.3378/027.083.0202)\n8 Pinker, S. & Bloom, P. 1990 Natural-language and\nnatural selection. Behav. Brain Sci. 13, 707 –726.\n(doi:10.1017/S0140525X00081061)\n9 Byrne, R. W. & Whiten, A. 1988 Machiavellian intelligence. Oxford, UK: Oxford University Press.\n10 Dunbar, R. I. M. 1998 The social brain hypothesis.\nEvol. Anthropol. 6, 178–190. (doi:10.1002/(SICI)15206505(1998)6:5&lt;178::AID-EVAN5.3.0.CO;2-8)\n11 Whiten, A. & Byrne, R. W. 1988 Tactical deception in\nprimates. Behav. Brain Sci. 11, 233–273. (doi:10.\n1017/S0140525X00049682)\n12 Harcourt, A. & Stewart, K. J. 1989 Functions of alliances in contests within wild gorilla groups. Behaviour\n109, 176 –190. (doi:10.1163/156853989X00213)\n13 Dunbar, R. I. M. & Shultz, S. 2010 Bondedness and\nsociality. Behaviour 147, 775 –803. (doi:10.1163/\n000579510X501151)\n14 Silk, J. B. 2002 Using the ‘F’-word in primatology. Behaviour 139, 421–446. (doi:10.1163/156853902760102735)\n15 Smuts, B. B. 1985 Sex and friendship in baboons.\nNew York, NY: Aldine Publishing.\n16 Cheney, D. L. & Seyfarth, R. M. 2007 Baboon metaphysics: the evolution of a social mind. Chicago, IL: University\nof Chicago Press.\n17 Dunbar, R. I. M. 2009 The social brain hypothesis and\nits implications for social evolution. Ann. Hum. Biol. 36,\n562 –572. (doi:10.1080/03014460902960289)\n18 Silk, J. B., Alberts, S. C. & Altmann, J. 2003 Social bonds\nof female baboons enhance infant survival. Science 302,\n1231–1234. (doi:10.1126/science.1088580)\n19 Silk, J. B., Beehner, J. C., Bergman, T. J., Crockford, C.,\nEngh, A. L., Moscovice, L. R., Wittig, R. M., Seyfarth,\nR. M. & Cheney, D. L. 2010 Strong and consistent\nsocial bonds enhance the longevity of female baboons.\nCurr. Biol. 20, 1359–1361. (doi:10.1016/j.cub.2010.\n05.067)\n20 McComb, K. & Semple, S. 2005 Coevolution of vocal\ncommunication and sociality in primates. Biol. Lett. 1,\n381 –385. (doi:10.1098/rsbl.2005.0366)\n21 Hauser, M. D. 1996 The evolution of communication.\nCambridge, MA: MIT Press.\n22 Mulligan, B. E. & Nellis, D. W. 1975 Vocal repertoire of\nthe mongoose Herpestes auropunctatus. Behaviour 55,\n237 –257. (doi:10.1163/156853975X00489)\n23 Watts, C. H. S. 1975 Vocalizations of Australian hopping mice (Rodentia: Nutumys). J. Zool. Lond. 177,\n247 –263. (doi:10.1111/j.1469-7998.1975.tb05982.x)\n24 le Roux, A., Cherry, M. I. & Manser, M. B. 2009 The\nvocal repertoire in a solitary foraging carnivore, Cynictis\npenicillata, may reflect facultative sociality. Naturwissenschaften 96, 575– 584. (doi:10.1007/s00114-0080506-5)\n25 Wong, J., Stewart, P. & Macdonald, D. 1999 Vocal\nrepertoire in the European badger (Meles meles): structure, context, and function. J. Mammal. 80, 570 –588.\n(doi:10.2307/1383302)\n26 Pollard, K. A. & Blumstein, D. T. 2011 Social group\nsize predicts the evolution of individuality. Curr. Biol.\n21, 413–417. (doi:10.1016/j.cub.2011.01.051)\n27 Brady, C. A. 1981 The vocal repertoires of the bush dog\n(Speothos venaticus), crab-eating fox (Cerdocyon thous),\nand maned wolf (Chrysocyon brachyurus). Anim. Behav.\n29, 649 –669. (doi:10.1016/S0003-3472(81)80001-2)\n28 Green, S. 1975 Communication by a graded vocal\nsystem in Japanese monkeys. In Primate behavior, pp.\n1 –102. New York, NY: Academic Press.\n29 Leger, D. W., Owings, D. H. & Gelfand, D. L. 1980\nSingle-note vocalizations of California ground squirrels:\ngraded signals and situation-specificity of predator and\nsocially evoked calls. Z. Tierpsychol. 52, 227 –246.\n(doi:10.1111/j.1439-0310.1980.tb00714.x)\n30 Meise, K., Keller, C., Cowlishaw, G. & Fischer, J. 2011\nSources of acoustic variation: implications for production specificity and call categorization in chacma\nbaboon (Papio ursinus) grunts. J. Acoust. Soc. Am. 129,\n1631– 1641. (doi:10.1121/1.3531944)\n31 Peters, G. & Sliwa, A. 1997 Acoustic communication in\nthe aardwolf, Proteles cristatus (Carnivora: Hyaenidae).\nZ. Sa ̈ugetierkd 62, 219–238.\n32 Fischer, J., Metz, M., Seyfarth, R. M. & Cheney, D. L.\n2001 Baboon responses to graded bark variants. Anim.\nBehav. 61, 925 –931. (doi:10.1006/anbe.2000.1687)\n33 Harris, M. A., Murie, J. O. & Duncan, J. A. 1983\nResponses of Columbian ground squirrels to playback\nof recorded calls. Z. Tierpsychol. 63, 318–330. (doi:10.\n1111/j.1439-0310.1983.tb00747.x)\n34 le Roux, A., Jackson, T. P. & Cherry, M. I. 2001 Does\nBrants’ whistling rat (Parotomys brantsii ) use an\nurgency-based alarm system in reaction to aerial and\nterrestrial predators? Behaviour 138, 757–773. (doi:10.\n1163/156853901752233398)\n35 Baldwin, J. D. & Baldwin, J. I. 1976 Vocalizations of howler\nmonkeys (Alouatta palliata) in southwestern Panama. Folia\nPrimatol. 26, 81–108. (doi:10.1159/000155733)\n36 Charles-Dominique, P. 1977 Ecology and behaviour of\nnocturnal primates: prosimians of equatorial West Africa.\nNew York, NY: Columbia University Press.\n37 Robinson, J. G. 1979 An analysis of the organization of\nvocal communication in the titi monkey Callicebus\nmoloch. Z. Tierpsychol. 49, 381 –405. (doi:10.1111/j.\n1439-0310.1979.tb00300.x)\n38 Masataka, N. 1982 A field study on the vocalizations\nof Goeldi’s monkeys (Callimico goeldii). Primates 23,\n206 –219. (doi:10.1007/BF02381161)\n1856 M. L. Gustison et al. Primate vocal complexity\nPhil. Trans. R. Soc. B (2012)39 Robinson, J. G. 1984 Syntactic structures in the vocalizations of wedge-capped capuchin monkeys, Cebus\nolivaceus. Behaviour 90, 46– 79. (doi:10.1163/\n156853984X00551)\n40 Range, F. & Fischer, J. 2004 Vocal repertoire of sooty\nmangabeys (Cercocebus torquatus atys) in the Taı ̈\nNational Park. Ethology 110, 301 –321. (doi:10.1111/j.\n1439-0310.2004.00973.x)\n41 Strushaker, T. T. 1967 Auditory communication among\nvervet monkeys (Cercopithecus aethiops). In Social communication among primates (ed. S. A. Altmann)\nChicago, IL: University of Chicago Press.\n42 Palombit, R. A. 1992 A preliminary study of vocal communication in wild long-tailed macaques (Macaca\nfascicularis). I. Vocal repertoire and call emission. Int. J.\nPrimatol. 13, 143 –182. (doi:10.1007/BF02547839)\n43 Hohmann, G. 1991 Comparative analyses of agespecific and sex-specific patterns of vocal behavior in\n4 species of Old-World monkeys. Folia Primatol. 56,\n133 –156. (doi:10.1159/000156538)\n44 Kudo, H. 1987 The study of vocal communication of\nwild mandrills in Cameroon in relation to their social\nstructure. Primates 28, 289 –308. (doi:10.1007/\nBF02381013)\n45 Bermejo, M. & Omedes, A. 1999 Preliminary vocal\nrepertoire and vocal communication of wild bonobos\n(Pan paniscus) at Lilungu (Democratic Republic of\nCongo). Folia Primatol. 70, 328–357. (doi:10.1159/\n000021717)\n46 Goodall, J. 1986 The chimpanzees of Gombe. Cambridge,\nMA: Belknapp.\n47 MacKinnon, J. 1974 The behaviour and ecology of wild\norangutans (Pongo pygmaeus). Anim. Behav. 22, 3 –74.\n(doi:10.1016/S0003-3472(74)80054-0)\n48 Strushaker, T. T. 1975 The red colobus monkey. Chicago,\nIL: University of Chicago Press.\n49 Long, C. V. 2007 Vocalisations of the degu Octodon\ndegus, a social caviomorph rodent. Bioacoustics 16,\n223 –244.\n50 Robbins, R. L. 2000 Vocal communication in freeranging African wild dogs (Lycaon pictus). Behaviour\n137, 1271 –1298. (doi:10.1163/156853900501926)\n51 Manser, M. B. 2001 The acoustic structure of suricates’\nalarm calls varies with predator type and the level of\nresponse urgency. Proc. R. Soc. Lond. B 268, 2315–\n2324. (doi:10.1098/rspb.2001.1773)\n52 Manser, M. B. 1998 The evolution of auditory communication in suricates, Suricata suricatta. PhD thesis,\nUniversity of Cambridge, UK.\n53 Ackers, S. H. & Slobodchikoff, C. N. 1999 Communication of stimulus size and shape in alarm calls of\nGunnison’s prairie dogs, Cynomys gunnisoni. Ethology\n105, 149–162. (doi:10.1046/j.1439-0310.1999.00381.x)\n54 Greene, G. & Meagher, T. 1998 Red squirrels, Tamiasciurus hudsonicus, produce predator-class specific alarm\ncalls. Anim. Behav. 55, 511 –518. (doi:10.1006/anbe.\n1997.0620)\n55 Crockford, C. & Boesch, C. 2003 Context-specific calls\nin wild chimpanzees, Pan troglodytes verus: analysis of\nbarks. Anim. Behav. 66, 115 –125. (doi:10.1006/anbe.\n2003.2166)\n56 Seyfarth, R. M., Cheney, D. L. & Marler, P. 1980 Vervet\nmonkey alarm calls: semantic communication in a freeranging primate. Anim. Behav. 28, 1070–1094. (doi:10.\n1016/S0003-3472(80)80097-2)\n57 Stephan, C. & Zuberbu ̈ hler, K. 2008 Predation\nincreases acoustic complexity in primate alarm calls.\nBiol. Lett. 4, 641 –644. (doi:10.1098/rsbl.2008.0488)\n58 Macedonia, J. M. & Evans, C. S. 1993 Variation among\nmammalian alarm call systems and the problem of\nmeaning in animal signals. Ethology 93, 177 –197.\n(doi:10.1111/j.1439-0310.1993.tb00988.x)\n59 Hauser, M. D., Teixidor, P., Field, L. & Flaherty, R. 1993\nFood-elicited calls in chimpanzees –effects of food quantity and divisability. Anim. Behav. 45, 817 –819. (doi:10.\n1006/anbe.1993.1096)\n60 Roush, R. S. & Snowdon, C. T. 2000 Quality, quantity,\ndistribution and audience effects on food calling in\ncotton-top tamarins. Ethology 106, 673 –690. (doi:10.\n1046/j.1439-0310.2000.00581.x)\n61 Ouattara, K., Lemasson, A. & Zuberbu ̈ hler, K. 2009\nCampbell’s monkeys concatenate vocalizations into context-specific call sequences. Proc. Natl Acad. Sci. USA\n106, 22 026 –22 031. (doi:10.1073/pnas.0908118106)\n62 Arnold, K. & Zuberbu ̈ hler, K. 2006 The alarm-calling\nsystem of adult male putty-nosed monkeys, Cercopithecus nictitans martini. Anim. Behav. 72, 643 –653.\n(doi:10.1016/j.anbehav.2005.11.017)\n63 Cap, H., Deleporte, P., Joachim, J. & Reby, D. 2008\nMale vocal behavior and phylogeny in deer. Cladistics\n24, 917 –931. (doi:10.1111/j.1096-0031.2008.00223.x)\n64 Knornschild, M., Glockner, V. & von Helversen, O.\n2010 The vocal repertoire of two sympatric species of\nnectar-feeding bats (Glossophaga soricina and G. commissarisi). Acta Chiropterol. 12, 205 –215. (doi:10.3161/\n150811010X504707)\n65 Sieber, O. J. 1984 Vocal communication in raccoons\n(Procyon lotor). Behaviour 90, 80– 113. (doi:10.1163/\n156853984X00560)\n66 Hamilton III, W. J. & Arrowood, P. C. 1978 Copulatory\nvocalizations of chacma baboons (Papio ursinus), gibbons (Hylobates hoolock), and humans. Science 200,\n1405–1409. (doi:10.1126/science.663622)\n67 Henzi, P. S. 1996 Copulation calls and paternity in\nchacma baboons. Anim. Behav. 51, 233 –234. (doi:10.\n1006/anbe.1996.0021)\n68 Gros-Louis, J. 2002 Contexts and behavioral correlates\nof trill vocalizations in wild white-faced capuchin monkeys (Cebus capucinus). Am. J. Primatol. 57, 189– 202.\n(doi:10.1002/ajp.10042)\n69 Silk, J. B., Cheney, D. L. & Seyfarth, R. M. 1996 The\nform and function of post-conflict interactions between\nfemale baboons. Anim. Behav. 52, 259 –268. (doi:10.\n1006/anbe.1996.0171)\n70 Palombit, R. A., Cheney, D. L. & Seyfarth, R. M. 1999\nMale grunts as mediators of social interaction with\nfemales in wild chacma baboons (Papio cynocephalus\nursinus). Behaviour 136, 221 –242. (doi:10.1163/\n156853999501298)\n71 Peters, G. & Tonkin-Leyhausen, B. A. 1999 Evolution\nof acoustic communication signals of mammals:\nfriendly close-range vocalizations in Felidae (Carnivora). J. Mamm. Evol. 6, 129–159. (doi:10.1023/\nA:1020620121416)\n72 Koda, H., Shimooka, Y. & Sugiura, H. 2008 Effects of\ncaller activity and habitat visibility on contact call rate of\nwild Japanese macaques (Macaca fuscata). Am. J.\nPrimatol. 70, 1055–1063. (doi:10.1002/ajp.20597)\n73 Marler, P. 1977 The evolution of communication. In How\nanimals communicate (ed. T. A. Sebeok), pp. 45–70.\nBloomington, IN: Indiana University Press.\n74 Marler, P. & Mitani, J. 1988 Vocal communication in\nprimates and birds: parallels and contrasts. In Primate\nvocal communication (eds D. Todt, P. Goedeking & D.\nSymmes), pp. 3–14. Berlin, Germany: Springer.\n75 Philips, M. & Austad, S. N. 1990 Animal communication\nand social evolution. In Interpretation and explanation in\nthe study of animal behavior, Vol. 1. Interpretation, intentionality and communication (eds M. Bekoff & D. Jamieson),\npp. 254–268. Boulder, CO: Westview.\nPrimate vocal complexity M. L. Gustison et al. 1857\nPhil. Trans. R. Soc. B (2012)76 Waser, P. M. 1982 The evolution of male loud calls\namong mangabeys and baboons. In Primate communication (eds C. T. Snowdon, C. H. Brown & M. R.\nPetersen), pp. 117 –143. Cambridge, UK: Cambridge\nUniversity Press.\n77 Janik, V. M. & Slater, P. J. B. 1997 Vocal learning in\nmammals. Adv. Stud. Behav. 26, 59–99. (doi:10.1016/\nS0065-3454(08)60377-0)\n78 Zuberbu ̈ hler, K. 2003 Referential signaling in nonhuman primates: cognitive precursors and limitations\nfor the evolution of language. Adv. Stud. Behav. 33,\n265 –307. (doi:10.1016/S0065-3454(03)33006-2)\n79 Seyfarth, R. M. & Cheney, D. L. 2010 Production, usage,\nand comprehension in animal vocalizations. Brain Lang.\n115, 92–100. (doi:10.1016/j.bandl.2009.10.003)\n80 Fant, G. 1960 Acoustic theory of speech production. With\ncalculations based on X-ray studies of Russian articulations.\n’s-Gravenhage, The Netherlands: Mouton.\n81 Fitch, W. T. 2000 The evolution of speech: a comparative review. Trends Cogn. Sci. 4, 258–267. (doi:10.1016/\nS1364-6613(00)01494-7)\n82 Hauser, M. D. 1992 Articulatory and social factors\ninfluence the acoustic structure of rhesus monkey vocalizations: a learned mode of production? J. Acoust. Soc.\nAm. 91, 2175–2179. (doi:10.1121/1.403676)\n83 Rendall, D., Rodman, P. S. & Emond, R. E. 1996 Vocal\nrecognition of individuals and kin in free-ranging rhesus\nmonkeys. Anim. Behav. 51, 1007–1015. (doi:10.1006/\nanbe.1996.0103)\n84 Fitch, W. T. 1997 Vocal tract length and formant frequency dispersion correlate with body size in rhesus\nmacaques. J. Acoust. Soc. Am. 102, 1213–1222.\n(doi:10.1121/1.421048)\n85 Fitch, W. T. & Giedd, J. 1999 Morphology and development of the human vocal tract: a study using magnetic\nresonance imaging. J. Acoust. Soc. Am. 106, 1511–\n1522. (doi:10.1121/1.427148)\n86 Fitch, W. T. & Kelley, J. P. 2000 Perception of vocal\ntract resonances by whooping cranes Grus americana.\nEthology 106, 559–574. (doi:10.1046/j.1439-0310.\n2000.00572.x)\n87 Lieberman, P. H., Klatt, D. H. & Wilson, W. H. 1969\nVocal tract limitations on the vowel repertoires\nof rhesus monkey and other nonhuman primates.\nScience 164, 1185–1187. (doi:10.1126/science.164.\n3884.1185)\n88 Snowdon, C. T. 2009 Plasticity of communication in\nnonhuman primates. Adv. Stud. Behav. 40, 239 –276.\n(doi:10.1016/S0065-3454(09)40007-X)\n89 Pepperberg, I. M. 1981 Functional vocalizations by\nan African grey parrot (Psittacus erithacus).\nZ. Tierpsychol. 55, 139–160. (doi:10.1111/j.14390310.1981.tb01265.x)\n90 Ralls, K., Fiorelli, P. & Gish, S. 1985 Vocalizations and\nvocal mimicry in captive harbor seals, Phoca vitulina.\nCan. J. Zool. 63, 1050 –1056. (doi:10.1139/z85-157)\n91 Konrad, R. & Geissmann, T. 2006 Vocal diversity and\ntaxonomy of Nomascus in Cambodia. Int. J. Primatol.\n27, 713 –745. (doi:10.1007/s10764-006-9042-3)\n92 Me ́ndez-Ca ́rdenas, M., Randrianambinina, B., Rabesandratana, A., Rasoloharijaona, S. & Zimmermann, E.\n2008 Geographic variation in loud calls of sportive\nlemurs (Lepilemur ssp.) and their implications for conservation. Am. J. Primatol. 70, 828 –838. (doi:10.1002/ajp.\n20554)\n93 Mitani, J. C., Hunley, K. L. & Murdoch, M. E. 1999 Geographic variation in the calls of wild chimpanzees: a\nreassessment. Am. J. Primatol. 47, 133–151. (doi:10.\n1002/(SICI)1098-2345(1999)47:2,133::AID-AJP4.3.\n0.CO;2-I)\n94 Moore, J. L., Manne, L., Brooks, T., Burgess, N. D.,\nDavies, R., Rahbek, C., Williams, P. & Balmford, A.\n2002 The distribution of cultural and biological diversity in Africa. Proc. R. Soc. Lond. B 269, 1645– 1653.\n(doi:10.1098/rspb.2002.2075)\n95 Slocombe, K. E. & Zuberbu ̈ hler, K. 2007 Chimpanzees\nmodify recruitment screams as a function of audience\ncomposition. Proc. Natl Acad. Sci. USA 104, 17 228–\n17 233. (doi:10.1073/pnas.0706741104)\n96 Slocombe, K. E., Kaller, T., Turman, L., Townsend, S.\nW., Papworth, S., Squibbs, P. & Zuberbu ̈ hler, K. 2010\nProduction of food-associated calls in wild male chimpanzees is dependent on the composition of the\naudience. Behav. Ecol. Sociobiol. 64, 1959 –1966.\n(doi:10.1007/s00265-010-1006-0)\n97 Papworth, S., Bo ̈ se, A., Barker, J., Schel, A. M. &\nZuberbu ̈ hler, K. 2008 Male blue monkeys alarm call\nin response to danger experienced by others. Biol.\nLett. 4, 472–475. (doi:10.1098/rsbl.2008.0299)\n98 Di Bitetti, M. S. 2005 Food-associated calls and audience effects in tufted capuchin monkeys, Cebus apella\nnigritus. Anim. Behav. 69, 911 –919. (doi:10.1016/j.\nanbehav.2004.05.021)\n99 Pollick, A. S., Gouzoules, H. & de Waal, F. B. M. 2005\nAudience effects on food calls in captive brown\ncapuchin monkeys, Cebus apella. Anim. Behav. 70,\n1273– 1281. (doi:10.1016/j.anbehav.2005.03.007)\n100 Zuberbu ̈ hler, K., Cheney, D. L. & Seyfarth, R. M. 1999\nConceptual semantics in a nonhuman primate. J. Comp.\nPsychol. 113, 33–42. (doi:10.1037/0735-7036.113.1.33)\n101 Doupe, A. J. & Kuhl, P. K. 1999 Birdsong and human\nspeech: common themes and mechanisms. Annu. Rev.\nNeurosci. 22, 567 –631. (doi:10.1146/annurev.neuro.\n22.1.567)\n102 Bouchet, H., Pellier, A., Blois-Heulin, C. & Lemasson,\nA. 2010 Sex differences in the vocal repertoire of adult\nred-capped mangabeys (Cercocebus torquatus): a multilevel acoustic analysis. Am. J. Primatol. 72, 360–375.\n(doi:10.1002/ajp.20791)\n103 Bohn, K. M., Schmidt-French, B., Schwartz, C.,\nSmotherman, M. & Pollak, G. D. 2009 Versatility and\nstereotypy of free-tailed bat songs. PLoS ONE 4,\n6746. (doi:10.1371/journal.pone.0006746)\n104 Hockett, C. F. 1960 The origin of speech. Sci. Am. 203,\n88–96. (doi:10.1038/scientificamerican0960-88)\n105 Cleveland, J. & Snowdon, C. T. 1982 The complex\nvocal repertoire of the adult cotton-top tamarin\n(Saguinus oedipus oedipus). Z. Tierpsychol. 58, 231 –\n270. (doi:10.1111/j.1439-0310.1982.tb00320.x)\n106 MacLanahan, E. B. & Green, K. M. 1977 The vocal\nrepertoire and an analysis of the contexts of vocalizations in Leontopithecus rosalia. In The biology and\nconservation of the Callitrichidae (ed. D. G. Kleiman),\npp. 251–269. Washington, DC: Smithsonian Institute.\n107 Aiello, L. C. & Dunbar, R. I. M. 1993 Neocortex size,\ngroup size, and the evolution of language. Curr. Anthropol. 34, 184 –193. (doi:10.1086/204160)\n108 Dunbar, R. I. M. & Dunbar, P. 1975 Social dynamics of\ngelada baboons. Basel, Switzerland: Karger.\n109 Richman, B. 1987 Rhythm and melody in gelada vocal\nexchanges. Primates 28, 199 –223. (doi:10.1007/\nBF02382570)\n110 Page, S. L., Chiu, C. & Goodman, M. 1999 Molecular\nphylogeny of Old World monkeys (Cercopithecidae) as\ninferred from g-globin DNA sequences. Mol. Phylogenet.\nEvol. 13, 348–359. (doi:10.1006/mpev.1999.0653)\n111 Bergman, T. J. & Kitchen, D. M. 2009 Comparing\nresponses to novel objects in wild baboons (Papio\nursinus) and geladas (Theropithecus gelada). Anim.\nCogn. 12, 63–73. (doi:10.1007/s10071-008-0171-2)\n1858 M. L. Gustison et al. Primate vocal complexity\nPhil. Trans. R. Soc. B (2012)112 Beehner, J. C., Bergman, T. J., Cheney, D. L., Seyfarth,\nR. M. & Whitten, P. L. 2006 Testosterone predicts\nfuture dominance rank and mating activity among\nmale chacma baboons. Behav. Ecol. Sociobiol. 59,\n469–479. (doi:10.1007/s00265-005-0071-2)\n113 le Roux, A., Beehner, J. C. & Bergman, T. J. 2011 Female\nphilopatry and dominance patterns in wild geladas.\nAm. J. Primatol. 73, 422–430. (doi:10.1002/ajp.20916)\n114 Snyder-Mackler, N., Bergman, T. J. & Beehner, J. C. In\npress. Defining higher levels in a gelada multilevel\nsociety. Int. J. Primatol. (doi:10.1007/s10764-0129584-5)\n115 Kawai, M., Ohsawa, H., Mori, U. & Dunbar, R. 1983\nSocial organization of gelada baboons: social units\nand definitions. Primates 24, 13–24. (doi:10.1007/\nBF02381450)\n116 Bergman, T. J. 2010 Experimental evidence for limited\nvocal recognition in a wild primate: implications for the\nsocial complexity hypothesis. Proc. R. Soc. B 277,\n3045–3053. (doi:10.1098/rspb.2010.0580)\n117 le Roux, A. & Bergman, T. J. 2012 Indirect rival assessment in a social primate, Theropithecus gelada. Anim.\nBehav. 83, 249 –255. (doi:10.1016/j.anbehav.2011.10.\n034)\n118 Dunbar, R. I. M. 1984 Reproductive decisions: an economic analysis of gelada baboon social strategies.\nPrinceton, NJ: Princeton University Press.\n119 Mori, U. 1979 Reproductive behaviour. In Ecological\nand sociological studies of gelada baboons (ed. M.\nKawai), pp. 183– 199. Tokyo: Kodansha.\n120 Dunbar, R. I. M. 1977 Feeding ecology of gelada\nbaboons: a preliminary report. In Primate ecology (ed.\nT. H. Clutton-Brock), pp. 250–273. London, UK:\nAcademic Press.\n121 Dunbar, R. I. M. & Bose, U. 1991 Adaptation to grasseating in gelada baboons. Primates 32, 1–7. (doi:10.\n1007/BF02381596)\n122 Iwamoto, T. 1993 The ecology of Theropithecus gelada.\nIn Theropithecus: the rise and fall of a primate genus\n(ed N. G. Jablonski), pp. 441– 452. Cambridge, UK:\nCambridge University Press.\n123 Fischer, J., Hammerschmidt, K., Cheney, D. L. & Seyfarth, R. M. 2001 Acoustic features of female chacma\nbaboon barks. Ethology 107, 33–54.\n124 Owren, M. J., Seyfarth, R. M. & Cheney, D. L. 1997 The\nacoustic features of vowel-like grunt calls in chacma\nbaboons (Papio cyncephalus ursinus): implications for production processes and functions. J. Acoust. Soc. Am. 101,\n2951–2963. (doi:10.1121/1.418523)\n125 Rendall, D., Seyfarth, R. M., Cheney, D. L. & Owren,\nM. J. 1999 The meaning and function of grunt variants\nin baboons. Anim. Behav. 57, 583– 592. (doi:10.1006/\nanbe.1998.1031)\n126 Fischer, J., Hammerschmidt, K., Cheney, D. L. & Seyfarth, R. M. 2002 Acoustic features of male baboon\nloud calls: influences of context, age, and individuality.\nJ. Acoust. Soc. Am. 111, 1465–1974. (doi:10.1121/1.\n1433807)\n127 Altmann, S. A. 1998 Foraging for survival: yearling baboons\nin Africa. Chicago, IL: University of Chicago Press.\n128 Altmann, S. A. & Altmann, J. 1970 Baboon ecology.\nChicago, IL: University of Chicago Press.\n129 Norton, G. W., Rhine, R. J., Wynn, G. W. & Wynn, R. D.\n1987 Baboon diet: a five-year study of stability and variability in the plant feeding and habitat of the yellow\nbaboon (Papio cynocephalus) of Mikumi National Park,\nTanzania. Folia Primatol. 48, 78–120. (doi:10.1159/\n000156287)\n130 Rowell, T. E. 1966 Forest living baboons in Uganda.\nJ. Zool. 149, 344 –364. (doi:10.1111/j.1469-7998.\n1966.tb04054.x)\n131 Hall, K. R. L. & DeVore, I. 1965 Baboon social behavior. In Primate social behavior (eds I. DeVore & K. R. L.\nHall), pp. 53–110. New York, NY: Holt, Rinehart &\nWinston.\n132 Aich, H., Moos-Heilen, R. & Zimmermann, E. 1990\nVocalizations of adult gelada baboons (Theropithecus\ngelada): acoustic structure and behavioural context.\nFolia Primatol. 55, 109– 132. (doi:10.1159/000156508)\n133 Lehner, P. N. 1996 Handbook of ethological methods.\nCambridge, UK: Cambridge University Press.\n134 Cheney, D. L., Seyfarth, R. M. & Silk, J. B. 1995 The\nrole of grunts in reconciling opponents and facilitating\ninteractions among adult female baboons. Anim.\nBehav. 50, 249–257. (doi:10.1006/anbe.1995.0237)\n135 Richman, B. 1976 Some vocal distinctive features used\nby gelada monkeys. J. Acoust. Soc. Am. 60, 718– 776.\n(doi:10.1121/1.381144)\n136 Fischer, J., Kitchen, D. M., Seyfarth, R. M. & Cheney,\nD. L. 2004 Baboon loud calls advertise male quality:\nacoustic features and their relation to rank, age, and\nexhaustion. Behav. Ecol. Sociobiol. 56, 140 –148.\n(doi:10.1007/s00265-003-0739-4)\n137 Kitchen, D. M., Cheney, D. L. & Seyfarth, R. M. 2005\nMale chacma baboons (Papio hamadryas ursinus) discriminate loud call contests between rivals of different\nrelative ranks. Anim. Cogn. 8, 1– 6. (doi:10.1007/\ns10071-004-0222-2)\n138 Wittig, R. M., Crockford, C., Seyfarth, R. M. &\nCheney, D. L. 2007 Vocal alliances in chacma baboons\n(Papio hamadryas ursinus). Behav. Ecol. Sociobiol. 61,\n899 –909. (doi:10.1007/s00265-006-0319-5)\n139 Andrew, R. J. 1976 Use of formants in the grunts of\nbaboons and other nonhuman primates. Ann. NY\nAcad. Sci. 280, 673– 693. (doi:10.1111/j.1749-6632.\n1976.tb25530.x)\n140 Palagi, E., Leone, A., Mancini, G. & Ferrari, P. F. 2009\nContagious yawning in gelada baboons as a possible\nexpression of empathy. Proc. Natl Acad. Sci. USA 106,\n19 262– 19 267. (doi:10.1073/pnas.0910891106)\n141 Dunbar, R. I. M. 1993 Coevolution of neocortical size,\ngroup size and language in humans. Behav. Brain Sci.\n16, 681 –693. (doi:10.1017/S0140525X00032325)\n142 Schreier, A. L. & Swedell, L. 2009 The fourth level of\nsocial structure in a multi-level society: ecological and\nsocial functions of clans in hamadryas baboons.\nAm. J. Primatol. 71, 948 –955. (doi:10.1002/ajp.20736)\n143 Aureli, F., Preston, S. D. & de Waal, F. 1999 Heart rate\nresponses to social interactions in free-moving rhesus\nmacaques (Macaca mulatta): a pilot study. J. Comp. Psychol. 113, 59–65. (doi:10.1037/0735-7036.113.1.59)\n144 Dunbar, R. I. M. 2010 The social role of touch in\nhumans and primates: behavioural function and neurobiological mechanisms. Neurosci. Biobehav. Rev. 34,\n260 –268. (doi:10.1016/j.neubiorev.2008.07.001)\n145 Keverne, E. B., Martensz, N. D. & Tuite, B. 1989 Betaendorphin concentrations in cerebrospinal fluid of\nmonkeys are influenced by grooming relationships.\nPsychoneuroendocrinology 14, 155 –161. (doi:10.1016/\n0306-4530(89)90065-6)\nPrimate vocal complexity M. L. Gustison et al. 1859\nPhil. Trans. R. Soc. B (2012)", "affiliations": [{"university": "University of Michigan", "country": "United States", "discipline": "Biology"}, {"university": "University of Michigan", "country": "United States", "discipline": "Psychology"}, {"university": "University of the Free State", "country": "South Africa", "discipline": "Zoology"}], "species_categories": ["Primate"], "specialized_species": ["geladas", "chacma baboons"], "computational_stages": ["Data Collection", "Pre-processing", "Sequence Representation", "Meaning Identification"], "linguistic_features": ["Vocal Auditory Channel and Turn-taking", "Broadcast and Direct Reception", "Semanticity", "Tradition and Cultural Transmission"], "status": "saved", "created_at": "2026-01-13T12:49:59.882666", "updated_at": "2026-01-13T13:56:02.968763", "committed_at": "2026-01-13T13:56:31.738454"}
{"id": "582bfd51-8d4d-4c45-be29-3104800fc8d8", "title": "Cued and detached representations in animal cognition", "authors": ["G\\\"{a}rdenfors,  Peter"], "year": "1995", "journal": "Behavioural Processes", "abstract": "", "doi": "10.1016/0376-6357(95)00043-7", "analysis_notes": "ELSEVIER Behavioural Processes 35 (1996) 263-273\nBEHAVI~URAL\nPROCESSES\nCued and detached representations in animal cognition\nPeter Giidenfors *\nCognitive Science, Lund University, Kungshuse!, S-222 22 Lund. Sweden\nAccepted 6 March 1995\nAbstract\nThis paper analyzes the function of certain aspects of cognition, like planning, deceiving, self-awareness, and\ncommunication. I distinguish between two kinds of representations of information. A cued representation stands\nfor something that is present in the current situation. Detached representations stand for objects or events that\nare neither present in the situation nor triggered by some recent situation. The inner environment of an animal is\ndefined as the collection of all detached representations. The fundamental difference between signals and\nsymbols is that the reference of a symbol is a detached representation, while a signal refers to a cued\nrepresentation. Detached representations make planning possible. I distinguish between immediate planning,\nwhere plans are made for present needs, and anticipatory planning, where future needs are predicted. The\nevolution of self-consciousness is outlined as a series of steps. The first is when other agents are seen as having\nan inner environment of their own. This is when deception becomes possible. A further step is when the agent\nrealizes that the other agents’ representations of the external world includes a representation of the inner\nenvironment of the agent itself. Then the agent can become self-conscious since it can form representations of\nits own representations.\nKeywords: Mental representation; Animal cognition; Planning; Deception; Symbols; Self-awareness\nL’homo tient debout. Saccouple, en toute saison, face a face. A le pouce opposable. Omnivore.\nCapable d’attention, me^me h des objets absents. Sous le nom de pen&e, reflexion, obsessions, etc., il\npeut rPver durablement pendant la veille, combiner ses r&es h ses perceptions, en tirer des projets\nd’actes, des coordinations de mouvements, we sorte de re’organisation des instincts, des d&sirs, etc. II\nmodifie le milieu. II accumule, conserve, pre’voit, innove; il a des moyens de parvenir.\nPaul ValCry, Mauvaises Pens&es et Autres\n* E-mail: Peter.Gardenfors@fil.lu.se\n0376.6357/96/$ I5.00 0 1996 Elsevier Science B.V. All rights reserved\nSST)10376.6357(95)00043-7264 P. Giirdenjbrs / Behavinural Processes 35 (1996) 263-273\n1. The notion of a representation: Why a snake can’t think of a mouse\nWhen analyzing the cognitive functions of the ‘higher’ animal species, we often ascribe them a\nform of consciousness that includes functions like imagining, planning, deceiving, choosing, being\naware of other minds, and maybe even being self-conscious. Our naive understanding of such\ncognitive functions derives from our understanding of the corresponding human functions.\nIn this paper, I want to examine these features of cognition from an evolutionary perspective.\nRather than directly comparing, e.g., animal planning with human planning, I will ask, firstly, what\ncould be the evolutionary value of having a capability for planning, and, secondly, what other\ncognitive functions are required for such a capacity to evolve. My focus in this paper will be to\nanalyze the evolutionary jbzctions of certain aspects of cognition rather than to study their\nneurophysiological foundations or their behavioral correlates.\nI will argue that in order to understand the functions of most of the higher forms of cognition, one\nmust rely on an analysis of how animals represent various things, in particular the surrounding world\nand its possibilities. However, the very notion of a mental representation is one of the enigmas of\ncontemporary cognitive science.\nRoitblat (1982, p. 354) characterizes representations, at the most general level, as those things\n“that allow past experience to affect later behavior”. This definition seems to be too liberal since it\nincludes, as Roitblat himself recognizes, even the behaviorists’ approach. Whiten and Byrne’s\nproposal is equally liberal: “By ‘representation’ we mean simply a neurally coded counterpart of\nsome aspect of the world.“(Whiten and Byrne, 1988: p. 235).\nYet another definition is proposed by Vauclair (1990, p. 3 12): “Representation is an individual\nphenomenon by which an organism structures its knowledge with regards to its environment. This\nknowledge can take two basic forms: either reference to internal substitutes (e.g., indexes or images)\nor use of external substitutes (e.g., symbols, signals, or words)“. Again, I find the general\ncharacterization too encompassing. However, Vauclair presents Hackett’s notion of ‘displacement’\n(Hackett, 1960) and von Glasersfeld’s criticism of it in his discussion of animal communication (von\nGlasersfeld, 19771, which comes close to the proposal made here (see Section 7.\nI have no elaborated theory of representation to present, but a distinction that seems to capture an\nimportant aspect is that between transduced and inferred information (compare Fodor, 1986). Some\nkinds of animal behavior, like phototaxis, are determined directly by psychophysical mechanisms that\ntransduce information about the environment. In such cases no representations are involved. In other\ncases, the animal uses the incoming information as cues to ‘perceptual inferences’, which add\ninformation to what is obtained by the psychophysical transducers. That which adds information to\nsensory input I propose to call a representation ‘.\nIn order to illustrate the distinction between transduced and inferred information, let me present an\nexample borrowed from Sjijlander (1993, pp. 3-4) of how the different kinds of information will\naffect animal behavior. It seems that a snake does not have a central representation of a mouse but\n’ Fodor (1986, pp. IS- 16) argues that transduced information is equivalent to information about nomic properties. He\nprefers non-nomicness as a characterization of representations since “there is independent reason to suppose that nomicness\nis the more fundamental notion: unlike transduction nomicness is a concept that we need outside the information sciences”.\n(p. 16) I don’t accept this argument, since I, among other things, don’t share the scientific realism that underlies his notion of\nnomicness. Hence I stick to transducibility.P. G~rden~ors/Behavioural Processes 35 (1996) 263-273 265\nrelies solely on transduced information. The snake exploits three different sensory systems in relation\nto prey, such as a mouse. To strike the mouse, the snake uses its visual system (or thermal sensors).\nWhen struck, the mouse normally does not die immediately, but runs away for some distance. To\nlocate the mouse, once the prey has been struck, the snake uses its sense of smell. The search\nbehaviour is exclusively wired to this modality. Even if the mouse happens to die right in front of\nsnake’s eyes, it will still follow the smell trace of the mouse in order to find it. Finally, after the\nmouse has been located, the snake must find its head in order to swallow it. This could obviously be\ndone with the aid of smell or sight, but in snakes this process uses only tactile information. Thus the\nsnake uses three separate modalities to catch and eat a mouse. It has no central representation of a\nmouse since there is no communication between the three systems (except that one takes over when\nthe other finishes).\nCompare this with a cat chasing a mouse! The cat relies on a combination of information from\nseveral sensors: eyes, ears, nose, paws and maybe even whiskers. It can predict that the mouse will\nappear at the other side of a curtain when it disappears on one side. It can ‘infer’ information about\nthe mouse even if there is no immediate sensory information, for example when it is waiting outside a\nmouse-hole. In this sense the cat has a central representation of a mouse that is, at least to some\nextent, independent of the information transduced from any of the sensory modalities. In more\ntechnical terminology borrowed from Piaget, one can say that the cat has achieved object permanence. In contrast, the snake has no unified representation of a mouse (if it is appropriate to say that it\nhas a representation at all).\nOne conclusion to draw from this comparison between the cognitive powers of a snake and a cat is\nthat a central representation is tightly connected with cross-modality, i.e., that information received\nvia one modality is coordinated with information from other modalities. Davenport (1976) presents\nsome major results on cross-modal perception in apes and monkeys (see also Murray, 1990). He has\nthe following remarks on the evolutionary value of cross-modality: “First, it appears that multimodal\ninformation extraction of environmental information is likely to result in more veridical perception,\nand may facilitate cognitive functioning. Second, in my view, cross-modal perception requires the\nderivation of modality-free information, a ‘representation’. That an organism can have the same\nrepresentations, concepts or percepts, regardless of the method of peripheral reception, confers great\nadvantage on that animal in coping with the demands of living (Davenport, 1976, p. 147)“.\nCategorization is, in general, exploiting representations. When, for example, a bird not only sees a\nparticular object, but sees it as food, the bird’s brain is adding information about the perceived object\nthat, e.g., leads to the conclusion that the object can be swallowed. Since information is added,\nmistakes become possible, i.e., the inferences drawn from the representation may turn out to be wrong\n(“Pardon me”, said the the hedgehog and climbed off the scrubbing-brush). When 1 speak of\ninferences, I am in no way implying that they are made in an explicit form, symbolic or otherwise\n(see Gardenfors, 1994). Nor am I assuming that the animal is, in any sense, aware of the\nrepresentation, only that there is some generalizing factor that determines its behavior.\n2. Cued vs. detached representations: Why a chimp can make a tool\nMy primary aim in this article is not to demarcate representations from non-representations.\nHowever, I want to emphasize that there are different kinds of representations. The central thesis of266 P. Giirdenj~rs/Behuuiourcrl Processes 35 (1996) 263-273\nthis paper is that in order to give an accurate analysis of many phenomena in animal and human\ncognition, it is necessary to distinguish between two kinds of representations: cued and detached.\nA cued representation stands for something that is present in the current external situation of the\nrepresenting organism. A cat that hears a crunching sound in the closet and comes to believe that\nthere is a mouse there is using its perceptual stimuli as a cue for its mouse representation. In general,\nthe represented object need not be actually present in the situation, but the representation must have\nbeen triggered by something in a recent situation. Delayed responses, in the behaviorist’s sense, are\nbased on cued representations according to this characterization.\nIn contrast, detached representations may stand for objects or events that are neither present in the\ncurrent situation nor triggered by some recent situation. A detached representation thus requires no\nperceptual cue in order to be activated. A memory of something, that can be evoked independently of\nthe context where the memory was created, would be an example of a detached representation. For\nanother example, consider a chimpanzee, who performs the following sequence of actions: walks\naway from a termite hill, breaks a twig, peels its leaves off to make a stick, returns to the termite hill,\nand uses the stick to ‘fish’ for termites. This behavior seems impossible to explain unless it is\nassumed that the chimp has a detached representation of a stick and its use.\nI am not claiming that it is possible to draw a sharp line between cued and detached representations. There are degrees of detachment, and, as will be seen below, there are different types of\ndetachment. However, I still believe that the rough distinction between the two major kinds of\nrepresentations is instrumental in that it directs our attention to key features of the representational\nforms.\nA closely related distinction is proposed by Gopnik (1982, p. 378) who wants to “distinguish\nrepresentations in which there is some direct causal connection between the states from those in which\nthere is no direct causal connection”. In most cases this will give the same results as distinguishing\nbetween cued and detached representations. However, there are many kinds of causal links which are\nnot separated by Gopnik, for example the distinction between transduced and inferred information.\nFurthermore, I will try to show that it is fruitful to separate different kinds of ‘detachment’. I thus\nbelieve that my analysis is more fine-grained than what would have resulted from applying Gopnik’s\ndistinction.\nA caveat concerning my use of the notion of representation is that I view representations as\ntheoretical terms, in the way conceived of in philosophy of science (e.g., Sneed, 1971). Representations are theoretical idealizations, similar to ‘forces’ in Newtonian mechanics, that are introduced to\npredict and explain empirical generalizations (see Lachman and Lachman, 1982).\n3. The inner environment: Why lizards don’t dream\nWhat is the main advantage of detached representations in comparison to cued ones? In order to\nanswer this question, I will elaborate on an idea introduced by Craik (1943, p. 61): “If the organism\ncarries a ‘small-scale model’ of external reality and of its own possible actions within its head, it is\nable to try out various alternatives, conclude which are the best of them, react to future situations\nbefore they arise, utilize the knowledge of past events in dealing with the present and future, and in\nevery way to react in a much fuller, safer and more competent manner to the emergencies which face\nit”.P. Giirdenfors/ Brhmiourul Processes 35 (1996) 263-273 261\nUnder the heading of the inner environment, this kind of ‘small-scale model’ has been made\npopular by Dennett (1978): “the inner environment is simply any internal region that can affect and\nbe affected by features of potential behavioral control systems” (p. 79). Such an environment is\nnecessary for representing objects (like food and predators), places (where food or shelter can be\nfound), actions (and their consequences), etc., even when these things are not perceptually present.\nThe evolution of this kind of representational power will clearly increase the survival value of the\nanimal.\nAs a tentative definition, the inner environment of an animal will in this paper be identified with\nthe system of all detached representations of the animal and their interrelations. In particular, the\ndynamic features of the objects represented are included in the inner environment. Such features are\nessential for inferring consequences of possible actions. Again, I am not assuming that the animal is\naware of its inner environment, or of the processes utilizing this construct. This would amount to\nself-awareness, as will be discussed in Section 6.\nLike all theories of mind, the inner environment is a metaphor. Metaphors are neither true nor\nfalse, but they can be more or less productive as heuristics for developing more precise theories. In\nwhat follows, I want to show that the metaphor of the inner environment can help us explain the\nevolutionary value of several cognitive functions.\nIt seems that many species of animals have inner environments. For example, the searching\nbehavior of rats is best explained if it is assumed that they have some form of ‘spatial maps’ in their\nheads. Evidence for this, based on their ability to find optimal paths in mazes, was collected by\nTolman as early as the 1930s (see Tolman, 1948). However, his results were swept under the carpet\nfor many years since they were clear anomalies for the behaviorist paradigm. Vauclair (1987)\nprovides a more recent analysis of the notion of a ‘cognitive mapping’.\nIt is difficult to assess when the inner environment first appeared in the animal kingdom, but a wild\nguess is that it is coordinated with the development of the neocortex, i.e., roughly with the appearance\nof mammals. However, it is only with the development of cross-modal representations that we obtain\nadvanced forms of an inner environment (Davenport, 1976, Murray, 1990). Sjijlander (1993) notes\nthat mammals play, but reptiles don’t. There is also evidence of dreaming, which clearly presumes\nan inner environment, only among the mammals (see Fagen, 1981). Thus dogs can dream about\nhunting, but lizards cannot.\nAlso, birds seem to have cognitive capacities that presuppose something like an inner environment\n(interestingly enough, it is only mammals and birds who have a constant body temperature). For\nexample, their advanced spatial representations are well documented. It should be noted though, that\neven if several other taxa have spatial abilities, by being able to utilize landmarks etc., this does not\nentail that they have an inner environment. One operational test for the existence of a spatial inner\nenvironment is the ability to take shortcuts when previous hinders are removed.\nMy aim in the remainder of the paper is to establish that existence of an inner environment is a\nprerequisite for the evolution of many higher cognitive functions. The functions I will consider are\nplanning, deception, self-awareness, and linguistic communication.\n4. Planning: Why the squirrel does not make any provisions for the winter\nOne of the main evolutionary advantages of an inner environment is that it frees an animal who is\nseeking a solution to a problem from dangerous trial-and-error behavior. Jeannerod ( 1994, p. 2) says268 P. Giirdenfors/ Behavioural Processes 35 (1996) 263-273\nthat “actions are driven by an internally represented goal rather than directly by the external world”.\nBy exploiting its inner environment, the animal can simulate a number of different actions in order to\n‘see’ their consequences and evaluate them. After these simulations, it can choose the most\nappropriate action to perform in theeouterenviromuent. Of course, the success of the simulations\ndepends on how well the mner envtronment is matched to the perceptions of the outer. A monkey\nwho imagines a branch where there is none is soon a dead monkey - evolutionary selection pressures\nwill, in the long run, result in a sufficient correspondence between the two environments.\nThe ability to envision various actions and their consequences is a necessary requirement for an\nanimal to be capable of planning. Following Gulz (1991, p. 46), I will use the following criterion: An\nanimal is planning its actions if it has a representation of a goal and a start situation and it is capable\nof generating a representation of partially ordered set of actions for itself for getting from start to goal.\nThe representations of the goal and the actions must be detached, otherwise the animal has no choice.\nIn brief, planning presupposes an inner environment.\nThere are several clear cases of planning among primates and less clear cases in other species (see,\ne.g. Chapters 5, 7, 8 and 9 in Ellen and Thinus-Blanc, 1987, and pp. 58-61 in Gulz, 1991). The\ntermite-fishing chimpanzee mentioned earlier is one such example. By the way, this is an example of\nplanned tool-making.\nHowever, all evidence for planning in non-human animals concerns planning for present needs.\nApes and other animals plan because they are hungry or thirsty, tired or frightened. Oakley (1961 p.\n187) notes: “Sultan, the chimpanzee observed by Kohler, was capable of improvising tools in certain\nsituations. Tool-making occurred only in the presence of a visible reward, and never without it. In the\nchimpanzee the mental range seems to be limited to present situations, with little conception of past or\nfuture”.\nHumans seem to be the only animal that can plan for future needs. Gulz (1991, p. 55) calls\nplanning for present needs immediate planning while planning for the future is called anticipatory\nplanning. Humans can predict that they will be hungry tomorrow and save some food, and we realize\nthat the winter will be cold, so we start building a shelter already in the summer. The crucial\ndistinction is that for an animal to be capable of anticipatory planning it must have a detached\nrepresentation of its fiture needs. In contrast, immediate planning only requires a cued representation\nof the current need. There is nothing in the available evidence concerning animal planning,\nnotwithstanding all its methodological problems, that suggests that any species other than Homo\nsapiens has detached representations of their desires.\nBut, isn’t the squirrel who is gathering and storing food for the winter engaged in anticipatory\nplanning? No, it is not planning at all. It has no detached representation of the winter, let alone its\nneeds during the winter. The gathering behavior is routine behavior of an instinctual nature that\nappears stereotypically without sensitivity to varying circumstances. For example, if one fills the\nsquirrel’s stores, it still continues gathering until the ‘urge’ is gone.\n5. Deception: Why the partridge feigning a broken wing isn’t fooling the fox\nI want to analyze the evolution of self-consciousness as a series of comparatively small steps. A\ngood planner must consider the actions of other individuals (in particular if the planner belongs to a\nsocial species). A special case of representations in the inner environment concerns the minds of otherP. Girdenfors/ Behuvioural Procrsses 35 (I 996) 263-273 269\nindividuals. In my opinion, the first step in the evolution of self-consciousness is when other agents\nare not only seen as acting things, but as having an inner environment of their own, with beliefs,\ndesires, etc.\nIt is only when this representational capacity is accomplished that deception becomes possible.\nDeception, in the intentional sense, presumes a representation of other minds. To see this, let us turn\nto the worthwhile survey of tactical deception in primates by Whiten and Byrne (1988). After their\ninitial attempt to define ‘tactical deception’ was criticized in the commentary, they ended up with the\nfollowing definition (1988, p. 271): “Acts from the normal repertoire of the AGENT, deployed such\nthat another individual is likely to misinterpret what the acts signify, to the advantage of the\nAGENT’ ’ .\nThe key word in this definition is ‘deployed’. When this word refers to human behavior, it refers to\nan intentional act. I submit that the ordinary use of deception presupposes that the deceiver has some\nrepresentation of how the individual to be deceived will interpret the deceiving act. In other words,\ndeception presupposes that the inner environment of the deceiver contains some form of representution of the inner environment of the target individual. Note that deception presumes all the cognitive\nfunctions of (immediate) planning, and some more, i.e., an inner environment containing a model of\nthe inner environment of other individuals. Thus, this analysis predicts that deception will occur later\nthan planning in the evolution of cognitive functions. This thesis is most naturally interpreted as a\nstatement about phylogeny, but can also be given an ontogenetical meaning.\nWhiten and Byrne (1988) present a series of examples of potential cases of deception among\nprimates. Most examples come from field observations of chimpanzees and baboons *. However,\nalmost all evidence is based on more or less anecdotal material. Lacking controlled experiments, it is\ntherefore strongly debatable whether the evidence can establish that deception in the intentional sense\noccurs among animals other than humans (see Bennett, 1988).\nHowever, there are cases when it is clear that deception is not taking place: The partridge feigning\na broken wing to lure away the fox from her chicks is not fooling the fox. ‘Fooling’ presumes an\nintention to make somebody else misinterpret the fooling act. There is no evidence that the partridge\nhas any representation of what the fox thinks. She merely acts instinctively when the fox approaches\nand can hence not have any intention to fool.\n6. Self-awareness: Why baboons don’t wear lipstick\nDeception, in the full intentional sense, presupposes that the deceiver has a representation of the\ndeceived one’s inner environment. On this level, an animal can have goals concerning the intentions\nof other individuals, e.g., want somebody to believe that an attack would fail. This is an example of a\nsecond-order intention.\n2 Gallup (1988, p. 255) notes that “the absence of any evidence of deception in orangutans, who, like chimpanzees, can\nalso correctly decipher mirrored information about themselves, is not surprising in so much as they lead a rather solitary\nexistence in the first place. In fact, I have even conjectured [...I that the reason orangutans are so reclusive may be because\nthey have learned that other orangutans cannot be trusted!“.270 P. Giirdenfors/Behuuioural Processes 35 (1996) 263-273\nBut a smart agent will not be duped: he will realize that somebody is trying to deceive him and\ncounteract. Hence, the really smart deceiver will foresee the reasoning of such a smart agent (see\nDennett, 1988). The important aspect of this escalation in smartness is that it can only work if the\npotential deceiver realizes that the agent he wants to deceive not only has her own representations of\nthe external world, but that her inner world contains a representation of the deceiver himself.\nDo animals other than humans have self-awareness? Gallup’s experiments (Gallup, 1977) show\nthat chimpanzees and orangutans, but no other primates, can recognize themselves in mirrors ‘. And\nwhen it comes to recognizing oneself in a photograph, only chimpanzees seem to be successful.\nBodily decorations only make sense when you have some form of awareness of your own body.\nSuch decorations occur in all human cultures, but in no other species in an intentional way. Thus,\nbaboons could never come up with the idea of using lipstick.\nBut recognizing oneself in a mirror or in a photograph only requires awareness of one’s own body,\nnot of one’s own mind. The final step in the evolution of higher-level inner representation is small but\ncrucial for self-awareness in its proper sense: I must realize that the inner environment of my\nopponent does not only contain a representation of myself as a bodily agent, but as an agent with\ninner representations as well. I propose that it is only after this insight that the agent can become\nself-conscious in the sense that it can form representations of its own representations. Some support\nfor this evolutionary point can also be obtained from recent results in developmental psychology (see\ne.g. Wimmer and Hard, 1991 and Gopnik, 1993).\nAs a final step, self-awareness can then develop as a shortcut in the representations involved in the\ndeception game: I can, in my inner environment, have a representation of my own inner environment.\nHowever, I submit that this kind of self-awareness could never develop without the previous\nestablishment of a representation of the inner environment of other individuals. In other words, I\nclaim that an ‘I’-experience must be preceded by a ‘you’-experience (see also Mead, 1934,\nGardenfors, In Press, b, and Gomez, 1994). This position contradicts the traditional Cartesian view on\nmind where humans are supposed to have direct access to their thoughts 4.\n7. Language: Why bees don’t tell stories to one another\nThinking does not presume a language. Humans, as well as animals, can simulate sequences of\nactions in their inner environments. Such simulations are, among other things, necessary for planning.\nI shall argue that language is a very late phenomenon on the evolutionary scene. As I have tried to\n3 Epstein, Lanza and Skinner (Epstein et al., 1980) performed a similar experiment intending to show that also pigeons\ncan learn the same kind of behavior. Davis (1989) argues, in my opinion convincingly, that their experiment does not show\nthat pigeons have any form of self-awareness.\n4 Gopnik (1993) calls this “the illusion of expertise.” She writes: “The commonsense picture proposes that we have\nintentional psychological states, then we have psychological experiences of the intentionality of those states, then we observe\nour own behavior that follows those states, and finally, we attribute the states to others with similar behavior. I suggest a\ndifferent sequence: First we have psychological states, observe the behaviors and experiences they lead to in ourselves and\nothers, construct a theory about the causes of those behaviors and experiences that postulates intentionality, and then, in\nconsequence, we have experiences of the intentionality of those states” (1993, p.12).P. Giirdenfors / Behuuiourul Procrsses 35 (1996) 263-273 271\nshow in the previous sections, an individual can have a great deal of cognitive functions, including\nself-awareness, without having a symbolic language 5.\nIn contrast, I submit that language presumes the existence of an intricate inner environment. In\norder to make this clear, I will propose a definition of the distinction between signals and symbols.\nBoth signals and symbols are tools of communication. The fundamental difference between them is\nthat the reference of a symbol is a detached representation, while a signal refers to a cued\nrepresentation. In other words, a signal refers to something in the outer environment, while a symbol\nrefers to the inner environment. Language consists of symbols - it can be used to talk about things\nnot present in the current situation. This idea can be traced back to Hackett’s (Hackett, 1960) notion\nof ‘displacement’. Sjalander (1993, pp. 5-6) expresses the point as follows: “The predominant\nfunction of language is to communicate about that which is not here and not now. A dog can ‘say’: I\nam angry, I want water, I want to go out, I like you, etc. But it has no communicative means enabling\nit to ‘say’: I was angry yesterday, nor can it ‘say’: I will be angry if you lock me up tonight again,\nand I will chew up the carpet. Likewise, the dog can ‘say’: There is a rat here! but it cannot ‘say’:\nThere is a rat in the next room.\n[ . . . 1 Clearly, if you live in the present, communicating mainly about how you feel and what you\nwant to do in the moment, the biological signals inherent in each species are sufficient”.\nSymbols refering to something in one person’s inner environment can be used to communicate as\nsoon as the listeners have, or are prepared to add, the corresponding references in their inner\nenvironments. For a mode1 theoretic account of how such communication can be established, see\nGardenfors ( 1993).\nMany animals have intricate systems of signals, for example, the dances of bees. However, even if\ntheir dances seem to have a kind of grammar, it still consists only of signals. The bees categorize, in\na sophisticated way, places where nectar can be found. The crucial point is that they only use their\ndances in a situated manner, and thus the dances are not symbols according to my criterion. The same\npoint is made by von Glasersfeld (1976, p. 222): “In my terms, the bees do not qualify for\nsymbolicity, because they have never been observed to communicate about distances, directions, food\nsources, etc., without actually coming from, or going to, a specific location”.\nIn spite of all attempts to teach apes various forms of symbolic codes (see e.g. Savage-Rumbaugh\nand Rumbaugh, 19931, humans seem to be the only animals that use language in a fully detached way.\nEven though the pygmy chimpanzee Kanzi’s performance is quite impressive, his use of symbols is\ndependent on the context: they mainly express requests to “direct teacher’s attention to places, things\nand activities” (Savage-Rumbaugh et al., 1985, p. 658). Human children, in contrast, very early use\nlanguage outside the context of request. Vauclair (1990, p. 319) notes that “the use of symbols by\napes is closely tied to the achievement of immediate goals, because the referents occur in the context\nof behavior on their objects”. This is congenial with Gulz’ ( 1991) conclusion that only humans are\nanticipatory planners. My conjecture is that this capability is required for the complete detachment of\nlanguage. We are still waiting for Kanzi to tell us a story by the camp fire.\n’ A similar point is made by Donald (1991). In Gardenfors (In Press, a), I write: “We all have the experience of\nsomething like an omnipresent inner monologue (or dialogue) while we are engaged in thinking. I believe that this\nexperience is deceptive. Firstly, we can ‘think’ without language. Consider, for example, the previously mentioned mental\nsimulation of a high jumper. Secondly, and more importantly, the inner speech is best interpreted as just parts of the\nsimulafions in the inner environment. The inner soliloquy is part of what we perceive in the inner environment”.272 P. Giirdenfors / Behoviourul Processes 35 (1996) 263-273\n8. Conclusion: The detachment of mind\nMy main point in this paper has been to introduce the distinction between cued and detached\nrepresentations. Using this distinction as an analytic tool, I have tried to provide an outline of the\n‘later’ parts of the evolution of cognition. I have tried to show that the notion of an inner environment\ncan serve as a basis for all higher cognitive functions like planning, deception, self-awareness, and\nlanguage.\nEach of these functions is based on different kinds of assumptions concerning the detached\nrepresentations that are involved. Anticipatory planning, in contrast to immediate planning, presumes\na detachment of the representation of the needs of the individual. For deception one must postulate an\ninner environment that contains representations of other individuals’ inner environments. Self-awareness assumes detached representations of one’s own inner environment. Finally, the referents of\nlinguistic symbols are to be found in the inner environment, in contrast to signals which refer to\nthings in the actual outer environment.\nIf the behaviorists were right, it would be questionable whether we would need the notion of\nrepresentation at all (see Epstein, 1982). In my opinion, however, there is convincing evidence that\nthe behaviorists are wrong and that animals have not only cued representations but also detached\nones. I have defined the inner environment of an animal as the collection of all its detached\nrepresentations. As I have tried to show, the general trend in the evolution of cognition is that more\nand more representations become detached. This will, by large, lead to increasing cognitive flexibility.\nIn other words, the evolution of cognition is the story of the detachment of mind.\nAcknowledgements\nResearch for this article has been supported by the Swedish Council for Research in the Humanities\nand Social Sciences. I wish to thank Christian Balkenius, Patrick Bateson, Lukas Biiiik, Henrik\nGedenryd, Paul Hemeren, Robert Pallbo, Joelle Proust, John Stewart, Michel Vancassel, Jacques\nVauclair, and in particular Jean-Marie Vidal for helpful comments and criticism.\nReferences\nBennett, J., 1988. Thoughts about thoughts. Behavioral and Brain Sciences 11, pp. 246-247.\nCraik, K., 1943. The Nature of Explanation. Cambridge University Press, Cambridge.\nDavenport, R.K., 1976. Cross-modal perception in apes. In: S.R. Hamad, H.D. Steklis, and J. Lancaster (Editors), Origins\nand Evolution of Language and Speech, Annals of the New York Academy of Science 280, pp. 143- 149.\nDavis, L.H., 1989. Selfconsciousness in chimps and pigeons. Philosophical Psychology 2, pp. 249-259.\nDennett, D., 1978. Brainstorms: Philosophical Essays on Mind and Psychology. MIT Press, Cambridge, MA.\nDennett, D., 1988. Why creative intelligence is hard to find. Behavioral and Brain Sciences 11, p. 253.\nDonald, M., 1991. Origins of the Modem Mind. Harvard University Press, Cambridge, MA.\nEllen, P. and Thinus-Blanc, C., eds., 1987. Cognitive Processes and Spatial Orientation in Animal and Man: Volume I\nExperimental Animal Psychology and Ethology, Martinus Nijhoff Publishers, Dordrecht.\nEpstein, R., 1982. Representation: A concept that fills no gaps. Behavioral and Brain Sciences 5, pp. 377-378.\nEpstein, R., Lanza, R.P. and Skinner, B.F., 1980. Self-awareness in the pigeon. Science 212, pp. 695-696.\nFagen, R., 198 1. Animal. Play Behavior. Oxford University Press, Oxford.P. Giirdenf)rs/Beh~uiourctl Processes 35 (1996) 263-273 273\nFodor, J.A., 1986. Why paramecia don’t have mental representations. Midwest Studies in Philosophy IO, pp. 3-23.\nGallup, G.G., 1977. Self-recognition in primates. American Psychologist 32, pp. 329-338.\nGallup, G.G., 1988. Toward a taxonomy of mind in primates. Behav. Brain Sci. 11, 255-256.\nGardenfors, P., 1993. The emergence of meaning. Ling. Phil. 16, pp. 285-309.\nGardenfors, P., 1994. How logic emerges from the dynamics of information. In: J. van Eijck and A. Visser (Editors), Logic\nand Information Flow, MIT Press, Cambridge, MA, pp. 49-77.\nGardenfors, P., In Press, a. Speaking about the inner environment. To appear in Of Thoughts and Words, Sture Allen\n(Editor), Imperial College Press, London.\nGardenfors, P., In Press, b. Language and the evolution of cognition. Paper presented at the colloquium Les Sciences de la\nCognition: Des modeles computationels a la philosophie de l’esprit, Lyon, December 1994.\nGlasersfeld, E. von, 1976. The development of language as purposive behavior. In: S.R. Hamad, H.D. Steklis, and J.\nLancaster (Editors), Origins and Evolution of Language and Speech, Annals of the New York Academy of Science 280,\npp. 212-226.\nGlasersfeld, E. von, 1977. Linguistic communication: theory and definition. In: D.M. Rumbaugh (Editor), Language\nLearning by a Chimpanzee: The LANA Project, Academic Press, New York, pp. 55-71.\nGomez, J.C., 1994. Mutual awareness in primate communication: A Gricean approach. In: Self-awareness in Animals and\nHumans, S.T. Parker, R.W. Mitchell, and M.L. Boccia (Editors), Cambridge University Press, Cambridge, pp. 61-80.\nGopnik, M., 1982. Some distinctions among representations. Behavioral and Brain Sciences 5, pp. 378-379.\nGopnik, A., 1993. How we know our minds: The illusion of first-person knowledge of intentionality. Behavioral and Brain\nSciences 16, pp. 1- 14.\nGulz, A., 1991. The Planning of Action as a Cognitive and Biological Phenomenon, Lund University Cognitive Studies 2,\nLund.\nHackett, C.F., 1960. The origin of speech. Scientific American 203(3), pp. 88-96.\nJeanne&, M., 1994. The representing brain, neural correlates of motor intention and imagery. Behavioral and Brain\nSciences 17, pp. 187-202.\nLachman, R. and Lachman, J.L., 1982. Memory representations in animals: Some metatheoretical issues. Behavioral and\nBrain Sciences 5, pp. 380-38 I.\nMead, G.H., 1934. Mind, Self, and Society. University of Chicago Press, Chicago.\nMurray, E.A., 1990. Representational memory in nonhuman primates. In: Neurobiology of Comparative Cognition, R.P.\nKesner and D.S. Olton (Editors), Lawrence Erlbaum Associates, Hillsdale, NJ, pp. 127- 155.\nOakley, K.P., 1961. On man’s use of fire, with comments on tool-making and hunting. In: S.L. Washburn (Editor), Social\nLife of Early Man, Aldine Publishing Co., Chicago, pp. 176-193.\nRoitblat, H.L., 1982. The meaning of representation in animal memory. Behavioral and Brain Sciences 5, pp. 353-372.\nSavage-Rumbaugh, E.S., Rumbaugh, D.M., and McDonald, K., 1985. Language learning in two species of apes.\nNeuroscience and Biobehavioral Review 9, pp. 653-665.\nSavage-Rumbaugh, E.S., and Rumbaugh, D.M., 1993. The emergence of language. In: K.R. Gibson and T. Ingold (Editors),\nTools, Language and Cognition in Human Evolution, Cambridge University Press, Cambridge, pp. 86- 108.\nSjolander, S., 1993. Some cognitive breakthroughs in the evolution of cognition and consciousness, and their impact on the\nbiology of language. Evolution and Cognition 3, pp. l-10.\nSneed, J., 197 1. The Logical Structure of Mathematical Physics, Reidel, Dordrecht.\nTolman, E.C., 1948. Cognitive maps in rats and men. Psychological Review 55, pp. 189-208.\nVauclair, J., 1987. A comparative approach to cognitive mapping. In: P. Ellen and C. Thinus-Blanc (Editors), Cognitive\nProcesses and Spatial Orientation in Animal and Man: Volume I Experimental Animal Psychology and Ethology,\nMartinus Nijhoff Publishers, Dordrecht, pp. 89-96.\nVauclair, J., 1990. Primate cognition: From representation to language. In: S.T. Parker and K.R. Gibson (Editors),\n‘Language’ and intelligence in monkeys and apes, Cambridge University Press, Cambridge, pp. 312-329.\nWhiten, A. and Byrne, R.W., 1988. Tactical deception in primates. Behavioral and Brain Sciences I 1, pp. 233-73.\nWimmer, H. and Hartl, M., 1991. Against the Cartesian view on mind: Young children’s difficulty with own false beliefs.\nBritish Journal of Developmental Psychology 9, pp. 125- 138.", "affiliations": [{"university": "Lund University", "country": "Sweden", "discipline": "Cognitive Science"}], "species_categories": ["Primate", "Terrestrial Mammal", "Bird", "Reptile"], "specialized_species": ["chimpanzee", "cat", "dog", "rat", "squirrel", "lizard", "bird", "ape", "fox", "partridge"], "computational_stages": ["Meaning Identification"], "linguistic_features": ["Semanticity", "Arbitrariness and Duality of Patterns", "Specialization", "Discreteness and Syntax"], "status": "saved", "created_at": "2026-01-13T12:49:59.882671", "updated_at": "2026-01-13T13:58:02.535518", "committed_at": "2026-01-13T13:58:09.672071"}
{"id": "20f930b5-ba0d-4167-9354-41445187a10b", "title": "Why Are No Animal Communication Systems Simple Languages?", "authors": ["Beecher,  Michael D."], "year": "2021", "journal": "Frontiers in Psychology", "abstract": "", "doi": "10.3389/fpsyg.2021.602635", "analysis_notes": "REVIEW\npublished: 19 March 2021\ndoi: 10.3389/fpsyg.2021.602635\nEdited by:\nIrene M. Pepperberg,\nHarvard University, United States\nReviewed by:\nCarel ten Cate,\nLeiden University, Netherlands\nSlawomir Wacewicz,\nNicolaus Copernicus University\nin Torun, Poland ´\nErich David Jarvis,\nDuke University, United States\n*Correspondence:\nMichael D. Beecher\nbeecher@uw.edu\nSpecialty section:\nThis article was submitted to\nEvolutionary Psychology,\na section of the journal\nFrontiers in Psychology\nReceived: 03 September 2020\nAccepted: 18 February 2021\nPublished: 19 March 2021\nCitation:\nBeecher MD (2021) Why Are No\nAnimal Communication Systems\nSimple Languages?\nFront. Psychol. 12:602635.\ndoi: 10.3389/fpsyg.2021.602635\nWhy Are No Animal Communication\nSystems Simple Languages?\nMichael D. Beecher1,2\n*\n1 Department of Psychology, University of Washington, Seattle, WA, United States, 2 Department of Biology, University\nof Washington, Seattle, WA, United States\nIndividuals of some animal species have been taught simple versions of human language\ndespite their natural communication systems failing to rise to the level of a simple\nlanguage. How is it, then, that some animals can master a version of language, yet\nnone of them deploy this capacity in their own communication system? I first examine\nthe key design features that are often used to evaluate language-like properties of natural\nanimal communication systems. I then consider one candidate animal system, bird\nsong, because it has several of the key design features or their precursors, including\nsocial learning and cultural transmission of their vocal signals. I conclude that although\nbird song communication is nuanced and complex, and has the acoustic potential\nfor productivity, it is not productive – it cannot be used to say many different things.\nFinally, I discuss the debate over whether animal communication should be viewed as a\ncooperative information transmission process, as we typically view human language, or\nas a competitive process where signaler and receiver vie for control. The debate points\nto a necessary condition for the evolution of a simple language that has generally been\noverlooked: the degree of to which the interests of the signaler and receiver align. While\nstrong cognitive and signal production mechanisms are necessary pre-adaptations for\na simple language, they are not sufficient. Also necessary is the existence of identical or\nnear-identical interests of signaler and receiver and a socio-ecology that requires highlevel cooperation across a range of contexts. In the case of our hominid ancestors, these\ncontexts included hunting, gathering, child care and, perhaps, warfare. I argue that the\nkey condition for the evolution of human language was the extreme interdependency\nthat existed among unrelated individuals in the hunter-gatherer societies of our hominid\nancestors. This extreme interdependency produced multiple prosocial adaptations for\neffective intragroup cooperation, which in partnership with advanced cognitive abilities,\nset the stage for the evolution of language.\nKeywords: animal communication, language evolution, animal cognition, animal language studies, information\nINTRODUCTION\nResearch programs on animal communication systems in nature have proceeded essentially\nindependently of research programs endeavoring to teach language to animals. This is surprising\nin light of the early, well-known efforts to relate these two research streams, especially by Hockett\n(1960) and Marler (1961). These efforts spurred two questions. First, can animals be taught human\nFrontiers in Psychology | www.frontiersin.org 1 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nlanguage, even a simplified version? Second, do the natural\ncommunication systems of any animals rise to the level of\nsimple language? Research since then has indicated that these\ntwo questions may have different answers: I would suggest a\nprovisional yes to the first, and a provisional no to the second.\nIf this view is correct, it raises a further question: why, then,\nif some animals can master a version of language, don’t they\nuse this capacity in their natural communication system? In\nthis paper I address this paradox, and make some suggestions\ntoward its resolution.\nMy paper is divided into four parts. First I consider the main\n“design features” of language proposed by Hockett as a basis\nfor evaluating language-like properties of animal communication\nsystems. Hockett concluded that some animal communication\nsystems have some of these design features, but none of them\nhave all the design features, especially the key ones. I will\ndesignate an animal communication system as a ‘simple language’\nsystem using a variation on the definition of Hewes (1973):\n“language [is] any system of animal communication which\nexhibits most of the design features set forth by Hockett” (Hewes,\n1973, p. 5). I narrow this definition by identifying four design\nfeatures – semanticity, arbitrariness, learnability and cultural\ntransmission, and productivity – as necessary for the system to be\nclassified as a simple language. Second, I discuss bird song, a case\nwhere several but not all of the key design features are present. I\nwill focus on one specific case of a song-based communication\nsystem that is clearly complex and nuanced, but nevertheless\nlacks three key design features, semanticity, arbitrariness and\nproductivity. Third, I consider the debate, not yet fully concluded,\nover whether animal communication should be conceived of as a\nprocess of information transfer or as manipulation of receiver by\nthe signaler. The debate is germane to our more specific question\nbecause it provides a clue as to why we find no simple languages\namong animals despite the apparent capacity for it in at least\nsome of them. Finally, I suggest that although there appear to be\nat least some animals with the cognitive capacity for a languagelike communication system, none of them have a social system\nwith extreme interdependency among individuals on the scale of\nthat which existed in the hominid hunter-gatherer system. I argue\nthat this extreme interdependency was a necessary condition for\nthe evolution of human language.\nDESIGN FEATURES OF LANGUAGE\nIn this section I consider the extent to which the most\nimportant design features of human language are found in animal\ncommunication systems. I use Hockett’s (1960) design features\nas a basis for comparison of natural animal communication\nsystems with human language. Although Hockett’s design\nfeatures may have limited use as a theoretical framework for\nmodern evolutionary linguistics (Wacewicz and Zywiczy ˙ nski ´ ,\n2015), it is a useful starting point for the comparative analysis\nof this paper. I have winnowed Hockett’s original design features\ndown to the few I consider the most fundamental ones that\ncan be used to directly compare human language with animal\ncommunication systems.\nSpecialization: The Purpose of Linguistic\nSignals Is Communication and Not Some\nOther Biological Function\nSpecialization, in Hockett’s sense, is the first defining feature of\na communication system, no matter how simple or complex it\nmight be. Otte (1974) defines communication signals as traits\n“fashioned or maintained by natural selection because they\nconvey information to other organisms”(Otte, 1974, p. 385).\nI discuss the vigorous debate over the ‘information’ aspect\nof this definition in Section “Communication: Information or\nInfluence? Mutual Benefit or Manipulation?”, but debaters on\nboth sides would agree that this definition captures the key\ndifference between true communication signals on the one hand,\nand tactical behaviors or inadvertent cues on the other. For\nexample, while we might describe an individual delivering a\nblow to a potential opponent as ‘sending a message,’ we mean\nthis only in a metaphorical sense. This behavior is primarily\ntactical, that is, the individual delivering the blow will directly\nbenefit it if its opponent responds by backing down. If instead of\ndelivering a blow the individual had said “I’m going to kill you,”\nor growled, or barked, or hissed, we would recognize these as true\ncommunication signals, having been shaped by natural selection\nfor the purpose of (literally) sending a message, and requiring\nadaptations in the receiver as well.\nHockett listed prevarication – the ability to transmit\nmisinformation, i.e., to lie or deceive – as one of his many\ndesign features, albeit a minor one, a corollary almost. In\nSection “Communication: Information or Influence? Mutual\nBenefit or Manipulation?”, I will argue that we should consider\nprevarication to be a fundamental, indeed foundational feature\nof animal communication systems: communication in animals\nis shaped by the tension between the sender’s and receiver’s\ninterests, and truth in communication is not a given, but rather,\nwhen it occurs, hard won.\nSemanticity: Specific Signals Are Directly\nTied to Certain Meanings\nTo say that a communication system is semantic is to say that\nit uses signals to represent particular things or actions. A wellknown example in animals are alarm signals given in response to\ndifferent predators. We can say in such cases that each of these\nsignals represents one of several different predators, or more\nprecisely, the appearance on the scene of one of these predators.\nFor example, vervet monkeys have three different alarm calls for\nthree different classes of predators: raptors, terrestrial mammals\nand snakes, predators which depend on an element of surprise\nto capture the monkey. In response to an aerial predator, such\nas a martial eagle, a monkey emits ‘cough’ calls and sender and\nreceivers take shelter in dense bushes or near the core of a tree.\nIn response to leopards, a monkey emits a ‘bark’ call and the\nmonkeys climb up to the tip of tree branches where leopards\ncannot safely go. Finally, if a monkey spots a dangerous snake,\nsuch as a python, it emits a ‘chutter’ call and the group gathers\naround the snake, standing upright and harassing it until it leaves\nthe area. Although the vervets use these same signals in other\ncontexts (e.g., intergroup fights) to represent different things,\nFrontiers in Psychology | www.frontiersin.org 2 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nthe modification of signal meaning in different contexts occurs\nin human language as well, and does not negatively impact the\nrepresentational quality of these signals (Seyfarth et al., 1990;\nPrice et al., 2015). Indeed, it is not unusual for an animal to use\na particular signal to mean different things in different contexts\n(Smith, 1997), similar to some words meaning totally different\nthings within different sentences.\nNevertheless, I will argue later in this paper that the\nsemanticity of animal communication systems is limited:\nalthough some things are represented by animal signals, the\nnumber of things is generally small. Attempts to catalog the\nnumber of different things signaled in animal communication\nsystems typically top out at 25 or so (vervet monkeys, Struhsaker,\n1967; Japanese macaques, Green, 1975; review in Hauser, 2000).\nThe limitation does not appear to be due to production\nconstraints (the ability to produce enough distinct signals or\nto recombine enough of them to enlarge the signal set) or to\nperceptual-cognitive constraints.\nArbitrariness: Languages Are Made Up\nof Arbitrary Symbols Which Have No\nIntrinsic or Logical Connection to What\nThey Represent\nA distinctive feature of human language is that not only are\nwords semantic, they are arbitrarily so. We could equally well\ncall dogs ‘cats’ and cats ‘dogs,’ or any other two words, so long\nas sender and receiver knew the convention, a point illustrated by\nthe existence of the many different languages of the world. These\nsignals seem totally arbitrary with respect to what they signify,\nand in theory they could be interchanged without problems, so\nlong as senders and receivers were both aware of the convention.\nHow about animal signals? It appears that in theory we could\ninterchange the vervet alarm signals without problems, provided\nof course that the receivers were aware of the ‘convention’ (i.e.,\nwere hard-wired appropriately). Identity signals – indicating\nspecies or individual identity, and occasionally group or kinship –\nare perhaps the most common animals signals that unequivocally\nhave the arbitrariness feature.\nBut many, perhaps most, animal signals are not arbitrary.\nSignals used in agonistic and mate attraction contexts are\ntypically “more of ” signals, i.e., more effective signals are louder,\nlonger, bigger, brighter, flashier, designed to impress or to shock\nand awe. I am unaware of any clear example where the reverse\nis true, where the more effective signal is the one that is less\nconspicuous, for example, a softer sound, a more subdued color,\na less vigorous display. An apparent exception might be the\n‘quiet song’ sung by many songbirds in intense conflict situations,\nbut this typically happens only when the bird is close to its\nopponent so that the quiet song is audible to the receiver (Searcy\net al., 2014); ‘normal’ song is loud because it is a long-distance\nsignal. Moreover, quiet song is typically different in other respects\nbesides loudness, for example, having some elements seen only in\nquiet song, such as very high frequency elements.\nOther animal signals are simple extensions or slight\nmodifications of tactical behaviors, e.g., of attack behavior in\nagonistic situations. For example, a threat signal in many\nmammals is the open mouth display, where the teeth, the\ncanines notably, are prominently displayed. Ethologists called\nthis a ‘ritualized’ display (Lorenz, 1966), i.e., one that has\nbeen modified by natural selection to be a display, since the\nmouth is held open, and attack withheld, rather than being\nthe beginning of an actual attack. Another common threat\nsignal is the raising of the hair or feathers, making the animal\nappear larger. Again, while these actions are plausibly considered\nritualized displays, they are not arbitrary signals. If they were,\nyou would also find cases where animals threaten by closing\ntheir mouths, or by making themselves appear small. In short,\nanimal signals functioning to impress an opponent or potential\nmating partner are usually inherently impressive, not arbitrarily\nselected to represent threat or desirability. Any naïve observer\nviewing a ritualized dominance interaction between two wolves\n(or dogs) would have no difficulty determining which animal was\ndominant and which was subordinate. An upright animal, with its\nhair raised, its tail raised, and staring at its opponent inherently\nappears dominant, whereas one with a flattened, slinking body,\nhair down, tail down, and looking away from the opponent,\ninherently appears subordinate.\nMany epigamic signals – signals designed to attract a mate\nand induce her to mate – are bright, striking ornaments, often\nones that function like supernormal stimuli (e.g., the tail of\nthe long-tailed widowbird, Andersson, 1982). Many epigamic\nsignals are energetically expensive and highly skilled behaviors,\nsuch as the complex male courtship dances of wolf spiders and\njumping spiders (Hebets and Uetz, 1999; Elias et al., 2012). The\nmotor performance revealed in these sorts of displays likely\nreflect whole-organism performance relating to survival, and thus\nshould be good indicators of individual signaler quality. There is\nconsiderable evidence that females choose mates in nature based\nupon their evaluations of male motor performance (reviewed\nin Byers et al., 2010). The relevant point here is that these\nsignals are not arbitrary, but inherently reflect the trait signaled:\nsignaler quality.\nEven in the example par excellence of communication of\ninformation about the external world – the honeybee dance\nlanguage – the signals are not quite so arbitrary as generally\nassumed. For example, if the dance is done outside the hive,\nwhere the sun is visible, the bee dances with respect to the\nactual position of the sun, rather than with respect to the vertical\n(Gould, 1975). That is, outside the hive, the symbology is not truly\narbitrary. Moreover, the distance to the target is represented by\nthe duration of the straight run – the further the distance, the\nlonger the run – so this is at least partially non-arbitrary as well.\nAlthough the words in human language are arbitrary – the\nexistence of different languages is the clearest evidence on\nthis point – they may be expressed in such a way to amplify\nor otherwise modify their meaning, as for example a loudly\nshouted “no” indicating stronger conviction. But what would\nbe considered an extra-linguistic feature for humans is often\nthe primary message in animals. For example, the initial stage\nof a battle between two male red deer consists of a roaring\ncontest (Clutton-Brock and Albon, 1979). This vocal signaling\nduel does far more than simply establish that each animal is a\nmale conspecific ready to defend or fight for the harem – this\nFrontiers in Psychology | www.frontiersin.org 3 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nundoubtedly was perceived by both parties before the contest\nbegan – rather, how loud and how long an individual roars\nestablishes how motivated and formidable he is, and is used by\nthe receiver to decide whether to continue the fight or depart.\nSimilarly, the plumage ornaments and courtship dance of a male\ngolden-collared manakin do far more than simply identify species\nand sex – that is simply the necessary first step – the brightness\nof the ornament and the skill of the dance determine whether the\nreceiver, the female, will choose to mate with this particular male\nor continue her search for the best possible mate (Stein and Uy,\n2006; Barske et al., 2011).\nIn summary, although we have examples of animal signals that\nare totally arbitrary, many others – perhaps most? – are not. I\nwould add that to date we have found nothing comparable to the\nmany different human languages, which are a consequence of the\narbitrariness feature. We do find geographical dialects in animals\n(e.g., Marler and Tamura, 1964; Wright and Dahlin, 2018), but\nas the name implies, these are relatively minor variations on\nthe basic signal set, nothing like the wholesale variation seen in\nhuman languages.\nLearnability and Cultural Transmission\nHuman language is both learned and taught. Most animal\ncommunication systems are neither. A well-known exception to\nthis generalization are the learned vocal communication signals\nof several taxa, most notably the oscine passerines (songbirds),\nhummingbirds and parrots among birds, and cetaceans and at\nleast some bat species among mammals (reviews in Janik, 2014;\nKnornschild, 2014; Nowicki and Searcy, 2014). Evidence for\nvocal learning and cultural transmission in some other birds\nand mammals as well (Walcott et al., 2006; Kroodsma et al.,\n2013; Stoeger and Manger, 2014; Garland and McGregor, 2020;\nBarker et al., 2021) suggests that this ability may lie closer to\nthe surface than is generally assumed, but at least at the present\ntime, vocal learning is thought to be rare in animals. Later in this\npaper I return to the best-studied example of vocal learning, song\nlearning in songbirds.\nWhere the communication signals are learned, we should\nexpect to find dialects, geographical variation in the signals.\nThe occurrence of dialects is one criterion for identifying\nthe occurrence of learning and potentially evidence for the\narbitrariness design feature. An example that may illustrate\nthe arbitrary nature of dialects is the recently-discovered\nmodification of the song in eastern white-throated sparrows to\nresemble the typical song of western white-throated sparrows.\nInvestigators have traced this change to eastern birds learning\nthe western version of the song on the migration grounds, where\nindividuals of the two populations mix (Otter et al., 2020). Most\neastern birds now sing the ‘western’ version of the song on\nthe breeding grounds, illustrating that the details of the song\nstructure are not crucial for its function. Although Otter et al.\n(2020) suggest that this change might have been driven by a\npreference on the part of eastern females, they give no evidence\nfor this hypothesis, nor plausible basis for it.\nPerhaps even rarer in animal communication systems than\nlearning is teaching. The commonly accepted criteria for\ndemonstrating teaching in non-human animals are that (1)\nteachers should modify their behavior in the presence of the\nlearner, (2) this change in behavior should result in no immediate\nbenefit to the teacher, and (3) the learner should acquire a\nbehavior quicker or better as a result (Caro and Hauser, 1992).\nIn song-learning studies the birds from whom the young bird\nlearns its song are conventionally referred to as ‘tutors,’ and\nalthough live birds are invariably more effective song tutors\nthan recorded song (review in Beecher, 2017), the term ‘tutor’\nis used purely as matter of convenience. In fact, in the most\ncommon context for song learning in nature, young birds learn\nfrom older birds who are or will be their territorial rivals, a\nvery different context from language learning in young humans,\nwhere ‘tutors’ are typically relatives or other interested parties\nwho ultimately (but not immediately) benefit from tutoring.\nNevertheless, even in the common songbird case where the young\nbird learns from territorial rivals, bird song tutoring would fit\nall three criteria for teaching if in fact the older bird reduces\nhis usual aggression when a young bird appears on his territory,\nincreases his counter-singing with the young bird in such a way\nas to facilitate learning, and benefits down the road from this\ntutoring (for example, the two cooperate in mutual defense of\ntheir territories, or against predators, or refrain from extra-pair\nmating with one another’s mates). We have indirect evidence\nfor song learning/teaching in song sparrows: mutual survival\nis greater in young birds and their primary tutor-neighbor\n(the one from whom they learn most of their songs) the more\nsongs the two of them ultimately share, i.e., the more songs\nthe tutee learned from the tutor, or the tutor taught the tutee\n(Beecher et al., 2020).\nProductivity: By Combining a Small\nNumber of Meaningless Units Into\nLarger Meaningful Signals, a Sender Is\nCapable of Producing Meaningful\nStatements About Virtually Anything\nThe sense in which I am using this term is captured by\nHauser (2000, p. 448): “the power of [human] language comes\nfrom our capacity to take meaningless syllables and combine\nthem into an unbounded number of meaningful words, and\nthen take these words and combine them into an unbounded\nnumber of meaningful expressions (Chomsky, 1986; StuddertKennedy, 1998).” I will define productivity as recombining\na smaller number of basic signal units to produce a larger\nnumber of signals, and thus, messages. Indeed, semanticity\n(representation) and productivity are probably the two central\nfeatures of human language: by combining basic phonetic units\ninto larger meaningful units, and combining these units further\nvia syntactical rules, we can say almost anything.\nAnimal communication systems are not productive in this\nsense, and this is the primary reason we do not refer to them\nas languages. We would be impressed if a vervet could say\nsomething like “Grab your infant and run from the leopard\ncoming from the west but watch out for the python who\nlikes to hide in the bushes just to the east of you.” A human\ncan say this kind of thing easily, combining a relatively small\nnumber of atomic units (phonemes) into very large number of\nFrontiers in Psychology | www.frontiersin.org 4 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nbasic signals (words) and combining these into a very large set\nof possible communications. I note that while there is some\ncontroversy in phonetics about exactly what are the units of\nproductive combination, there is agreement that all natural\nlanguages (including sign language) are made up of meaningless\natomic units that are combined into larger meaningful wholes\n(Zuidema and de Boer, 2009).\nInstead of productivity, we could describe the communication\nsystem in terms of information capacity. The information\ncapacity of human language is essentially infinite, in the sense\nthat, in theory, we can communicate virtually anything. Our\nmotor, sensory and cognitive capacities obviously will reduce\nhow much information actually gets transmitted and received.\nBut still, the fact is that we can transmit an enormous amount\nof information with language. Attempts to measure information\ncapacity or information transmission in animals, on the other\nhand, have given rather modest results. Two estimates of the\ninformation about distance and direction in the honeybee dance\nlanguage have given a high value of 14.9 bits (Gould, 1975)\nand a low value of 7.4 bits (Schürch and Ratnieks, 2015). My\ngroup has estimated the information capacity of the call signature\nsystem that parents of the colonial cliff swallow use to find their\noffspring in their large breeding colonies (Medvin et al., 1993).\nWe estimated the capacity as 8.76 bits, and the estimate would be\nsomewhat larger if we included information that can be derived\nfrom visual differences among cliff swallow chicks (Stoddard and\nBeecher, 1983). The information capacity of human language of\ncourse is orders of magnitude larger than this.\nWe certainly find the potential for productivity in bird\nsong. For example, most songbirds have multiple songs (song\n‘repertoires’), and the different songs are made up of different\nsyllables or notes in different orders, and these smaller units can\nbe used in more than one song. Still, although the units are there,\nand although songbirds may possess the cognitive capacity to\ncomprehend hierarchical structuring in vocal signals (Gentner\net al., 2006; but see van Heijningen et al., 2009), they do not\nuse these capacities to form different songs representing different\nthings. As Hauser (2000, p. 450) puts it, “in contrast to the\nrecombination of words into sentences by humans, the output of\nsongbird recombination does not change its meaning.” A minor\nexception are some songbirds who use some song types in a\nterritorial defense context and others in a mate attraction context\n(e.g., Byers, 1996). As discussed in the next section, theories on\nthe function of song repertoires abound, but they all agree that\nthe different songs function simply to provide diversity, rather\nthan to represent different things.\nSumming Up\nTable 1 summarizes the conclusions of this section. The natural\ncommunication systems of animals fall short of human language\non a number of the key design features of language. They\ncome closest on semanticity, where signals sometimes represent\nthings in the external world or within the signaler, and the\nsignals are sometimes truly arbitrary. However, more commonly\nanimal signals are not arbitrary but inherently meaningful,\ne.g., an animal making itself appear large is more frightening\nthan an animal making itself appear small. Most animal\ncommunication signals and responses are neither learned nor\nculturally transmitted. And, so far as we know, no animal\ncommunication has the sine qua non of language: productivity.\nBIRD SONG: COMPLEXITY WITHOUT\nPRODUCTIVITY\nThe oscine passerines (songbirds) are one of the rare animal taxa\nin which individuals learn their vocal communication signals.\nIn most animals, these vocal signals are ‘hard-wired,’ that is,\nthey develop normally whether or not the animal is exposed to\nthem early in life. It has long been noted that vocal learning in\nsongbirds has many similarities to language learning in humans\n(Marler, 1970; Doupe and Kuhl, 1999). These similarities include\nthe following. (1) The young bird needs to be exposed to normal\nspecies vocal signals in order to produce them as an adult. (2)\nThe sensory phase of song learning precedes the motor phase.\n(3) Auditory feedback (which can be abolished by deafening) is\nTABLE 1 | Key design features of communication systems (after Hockett, 1960, pruned and combined).\nFound in\nanimals?\nDesign feature Comment\nYes Specialization. The purpose of linguistic signals is communication and not some\nother biological function.\nTrue of animal communication systems, but this is essentially by\ndefinition.\nYes but\nlimited\nSemanticity. Specific signals are directly tied to certain meanings. Clear example are the alarm calls given to different classes of\npredators in a number of species. But the number of different things\nsignaled is typically very small.\nYes but\nrare\nArbitrariness. There is an arbitrary relationship between a signal and its\nmeaning. There is no inherent relationship between the form of a signal and\nwhat it refers to.\nAnimal signals are sometimes arbitrary. Often they have inherent\nmeaning that can be readily perceived by a naïve observer, e.g.,\nsignals used in mate attraction or agonistic encounters that are\ndesigned to impress or shock and awe.\nYes but\nrare\nLearnability and Cultural transmission. Human language is learnable, teachable and culturally transmitted.\nBird song appears to be one of the few animal examples that\npasses at least two of these criteria (teaching still not established).\nNo Productivity (based on Arbitrariness, Discreteness and Duality of patterning):\nlanguage made up of small meaningless units which can be combined into\nmany larger meaningful units which can be combined to say virtually anything.\nSome animals appear to have the motor and cognitive capacity for\na productive, language-like communication system but they do not\nuse this capacity to develop language-like communication systems.\nFrontiers in Psychology | www.frontiersin.org 5 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nnecessary for the translation of memorized sensory input into\nmotor production. (4) Vocal learning is most efficient in (and\nsometimes restricted to) a sensitive period early in life. (5) There\nare specialized parts of the brain dedicated to the vocal control\nsystem. (6) Song is socially learned and culturally transmitted,\nand in at least some cases it may be actively taught (e.g., CarousoPeck and Goldstein, 2019; Beecher et al., 2020). While notable\ndifferences exist among songbird species with regard to the\nnormal progression of song learning (Beecher and Brenowitz,\n2005), these six features are essentially true for all of the many\nsongbirds that have been studied to date.\nDespite the notable parallels between bird song learning\nand human language learning, none of the many studies\nendeavoring to teach a version of human language to animals\nhave focused on songbirds. This is all the more surprising\ngiven the language learning shown by Alex the African Gray\nParrot, a member of another avian taxon with vocal learning, the\npsittacines (Pepperberg, 1981, 1987). Moreover, songbirds have\nstrong cognitive capacities, a highly-developed vocal production\nmechanism, and a vocabulary of basic sound units in their\nsong that rivals or exceeds the basic sound units of human\nlanguage. There are even songbird species that can mimic human\nspeech sounds (e.g., Hill Mynah birds). On the face of it, all the\nrequisites would seem to be there to support a simple language\nin a songbird.\nWhat Is the Function of a Song\nRepertoire?\nIn contrast to well-studied white-crowned sparrows and zebra\nfinches, in most songbird species an individual bird will sing\nmultiple songs (has a song ‘repertoire’). For example, song\nsparrows typically have nine (plus or minus two or so) very\ndifferent songs. Each of these songs is made up of 5 or 6 distinct\nelements, and the order of these elements is important (Horning\net al., 1993). The songs do not have individual signatures and\nthe nine or so songs in a song sparrow’s repertoire are as\ndifferent among themselves as would be a collection of songs\ntaken at random one from each of nine or so different birds\n(Beecher et al., 1994). Song sparrows are somewhere on the\nmiddle of the song repertoire complexity scale: many species\nhave larger and even more complex song repertoires. The key\npoint for this discussion is that song repertoires provide clear\npotential for productivity, as song sparrows and many other\nsongbirds have as many or more distinct units in their vocal\ncommunication systems (e.g., about 100 in indigo buntings,\nThompson, 1970; and in swamp sparrows, Marler and Pickert,\n1984) as there are in human language (a typical language has\n40–45 phonemes).\nThe most popular hypothesis about song repertoires for north\ntemperate zone songbirds – where only males sing – is that\nthey are an epigamic signal produced by males to attract females\nand that larger repertoires are more attractive than smaller\nones (Catchpole, 1987; Searcy and Yasukawa, 1996; MacDougallShackleton, 1997; Collins, 2004). Focusing on just the wellstudied song sparrow, the evidence for this hypothesis is mixed\n(Searcy, 1984; Reid et al., 2004; Hill C. E. et al., 2011). The\nhandicap principle, discussed in the next section, would suggest\nthat if large song repertoires are preferred, it is because they are an\nindicator of some aspect of male quality. Reid et al. (2005) found\nsupport for this idea: song repertoire size in male song sparrows\ncorrelated with enhanced cell-mediated immune response (CMI)\nand relative heterozygosity. Anderson et al. (2017) hypothesized\nthat female song sparrows might prefer large-repertoire males\nbecause this feature is an indicator the overall learning ability\nof the male. However, they found no correlations between\nrepertoire size (or two other measures of song learning ability)\nwith an overall measure of learning ability (based on five different\nlearning tasks). I should note, however, that a correlation of vocal\nlearning ability with both overall learning ability and mating\nsuccess has been found in another songbird, the Satin Bowerbird,\na vocal mimic: in this case the vocal learning ability is the ability\nof males to mimic the calls of other local bird species, both the\nnumber of species mimicked, and the accuracy of the mimicry\n(Coleman et al., 2007; Keagy et al., 2009).\nAccording to another hypothesis, song repertoires play\na role in territorial competition, which in north temperate\nzone songbirds, where only males sing, is largely male-male\ncompetition, but outside the north temperate zone where both\nsexes sing, is pair-pair competition (e.g., Levin, 1996; Langmore,\n1998; Logue and Gammon, 2004). There are several hypotheses\nas to how repertoires might work in the territorial competition\ncontext. Song is used by most territorial songbirds at least in\npart as a keep-out signal, to ‘post’ their territory. Kroodsma\n(1988) argues that the vocal diversity provided by a repertoire\nfunctions to hold the attention of territorial competitors by\ndishabituating them to the territory owner’s singing, i.e., by\nholding their attention. As one piece of evidence, he points to\na positive correlation between repertoire size and population\ndensity in marsh wren populations, and also to the finding that\nbirds in denser populations cycle through their songs faster,\nagain a behavior that should reduce habituation (Kroodsma,\n1977). In contrast, song sparrows sing their much smaller\nrepertoires with eventual variety, i.e., singing each one of\ntheir song types many times before switching to another\ntype, and this would seem to argue against the dishabituation\nhypothesis. In western, resident populations of song sparrows,\nsong repertoires may function primarily to provide a bird with\nsongs matching all (or most) of his neighbors, and thus potential\nindividualized replies to each one of them (Beecher et al., 1997;\nand see next section).\nAlthough as this brief discussion indicates, the theoretical\ndebate has not yet concluded, the take-away point is that\nnone of these hypotheses view song repertoires as a form of\nsemantic communication. Rather they view repertoires as having\na direct effect on the receiver (dishabituation), or as permitting\nindividualized replies to multiple neighbors, or as quantitative\nsignals with inherent rather than semantic meaning, that is, more\nsongs (or more song syllables) are simply more effective.\nI should add that most single-song species appear to have\nthe potential to develop song repertoires yet do not tap into\nthis potential. For example, when examined over an entire\npopulation, indigo buntings have a repertoire of over a 100\ndistinct song syllables, yet a given individual uses just 6–8 of\nFrontiers in Psychology | www.frontiersin.org 6 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nthese in the single song it develops (Rice and Thompson, 1968;\nThompson, 1969; Baker and Boylan, 1995).\nAn Example: Communication in a\nNegotiation Context\nAlthough the different songs in a bird’s repertoire do not have\ndifferent meanings, a bird having a song repertoire can still use\nthe different songs to communicate in more subtle, nuanced\nways than might at first be suspected. In this section I describe\none such case: how song sparrows use the songs in their song\nrepertoire to negotiate territorial disputes. The general point I will\nmake is that their communication system is surprisingly complex\nand versatile, despite being neither semantic nor productive.\nAlthough I will not attempt to generalize to all songbirds given\nthe incredible diversity of the song communication systems seen\nin this group (Beecher and Brenowitz, 2005), I suspect that this\nconclusion – complexity without productivity – applies broadly\nto songbirds, and perhaps to all animals.\nSong sparrows have a territorial system like that found in\nmany animals and typical of many songbirds. An individual\ncarves out a territory where the mated pair will nest and\nraise their young, doing most of their feeding on the territory.\nSuitable habitat is typically densely occupied by conspecifics,\nso territorial disputes can arise during both the establishment\nand maintenance stages. The relationship between territorial\nneighbors can become relatively non-hostile once established,\nhowever, on the principle that the enemy you know is better\nthan the enemy you don’t know, generally referred to as the\n‘Dear Enemy’ relationship (Fisher, 1954; Akçay et al., 2009,\n2010; Beecher and Akçay, 2014). Because in territorial animals,\nneighbors have no fences, neighbors need to renegotiate territory\nboundaries from time to time. Negotiation can progress into\nfighting but avoiding fighting may benefit both parties and this\ncommon interest favors reliable signaling. Therefore, as I will\ndiscuss in Section “Communication: Information or Influence?\nMutual Benefit or Manipulation?”, we should expect to find some\ndegree of honest communication concerning not only fighting\nability (resource-holding potential) but also motivation to fight\n(e.g., at a particular point in time, one party may have more to\nlose than the other).\nSong sparrows in western, resident populations use their\nrepertoires in a complex way to carry out territory negotiations.\nAlthough they will engage in serious fights, established neighbors\nuse their signaling system to avoid fighting if possible. Before\nfighting they typically give their high-level threat signals, wing\nwaves and soft song (Searcy and Beecher, 2009; Searcy et al., 2014;\nAkçay et al., 2015a). But before reaching this stage, they use the\nsongs in their repertoires to escalate or de-escalate the dispute\nfollowing a set of ‘conventions’ predicated on which songs the two\nbirds happen to share (Beecher et al., 1996, 2000; Burt et al., 2001,\n2002; Beecher and Campbell, 2005; Akçay et al., 2011; Templeton\net al., 2012; Akçay et al., 2013, 2015b). Because western song\nsparrows learn songs from their neighbors in the area to which\nthey disperse after fledging, a bird typically shares some of his\nsongs with each of his immediate neighbors. The set of songs\nhe shares with one neighbor is typically different from the set\nhe shares with another. A partial example is shown in Figure 1.\nFor example, if we represent the different songs of a bird with\ndifferent capital letters, and the shared songs of neighbors with\nthe same capital letter, then Bird 1 might share his song types\nA, B, and C with his neighbor Bird 2, his song types C, D, and\nE with another neighbor, his song types E and F with a third\nneighbor, and finally G, H, and I with no neighbors (e.g., the\nbird he learned these songs from may have died). A typical\nterritorial negotiation might occur as follows. Suppose Bird 1’s\nmate finds an ideal place to build her nest just over the previouslyestablished boundary with Bird 2. Bird 1, aiming to establish this\nnew boundary, moves to that point and sings at his neighbor.\nTypically the two birds would still be a considerable distance\napart at this point and out of sight of one another (territories are\nlarge and song is a long-distance signal). Although Bird 1 could\nsing any one of his 9 songs to Bird 2, in this circumstance he\nwould typically ‘address’ Bird 2 by singing one of their shared\ntypes, A, B, or C. Let us say bird 1 sings B. Bird 2 can escalate\nby replying with his B’ (i.e., his most similar song to Bird 1’s B).\nThis ‘type match’ is a low-level threat signal and would be the\nfirst step in escalation. Alternatively, he could ‘confirm’ without\nescalating by replying with A’ or C’ (‘repertoire matches’, Beecher\net al., 1996). Note that this type of reply is only possible if Bird 2\nknows Bird 1 well enough to know which songs they share and\nwhich songs they don’t. Finally, rather than type-matching or\nrepertoire-matching, Bird 2 can de-escalate by singing one of his\nunshared types, e.g., D, E, F, G, H or I. Singing an unshared type\nis better than not singing at all because it signals that although\nthe singer is not engaging, he is on territory and has heard his\nneighbor; it is a signal likely used for example when the bird is\nbusy feeding recently-fledged young. If Bird 2 does type match\nbird 1 (sings B’), Bird 1 in turn can continue to sing that song\ntype (‘stay on type’), or he can de-escalate by switching to another\nshared song (A or C, ‘repertoire match’), or de-escalate further by\nswitching to an unshared type (e.g., D or E), or disengage totally\nby stopping singing.\nEach ‘convention’ – type matching, repertoire matching,\nstaying on type, switching to an unshared type – has a distinct\nsignaling function in this graded signaling system, with both type\nmatching and staying on type when type-matched signaling a\nreadiness to escalate, repertoire matching signaling recognition of\nthe sender and engagement but stopping short of escalation, and\nswitching to an unshared type signaling de-escalation. The system\nwhile not in itself resolving anything, does give the neighbors\ntime to defuse the situation or work out a compromise. Note,\nhowever, that the semantic content is limited. No particular song\nin the repertoire means a particular thing. A song’s meaning is\ndefined entirely by the context of who the receiver is, and even\nthen there are essentially only three meanings, roughly ‘back off,’\n‘I hear you and know who you are,’ and ‘I’m busy now.’\nSumming Up\nSongbirds check several of the design feature boxes and they\nwould appear to have the potential to use their songs in a\nproductive way, i.e., to use their signaling system to say many\nthings. However, despite considerable debate concerning the\nfunction of song repertoires, the different repertoire hypotheses\nall agree on one point: that the function of the vocal diversity\nFrontiers in Psychology | www.frontiersin.org 7 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nFIGURE 1 | Partial song repertoires of two neighboring birds. Shared songs are shown in the top three rows, and four of their unshared songs in the bottom two\nrows (they are arbitrarily paired). Frequency scale: 0–10 kHz. Songs are 2–3 s long.\nis diversity per se, not the transmission of different messages\nwith different songs. Perhaps even more surprising, many singlesong species have large song syllable repertoires an individual\ncould tap into, but instead each individual uses just several\nof these syllables to develop its single song. No songbird\nrearranges its multiple song syllables into different songs that\nsignal different things. I echo here the conclusion of Fitch and\nJarvis (2013, p. 502): although songbirds (and parrots) have vocal\nlearning and a complex vocal repertoire, they do not “use their\nsongs to communicate combinatorial propositional meanings,\ni.e., semantics.”. Songbirds may use their repertoires in subtle,\nnuanced ways, as with the song sparrow hierarchical signaling\nsystem I described above, but what the system achieves seems\nbetter described as the management of behavioral conflict than\nas an impressive transmission of information. That is, the system\nmay function well, but it does not function like a language.\nCOMMUNICATION: INFORMATION OR\nINFLUENCE? MUTUAL BENEFIT OR\nMANIPULATION?\nIn this section I discuss the debate within the field about\nthe fundamental nature of animal communication. I believe\nthis debate has provided us with a key to understanding\nwhy we find no examples of a simple language among the\nmany communication systems of non-human animals, and true\nlanguage only in the human animal.\nWe can trace the real beginning of the field of animal\ncommunication to the classical ethologists (e.g., Tinbergen,\n1952). The ethologists provided detailed descriptions of animal\nsignaling systems in nature, developed theories about the\nunderlying proximate causes (e.g., sign stimuli, innate release\nmechanisms, and fixed action patterns) and evolutionary\nprocesses (e.g., ritualization), and most relevant here, established\nthe view of animal communication as – like human language –\nan information transfer process. On the question of the function\nof animal signaling systems, they took a group-selectionist\nperspective: the benefit that a signaling system provided went not\nto signaler or receiver per se, but to the species (see Tinbergen,\n1964 definition in Table 2).\nFollowing the revolution of the 1960’s and 1970’s first known\nas sociobiology (Wilson, 1975) and subsequently as behavioral\necology (Krebs and Davies, 1978), natural selection came to be\nviewed as acting on individuals, rather than species or groups\n(Williams, 1966). For some researchers, the shift from naïve\ngroup selection to individual selection did not entail a significant\nchange in view: it was simply assumed that signaler and receiver\nFrontiers in Psychology | www.frontiersin.org 8 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nTABLE 2 | Definitions.\nTinbergen, 1964 “One party. . . emits a signal, while the other party. . . responds in such a way that the welfare of the species is\npromoted.”\nMarler, 1968 In “true communication. . . both parties seek to maximize the efficiency of information transfer.”\nOtte, 1974, p. 385 Signals: “behavioral, physiological, or morphological characteristics fashioned or maintained by natural selection\nbecause they convey information to other organisms”\nDawkins and Krebs, 1978, p. 283 “Communication is said to occur when an animal, the actor, does something which appears to be the result of selection\nto influence the sense organs of another animal, the reactor, so that the reactor’s behavior changes to the advantage of\nthe actor.”\nGreen and Marler, 1979, p. 73 “Communication consists of the transmission of information from one animal to another.”\nKrebs and Dawkins, 1984, p. 401 They call the sender role the ‘manipulator’ and the receiver role the ‘mind-reader.’ “The manipulator role is selected to\nalter the behavior of others to its advantage, the mind-reader role to anticipate the future behavior of others.”\nSmith, 1997, p. 11 Communication: “any sharing of information between entities—in social animals, between individual animals”\nBradbury and Vehrencamp, 1998, p. 3 True communication: “information exchange from which both sender and receiver benefit.”\nMaynard Smith and Harper, 2003, p. 3 A signal is “any act or structure that alters the behavior of other organisms, which evolved because of that effect, and\nwhich is effective because the receiver’s response has also evolved.”\nOwren et al., 2010, p. 771 Animal Signaling: “the use of specialized, species-typical morphology or behavior to influence the current or future\nbehavior of another individual.”\nboth benefited from the transmission of information, and so\nthis basic parallel with human language was maintained (see\nTable 2 definitions of Marler, 1968; Otte, 1974). The assumption\nof mutual benefit seemed natural in cases where sender and\nreceiver have a strong common interest, e.g., the honeybee ‘dance\nlanguage’ where scout and recruit are both working toward the\nsame end, to provide food for their relatives in the hive. But as\ninvestigators began considering the many cases where signaler\nand receiver have conflicting interests, such as in agonistic\nencounters over an indivisible resource, they began to question\nthe mutual-benefit, information transmission view. They asked\ntwo questions about such cases. First, do both parties have to\nbenefit? Second, do we need to even talk about ‘information\ntransmission’? Isn’t the signaler simply selected to manipulate\n(or influence) the behavior of the receiver to its advantage? The\nmanipulation viewpoint was famously developed by Dawkins and\nKrebs (1978) who argued that rather than expecting signalers to\nsignal honestly, we should expect them to manipulate the receiver\nto their own advantage, e.g., to convince opponents to retreat, or\npotential partners to mate with them.\nSince the Dawkins and Krebs (1978) paper, the debate\nhas continued as to whether it is justified or productive to\nconceptualize animal signaling as an information transmission\nprocess in which both parties benefit. Simplifying somewhat,\nI will distinguish between the Information Transmission and\nManipulation approaches to animal communication. Strong\narguments on the manipulation side since Dawkins and Krebs\n(1978) include Krebs and Dawkins (1984), Owings and Morton\n(1998), Scott-Phillips (2008), Rendall et al. (2009), and Owren\net al. (2010). Strong arguments on the information side over this\nsame period include Green and Marler (1979), Smith (1997),\nBradbury and Vehrencamp (1998), Searcy and Nowicki (2005),\nCarazo and Font (2010), Seyfarth et al. (2010), and Wiley (2013).\nDefinitions from some of these sources are included in Table 2.\nIn conceiving of signaling as manipulation, Dawkins and\nKrebs (1978) essentially treated the communication interaction\nlike a zero-sum game. This seems reasonable in cases like disputes\nover an indivisible resource (a food item, a territory, and a\nmate), and also in epigamic selection, where a male tries to\npersuade a female to mate with him now rather than to continue\nsearching for a possibly better male. Although the manipulation\nview was enlightening in many respects, as originally presented\nit had a serious weakness: it gave no agency to the receiver.\nWhile it was sensible to expect signalers to signal for their own\nbenefit, why should we expect receivers to be passive in these\nevolutionary scenarios, especially if being manipulated by the\nsignaler is costly? Rather, we should expect receivers to show\n‘sales resistance’ to signals that carry misinformation or are pure\npropaganda (“I am the best,” “I will fight you to death”). Indeed,\nreceivers can do more than simply ignore signals that do not\nbenefit them: they can require signals that do benefit them, even\nif those signals are costly to the sender. For example, in many\nspecies males must sing or call to attract a female for mating. If\nthe male does not vocalize, potential female receivers will simply\nnot engage. Moreover, these vocal signals may attract predators,\na cost borne by the signaler but not the receiver. Indeed, the\nmost effective or most-preferred signals may be the most costly,\ne.g., most conspicuous not just to the intended receiver but to\npredators as well. This is the case for a male túngara frog (Ryan\nand Rand, 1990). Males attract females to mate with a ‘whine’\ncall or a ‘whine-chuck’ call. When a male adds chucks to his\ncalls, he not only attracts more females, but also predators: frogeating bats that home in specifically on the chucks. Similarly, a\ncalling male field cricket attracts more females than does a silent\nmale, but he also attracts more parasitoid flies, and louder calls\nattract both more females and more parasitoid flies (Cade, 1975).\nIn some populations the rate of fly parasitism is so high that\nmales have lost the ability to sing (Zuk et al., 2006). As another\nexample, territorial animals often vocalize as a “keep-out” signal.\nWhen a territorial songbird is deprived of its voice, however,\npotential rivals show up and proceed to take over its territory\n(e.g., McDonald, 1989).\nIf we reframe our view of the communication system as\nbeginning with the implicit requirement that the receiver imposes\non the signaler—to signal—rather than with the signal itself, it\nis apparent that receivers can be conceived of as manipulating\nFrontiers in Psychology | www.frontiersin.org 9 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nsignalers, and in the ‘receiver manipulation’ view, the potential\ncosts to the sender are secondary to the potential benefits to\nthe receiver. A possible benefit for the female túngara frog –\nthe receiver in our example – might be a shorter search time in\nnavigating to the male who adds the more localizable chucks to\nhis calls, perhaps lessening her vulnerability to predation.\nThe receiver manipulation view prompts us to consider how\nthe receiver might demand a more honest signal. There are two\nrelated possibilities. First, the receiver can selectively attend to\nsignals that are inherently honest due to physical constraints. For\nexample, in many frogs and toads, size is the most important\nweapon in male battles over mating opportunities and size\nis reliably predicted by the pitch of the animal’s vocalization:\nlarger animals give lower-pitched calls. Davies and Halliday\n(1978) showed that playback of low-pitched calls was sufficient\nto discourage smaller males from entering into battle with\nan apparently larger male. A second way to require a more\nreliable signal has generally been discussed under the rubric\nof the ‘handicap’ principle. This principle was first proposed\nby Zahavi (1975), modified and formalized by Grafen (1990),\ngiven the intuitively pleasing graphical formulation by Johnstone\n(1997) shown in Figure 2, and is still being subjected to\nfurther modification and clarification (e.g., Penn and Számadó,\n2018). But the basic principle is straight-forward, and can be\nverbalized as follows: signals whose degree of expression is\ndependent on the health, general condition or vigor of the\nsignaler are inherently honest expressions of that individual’s\nquality. For a high-quality signaler, a ‘bigger’ signal is a smaller\nhandicap (less costly, or more affordable) than it is for a\nlow-quality signaler, thus ‘big’ signals are reliable signals of\nFIGURE 2 | Johnstone’s graphical model of the Handicap principle. The basic\nassumption is that it costs a high-quality signaler less to signal at its optimum\nlevel than it costs a low-quality signaler to signal at that level. The optimum or\nequilibrium level (where the difference between the costs and benefits of\nsignaling are greatest) for the low quality signaler is lower (opt low) than that\nfor the high-quality signaler (opt high). Thus the signaling level is a reliable\nindicator of signaler quality.\nsignaler quality. One of the clearest demonstrations of honesty\nin an epigamic signal was carried out by Petrie and her\ncolleagues on that poster animal for epigamic signaling, the\npeacock. Petrie and colleagues demonstrated that in their peacock\npopulation, females preferred a mate with more eyespots in his\nfeather train (whether the difference was natural, or produced\nby experimental manipulation), and that females mated with\nmales with more eyespots had more young surviving to a\nyear of age than females mated to males with fewer eyespots\n(Petrie et al., 1991; Petrie, 1994; Petrie and Halliday, 1994).\nAlthough the generality of these results has been questioned by\nstudies on other populations (Takahashi et al., 2008; Dakin and\nMontgomerie, 2011), the example provides a clear illustration\nof the predictions generated by the handicap principle, and how\nthey should be tested.\nThe handicap principle should maintain some degree of\nhonesty in any signaling system where signaler and receiver have\nnon-identical interests, such as virtually all mating and agonistic\ncontexts. A low-quality individual can only ‘lie’ by diverting\nenergy into signal development and expression that it needs for\nmaintenance, and so as Searcy and Nowicki (2005) succinctly put\nit, lying becomes more costly than signaling honestly. Searcy and\nNowicki suggest that ‘reliable’ is a better word here than ‘honest,’\nfor several reasons. First, as with reliability testing in science\nand elsewhere, we understand that although perfect reliability is\nunattainable, partial reliability may be good enough. In contrast,\n‘honesty’ is generally taken to mean absolute honesty. Second,\nreliability of a signal is empirically measurable. Thus instead\nof debating whether an animal signal is informative or not,\nwe can measure if it predicts something important about the\npresent state of affairs or future events. Thus for example, in\nan agonistic situation a ‘threat signal’ should predict subsequent\nescalation, and the strongest ‘threat’ signal should predict attack\n(Searcy and Beecher, 2009).\nSumming Up: Two Perspectives\nHistorically, the Information Transmission and Manipulation\nviews of animal communication systems have been presented\nas in opposition. I suggest that in fact they are simply different\nperspectives on the same process. Once we give the receiver\nagency, and accept that manipulation is a two-way or reciprocal\nprocess in animal communication, we see that the two views have\nmore in common than was at first thought. This rapprochement\nis nicely captured in the evolution of Dawkins and Krebs’s papers\non the topic. In their original paper, Dawkins and Krebs (1978)\nfocused on signalers and argued that “natural selection favors\n[signalers] who successfully manipulate [receivers] whether or\nnot this is to the advantage of the manipulated individuals.”\nHowever, 6 years later in a follow-up paper (Krebs and Dawkins,\n1984) they expanded their view to include receiver interests,\nnoting that receivers would be favored to resist manipulation\nand to attempt to “read the minds” of signalers. Finally, Krebs\n(1991), discussing Zahavi’s handicap principle, concluded that\nthe manipulation and honest signaling views are probably\nnot incompatible: “Dawkins and Krebs (1978) discussed a\ncoevolutionary process without specifying an end point, whereas\nZahavi was concerned mainly with the end-point itself, so it is\nFrontiers in Psychology | www.frontiersin.org 10 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\npossible to imagine an evolutionary arms race of manipulation\nand sales resistance which end up with honest signaling”\n(Krebs, 1991, p. 67).\nFigure 3 is a schematic representation of what I will call the\nReciprocal Manipulation view. It shows communication taking\nplace on a battleground in which signaler and receiver are each\nselected to manipulate the other, the battle being settled in the\nlong run with the compromise of mostly-honest (reliable) signals.\nThe “management-assessment” theory of Owings and Morton\n(1997, 1998) is quite similar to the Reciprocal Manipulation\nview. Their theory captures the dynamics of signalers attempting\nto manage receivers and receivers assessing signalers. In their\nwords “the process of assessment is more active than has been\ngenerally recognized, and is responsible for the ‘informational’\ncouplings between individuals” (1997, p. 359). However, receivers\ndo more than just assess signalers, they manipulate them as\nwell, requiring them to signal in the first place, and requiring\na relatively honest signal as a prerequisite for responding to\nthe signal. The Reliable Signaling view of Searcy and Nowicki\n(2005) is essentially identical to the Reciprocal Manipulation\nview, with the superficial difference that the former focuses on\nthe information transmission aspect (reliable signaling) while\nthe latter focuses on the manipulation aspect (the conflicting\nmotivations of signaler and receiver).\nThe Reciprocal Manipulation and Information Transmission\nviews each seem most helpful in different circumstances\n(Table 3). Where the interests and thus motivations of the two\nparties differ, the Reciprocal Manipulation highlights the clash.\nIn contrast, where the interests and motivations of the two\nparties are more in line, the Information Transmission viewpoint\nfocuses on the essence of the interaction. Indeed, where the\noverlap of sender and receiver interests is considerable, as\nfor example between related individuals, or mates caring for\noffspring, or individuals in a social group where individuals are\nFIGURE 3 | Schematic suggesting the opposing pressures favoring signaler\nover receiver or vice-versa. Where interests of signaler and receiver are\ncoincident or nearly so (light gray to white) reliable communication will occur.\nAt the extremes of the space (darker), where interests of one or the other of\nthe two parties predominates, signaling will be disfavored. In the intermediate\n(gray) region, one party may benefit more than the other, but signaling may still\nbe ‘reliable enough.’\nstrongly interdependent, reliable, mutually beneficial signals will\nbe favored. But even where the interests of sender and receiver\nare partially opposed, selection acting on both parties will move\nthem to the region where both parties benefit on average, and\nsignals will still be reliable, if less so. This game theory dynamic\nhas been clearly laid out elsewhere (Maynard Smith and Harper,\n2003; Godfrey-Smith, 2013).\nI believe that the clash between these views of animal\ncommunication has ultimately led us to a clearer view of\nanimal communication systems than the original humanoriented information transmission view. Most animal\ncommunication systems are somewhere on the continuum\nfrom pure manipulation to pure communication, from arms race\n(where sender and receiver have different interests, each selected\nto behave so as to benefit themselves) to pure information\ntransmission (where sender and receiver have identical interests,\nand where signals benefit or cost both parties in the same way or\nto the same degree). A fuller development of these ideas can be\nfound in Beecher (2020).\nIn conclusion, I have argued that we should expect that natural\ncommunication systems will generally be reliable, even if not\nperfectly honest, with signaler and receiver both benefiting on\naverage. However, returning to the main theme of this paper,\nthere is no reason to expect such systems to blossom into\nsimple languages unless signalers and receivers have identical\nor near-identical interests, and if the ecological selective context\nrequires strong cooperation. There are cognitive prerequisites\nas well – otherwise one might predict that honeybees should\nhave a simple language – but the brake on the evolution to\nlanguage-like signaling systems in species with the requisite\ncognitive capacity is provided by the generally divergent\ninterests of signaler and receiver. Otherwise, bonobos, dolphins\nand some other vertebrates who seem to have the necessary\ncognitive prerequisites would have a more language-like natural\ncommunication systems than they do.\nWHY ARE THERE NO NATURAL\nLANGUAGE SYSTEMS IN ANIMALS?\nResearch on teaching animals simple human language indicate\nthat at least some animals appear to have the cognitive capacity to\ndecode language or language-like expressions. Herman’s dolphins\ncould comprehend a sign language command such as “take\nthe ball to the hoop” and to distinguish it from a similar but\nsyntactically different command like “take the hoop to the ball”\n(Herman, 2010). Kanzi the bonobo could respond correctly\nto novel verbal commands such as “Can you put the pine\nneedles in the refrigerator?” (Savage-Rumbaugh et al., 1993).\nPepperberg (1981, 1987) and Pailian et al. (2020) have shown\nthat African gray parrots can follow verbal directions to solve\ndifficult problems, including some that challenge humans. Yet\ndespite having the apparent capacities, at least to some extent,\nno non-human animal uses even a rudimentary language in its\nday-to-day existence. This includes groups like the songbirds that\nseem to have a crucial design feature, the learning and cultural\ntransmission of a complex set of vocal signals. Some animals\nFrontiers in Psychology | www.frontiersin.org 11 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nTABLE 3 | Differences between reciprocal manipulation and information transmission perspectives.\nPerspective\nReciprocal manipulation Information transmission\nFocus on which aspect of the coevolutionary process? On the process itself On the end point of the process\nMost useful when sender and receiver interests are: Divergent Coincident\nFocus on what variable? Differing motivations of sender and receiver Information transmitted from sender to receiver\nappear to be smart enough, or capable enough to handle a simple\nlanguage, but we have yet to discover an animal communication\nsystem – in nature – that rises to this level. Thus it appears that\nsome missing element other than cognitive or motor limitations\nhas blocked language evolution in non-human animals. Although\nit is possible that yet some other cognitive limitation has not\nbeen clearly identified (Hauser et al., 2002; Pinker and Jackendoff,\n2005), I focus in this final section on a candidate for the missing\nelement that is not purely a cognitive mechanism.\nA clue as to the missing element comes from the honeybee\n‘dance language.’ Despite a relatively simple nervous system,\nhoneybees are able not only to transmit precise information\nabout events in the external world, but also to use this system\nin two very different contexts (when talking about the location of\ndesirable food sources or about the location of suitable hive sites).\nThe key ingredient for the evolution of this system, I would argue,\nis zero conflict of interest between sender and receiver. Both scout\nand recruit are sister sterile workers and they are both working\nto feed sisters and brothers slated to be future reproductives.\nHumans also evolved in a social system featuring extraordinary\nlevels of cooperation, but significantly this cooperation was not\nrestricted to close relatives, as it is in the honeybees and other\nsocial insects, ruling out kin selection as a sufficient explanation\n(but see Fitch, 2004).\nI will reframe the question from “why not them?” to the\nquestion of “why us” (phrasing suggested by Hrdy, 2009)?\nHow did the human animal become the one species to evolve\nlanguage? As I argued in the previous section, the field has\narrived at a consensus concerning the factors that shape animal\ncommunication systems: the pressure for sender and receiver\neach to shape the interaction to its benefit inevitably both\nstimulates and constrains the evolution of the communication\nsystem. Very unusual circumstances are required for a true\nlanguage system to evolve. Three essential conditions have to\nbe met. First, the species must have the underlying cognitive\ncapacity. Honeybees may lack this, but some other animals may\nhave it. Second, and this is the clue provided by honeybees, sender\nand receiver must have identical or near identical interests. Third,\nindividuals must have a compelling need to transmit information\nacross multiple contexts. These are precisely the conditions that\nexisted in pre-human and early human hunter-gatherer societies,\nthe context in which humans and our hominid precursors spent\nsome 95% of our evolutionary history. The description of the\nprototypical hunter-gatherer society that follows is based on\ninformation from a number of sources (including Boehm, 1999;\nBowles, 2006; Hrdy, 2009; Hill K. et al., 2011; Knight and Power,\n2011; Lee, 2018).\nOur hunter-gather ancestors lived in small social groups\nwhere individuals were strongly interdependent, and cooperation\nacross multiple contexts was essential for survival. Most highly\ncooperative animal societies such as the eusocial insects are\ntypically just very large families, but the human hunter-gatherer\nsocieties we know – and which we assume to be typical of\nthe ancestral type – consisted of members of several kin lines.\nThus human societies then – and now as well – required\nextensive cooperation among unrelated individuals. Humans are\nthe supreme cooperators in the animal world, but because this\ncooperation is not supported by high kin relatedness, it has\nto withstand a strong undercurrent of individual competition.\nWe sometimes lose sight of the human affinity for withingroup cooperation because of its paradoxical coexistence with\nintense between-group competition and tribalism. Irreconcilable\nconflicts within ancestral hunter-gatherer groups surely occurred,\nbut were often resolved by individuals leaving one group for\nanother (hunter-gatherer societies being classic examples of\nfission-fusion societies).\nStudents of human evolution, while differing as to what were\nthe key selective contexts, or the key adaptations, all agree that\nhuman evolution has been characterized by remarkable levels\nof within-group cooperation among unrelated individuals, on\na scale not seen in any non-human animal. Several contexts\nstand out as crucial for the high level of cooperation found\nin hunter-gather societies. They begin, of course, with hunting\nand gathering. Effective group hunting (usually done by men)\nrequires sharing of information about distant prey and discussion\nof strategies for capturing prey. In essentially the same way,\ngathering of plants and fruits (usually done by women) requires\nthe ability to track the growing schedules and locations of many\nplants and fruits in the area and the ability to discuss and\ncoordinate foraging activities efficiently. Furthermore, huntergatherer societies periodically have to pick up and move to a\nnew, more abundant locale. These moves require discussion and\ngroup consensus, with input from all parties, especially older,\nmore experienced men and women.\nA second, equally important axis of cooperation is childraising. Humans are unique among primates in the time and\ncost required to raise an offspring. Humans solved this problem\nby involving the whole group in the process. Hrdy (2009) has\npointed out that this pattern of cooperative breeding sets humans\napart from the exclusive mother-centered parenting of our closest\nrelatives, the great apes. In these early human societies, many\nindividuals played a role in the cooperative care. For starters, the\nwhole group participated in that food brought back to the camp\nwas typically shared among all individuals, without reference\nFrontiers in Psychology | www.frontiersin.org 12 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nto their role in procuring the food. Then unlike most mammals,\nthe father participated in child care alongside the mother. Other\nrelatives were routinely involved in direct child care, especially\nolder siblings and grandparents, often aunts and uncles too, and\nsometimes non-relatives as well.\nFinally, within-group cooperation is essential for success in\nbetween-group competition, warfare in particular. This aspect of\nour hunter-gather heritage is strongly debated in anthropology.\nUsing the terms of Lee (2018), the Peaceful school views\nsignificant inter-group competition as not beginning until the\nAgricultural era, when property gave humans something to fight\nover. The Bellicose school (e.g., Kelly, 2000; Gat, 2015) believes\ninter-group competition dates further back in our evolutionary\npast. But whenever it started, warfare would certainly promote\nadaptations for within-group cooperation.\nIn recent years various investigators have proposed key\nadaptations that may have allowed human societies to achieve\nthis high level of cooperation in the absence of the glue of a very\nhigh level of kinship. Although there is not complete agreement\nas to which of these adaptations were most crucial, taken together\nthey coalesce into a suite of psychological adaptations that\npromote prosocial within-group interactions within a context\nof near-complete interdependence. Indeed, Tomasello et al.\n(2012) have dubbed this the Interdependence hypothesis. The\nspecific adaptations include: shared intentionality (Tomasello\net al., 2005), egalitarianism (Boehm, 1999), social learning\nand communication (Herrmann et al., 2007), intersubjectivity\nand empathy (Hrdy, 2009), moral intuitions (Haidt, 2012),\nadaptations for teaching and receiving teaching, and thus cultural\ntransmission (Sterelny, 2012; Henrich, 2016; Whiten, 2017),\nproactive aggression (Wrangham, 2018) and self-domestication\n(Wrangham, 2019). These adaptations of our social mind appear\nto be what set us apart from the other great apes, who it\nhas been argued are otherwise just as cognitively advanced\n(Herrmann et al., 2007). This suite of adaptations has enabled\nus to live in complex, cooperative societies. Despite our equally\nextraordinary proactive (deliberate and planned) aggressive\ntendencies, directed typically at out-groups, as in wars, pogroms,\ncrusades and the like (Wrangham, 2018), no other social animal\nhas achieved the level of within-group docility and cooperation\nwithout high within-group relatedness that is found in the human\nspecies. I note that Knight (2018) has an advanced an argument\nsimilar to the one I have presented here.\nLanguage unquestionably represents the pinnacle of evolved\nanimal communication systems, and as noted at the beginning\nof this section, attempts to teach language to animals have\nnot significantly changed this view. Language is often given\npride of place in human evolution. In this view the other\nadaptations mentioned above came only after some form of\nlanguage was in place. I favor the view of Hrdy (2009), that this\nmay well reverse cause and effect. The evolution of language\nmay have only become possible when the posited unique suite of\nprosocial, communicative and mind-reading adaptations were in\nplace. The crucial importance of communication in the strongly\ninterdependent social system of early humans would have created\nthis prosocial suite of adaptations, and would have laid the\ngroundwork for evolving a true language.\nAUTHOR CONTRIBUTIONS\nThe author confirms being the sole contributor of this work and\nhas approved it for publication.\nACKNOWLEDGMENTS\nMany thanks to editor IP, three reviewers, John Byers, Doug\nMock, Trish Schwagmeyer, and Bill Searcy for their very\nthoughtful reviews of the manuscript.\nREFERENCES\nAkçay, Ç, Anderson, R. C., Nowicki, S., Beecher, M. D., and Searcy, W. A. (2015a).\nQuiet threats: soft song as an aggressive signal in birds. Anim. Behav. 105,\n267–274. doi: 10.1016/j.anbehav.2015.03.009\nAkçay, Ç, Campbell, S. E., and Beecher, M. D. (2015b). The fitness consequences of\nhonesty: under-signalers have a survival advantage in song sparrows. Evolution\n69, 3186–3193. doi: 10.1111/evo.12818\nAkçay, Ç, Reed, V. A., Campbell, S. E., Templeton, C. N., and Beecher, M. D.\n(2010). Indirect reciprocity: song sparrows distrust aggressive neighbors based\non eavesdropping. Anim. Behav. 80, 1041–1047. doi: 10.1016/j.anbehav.2010.\n09.009\nAkçay, Ç, Tom, M. E., Campbell, S. E., and Beecher, M. D. (2013). Song\ntype matching is an honest early threat signal in a hierarchical animal\ncommunication system. Proc. R. Soc. Lond. B Biol. Sci. 280:20122517. doi:\n10.1098/rspb.2012.2517\nAkçay, Ç, Tom, M. E., Holmes, D., Campbell, S. E., and Beecher, M. D.\n(2011). Sing softly and carry a big stick: signals of aggressive intent in\nsong sparrows. Anim. Behav. 82, 377–382. doi: 10.1016/j.anbehav.2011.\n05.016\nAkçay, Ç, Wood, W. E., Searcy, W. A., Templeton, C. N., Campbell, S. E., and\nBeecher, M. D. (2009). Good neighbour, bad neighbour: song sparrows retaliate\nagainst aggressive rivals. Anim. Behav. 78, 97–102. doi: 10.1016/j.anbehav.2009.\n03.023\nAnderson, R. C., Searcy, W. A., Peters, S., Hughes, M., DuBois, A. L., and\nNowicki, S. (2017). Song learning and cognitive ability are not consistently\nrelated in a songbird. Anim. Cogn. 20, 309–320. doi: 10.1007/s10071-016-\n1053-7\nAndersson, M. (1982). Female choice selects for extreme tail length in a widowbird.\nNature 299, 818–820. doi: 10.1038/299818a0\nBaker, M. C., and Boylan, J. T. (1995). A catalog of song syllables of indigo and\nlazuli buntings. Condor 97, 1028–1040. doi: 10.2307/1369541\nBarker, A. J., Veviurko, G., Bennett, N. C., Hart, D. W., Mograby, L., and Lewin,\nG. R. (2021). Cultural transmission of vocal dialect in the naked mole-rat.\nScience 371, 503–507. doi: 10.1126/science.abc6588\nBarske, J., Schlinger, B. A., Wikelski, M., and Fusani, L. (2011). Female choice\nfor male motor skills. Proc. R. Soc. Lond., B Biol. Sci. 278, 3523–3528. doi:\n10.1098/rspb.2011.0382\nBeecher, M. D. (2017). Birdsong learning as a social process. Anim. Behav. 124,\n233–246. doi: 10.1016/j.anbehav.2016.09.001\nBeecher, M. D. (2020). “Animal communication,” in Oxford Encyclopedia of the\nHistory of Psychology, ed. W. Pickren (Oxford, UK: Oxford University Press).\nBeecher, M. D., and Akçay, Ç (2014). “Friends and enemies: how social dynamics\nshape communication and song learning in song sparrows,” in Animal Behavior,\ned. K. Yakusawa (Santa Barbara, CA: Praeger), 33–61.\nBeecher, M. D., Akçay, Ç, and Campbell, S. E. (2020). Birdsong learning is mutually\nbeneficial for tutee and tutor in song sparrows. Anim. Behav. 166, 281–288.\ndoi: 10.1016/j.anbehav.2020.05.015\nFrontiers in Psychology | www.frontiersin.org 13 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nBeecher, M. D., and Brenowitz, E. A. (2005). Functional aspects of song learning in\nsongbirds. Trends Ecol. Evol. 20, 143–149. doi: 10.1016/j.tree.2005.01.004\nBeecher, M. D., and Campbell, S. E. (2005). The role of unshared songs in singing\ninteractions between neighbouring song sparrows. Anim. Behav. 70, 1297–1304.\ndoi: 10.1016/j.anbehav.2005.03.008\nBeecher, M. D., Campbell, S. E., and Burt, J. M. (1994). Song perception in the\nsong sparrow: birds classify by song type but not by singer. Anim. Behav. 47,\n1343–1351. doi: 10.1006/anbe.1994.1182\nBeecher, M. D., Campbell, S. E., Burt, J. M., Hill, C. E., and Nordby, J. C. (2000).\nSong type matching between neighboring song sparrows. Anim. Behav. 59,\n21–27. doi: 10.1006/anbe.1999.1276\nBeecher, M. D., Nordby, J. C., Campbell, S. E., Burt, J. M., Hill, C. E., and\nO’Loghlen, A. L. (1997). “What is the function of song learning in songbirds?,”\nin Communication. Perspectives in Ethology, Vol. 12, eds D. H. Owings, M. D.\nBeecher, and N. S. Thompson (New York, NY: Plenum Press), 77–97. doi:\n10.1007/978-1-4899-1745-4_4\nBeecher, M. D., Stoddard, P. K., Campbell, S. E., and Horning, C. L. (1996).\nRepertoire matching between neighbouring song sparrows. Anim. Behav. 51,\n917–923. doi: 10.1006/anbe.1996.0095\nBoehm, C. (1999). Hierarchy in the Forest: The Evolution of Egalitarian Behavior.\nCambridge, MA: Harvard University Press.\nBowles, S. (2006). Group competition, reproductive leveling, and the evolution of\nhuman altruism. Science 314, 1569–1572. doi: 10.1126/science.1134829\nBradbury, J. W., and Vehrencamp, S. L. (1998). The Principles of Animal\nCommunication. Sunderland, MA: Sinauer Associates.\nBurt, J. M., Bard, S. C., Campbell, S. E., and Beecher, M. D. (2002). Alternative\nforms of song matching in song sparrows. Anim. Behav. 63, 1143–1151. doi:\n10.1006/anbe.2002.3011\nBurt, J. M., Campbell, S. E., and Beecher, M. D. (2001). Song type matching as\nthreat: a test using interactive playback. Anim. Behav. 62, 1163–1170. doi:\n10.1006/anbe.2001.1847\nByers, B. E. (1996). Messages encoded in the songs of chestnut-sided warblers.\nAnim. Behav. 52, 691–705. doi: 10.1006/anbe.1996.0214\nByers, J., Hebets, E., and Podos, J. (2010). Female mate choice based upon male\nmotor performance. Anim. Behav. 79, 771–778. doi: 10.1016/j.anbehav.2010.\n01.009\nCade, W. (1975). Acoustically orienting parasitoids: fly phonotaxis to cricket song.\nScience 190, 1312–1313. doi: 10.1126/science.190.4221.1312\nCarazo, P., and Font, E. (2010). Putting information back into biological\ncommunication. J. Evol. Biol. 23, 661–669. doi: 10.1111/j.1420-9101.2010.\n01944.x\nCaro, T. M., and Hauser, M. D. (1992). Is there teaching in nonhuman animals?\nQ. Rev. Biol. 67, 151–174.\nCarouso-Peck, S., and Goldstein, M. H. (2019). Female social feedback reveals\nnon-imitative mechanisms of vocal learning in zebra finches. Curr. Biol. 29,\n631–636.e633. doi: 10.1016/j.cub.2018.12.026\nCatchpole, C. K. (1987). Bird song, sexual selection and female choice. Trends Ecol.\nEvol. 2, 94–97. doi: 10.1016/0169-5347(87)90165-0\nChomsky, N. (1986). Knowledge of Language: Its Nature, Origin, and Use.\nNew York, NY: Praeger.\nClutton-Brock, T. H., and Albon, S. D. (1979). The roaring of red deer and\nthe evolution of honest advertisement. Behaviour 69, 145–170. doi: 10.1163/\n156853979X00449\nColeman, S. W., Patricelli, G. L., Coyle, B., Siani, J., and Borgia, G. (2007). Female\npreferences drive the evolution of mimetic accuracy in male sexual displays.\nBiol. Lett. 3, 463–466. doi: 10.1098/rsbl.2007.0234\nCollins, S. A. (2004). “Vocal fighting and flirting: the functions of birdsong,”\nin Nature’s Music: The Science of Birdsong, ed. P. M. H. Slabbekoorn\n(New York, NY: Academic Press), 39–79. doi: 10.1016/b978-012473070-0/\n50005-0\nDakin, R., and Montgomerie, R. (2011). Peahens prefer peacocks displaying more\neyespots, but rarely. Anim. Behav. 82, 21–28. doi: 10.1016/j.anbehav.2011.\n03.016\nDavies, N. B., and Halliday, T. R. (1978). Deep croaks and fighting assessment in\ntoads Bufo bufo. Nature 274, 683–685. doi: 10.1038/274683a0\nDawkins, R., and Krebs, J. R. (1978). “Animal Signals: information or\nmanipulation?,” in Behavioural Ecology: An Evolutionary Approach, eds J. R.\nKrebs and N. B. Davies (Oxford: Blackwell), 282–309.\nDoupe, A. J., and Kuhl, P. K. (1999). Birdsong and human speech: common themes\nand mechanisms. Annu. Rev. Neurosci. 22, 567–631. doi: 10.1146/annurev.\nneuro.22.1.567\nElias, D. O., Maddison, W. P., Peckmezian, C., Girard, M. B., and Mason,\nA. C. (2012). Orchestrating the score: complex multimodal courtship in\nthe Habronattus coecatus group of Habronattus jumping spiders (Araneae:\nSalticidae). Biol. J. Linn. Soc. 105, 522–547. doi: 10.1111/j.1095-8312.2011.\n01817.x\nFisher, J. B. (1954). “Evolution and bird sociality,” in Evolution as Process,\neds J. Huxley, A. C. Hardy, and E. B. Ford (London: Allen & Unwin),\n71–83.\nFitch, W. T. (2004). “Evolving honest communication systems: kin delection and\n‘mother tongues’,” in Evolution of Communication Systems: A Comparative\nApproach, eds D. K. Oller and U. Griebel (Cambridge, MA: MIT Press),\n275–296.\nFitch, W. T., and Jarvis, E. (2013). “Birdsong and other animal models for\nhuman speech, song, and vocal learning,” in Language, Music, and the Brain:\nA Mysterious Relationship, ed. M. A. Arbib (Cambridge, MA: MIT Press).\nGarland, E. C., and McGregor, P. K. (2020). Cultural transmission, evolution, and\nrevolution in vocal displays: insights from bird and whale song. Front. Psychol.\n11:544929. doi: 10.3389/fpsyg.2020.544929\nGat, A. (2015). Proving communal warfare among hunter-gatherers: the\nquasi-rousseauan error. Evol. Anthropol. 24, 111–126. doi: 10.1002/evan.\n21446\nGentner, T. Q., Fenn, K. M., Margoliash, D., and Nusbaum, H. C. (2006). Recursive\nsyntactic pattern learning by songbirds. Nature 440, 1204–1207. doi: 10.1038/\nnature04675\nGodfrey-Smith, P. (2013). “Information and influence in sender-receiver models,\nwith applications to animal behavior,” in Animal Communication Theory:\nInformation and Influence, ed. U. E. Stegmann (Cambridge: Cambridge\nUniversity Press).\nGould, J. L. (1975). Honey bee recruitment: the dance-language controversy.\nScience 189, 685–693. doi: 10.1126/science.1154023\nGrafen, A. (1990). Biological signals as handicaps. J. Theor. Biol. 144, 517–546.\ndoi: 10.1016/s0022-5193(05)80088-8\nGreen, S. (1975). “Variation of vocal pattern with social situation in the Japanese\nmacaque (Macaca fuscata): a field study,” in Primate Behavior, ed. L. A.\nRosenblum (New York, NY: Academic Press), 1–102. doi: 10.1016/b978-0-12-\n534004-5.50006-3\nGreen, S., and Marler, P. (1979). “The analysis of animal communication,” in Social\nBehavior and Communication, eds P. Marler and G. Vandenbergh (New York,\nNY: Plenum), 73–158. doi: 10.1007/978-1-4615-9116-0_3\nHaidt, J. (2012). The Righteous Mind: Why Good People are Divided by Politics and\nReligion. New York, NY: Pantheon.\nHauser, M. D. (2000). A primate dictionary? Decoding the function and meaning\nof another species’ vocalizations. Cogn. Sci. 24, 445–475. doi: 10.1207/\ns15516709cog2403_5\nHauser, M. D., Chomsky, N., and Fitch, W. T. (2002). The faculty of language:\nwhat is it, who has it, and how did it evolve? Science 298, 1569–1579. doi:\n10.1126/science.298.5598.1569\nHebets, E. A., and Uetz, G. W. (1999). Female responses to isolated\nsignals from multimodal male courtship displays in the wolf spider\ngenusSchizocosa(Araneae: Lycosidae). Anim. Behav. 57, 865–872. doi: 10.1006/\nanbe.1998.1048\nHenrich, J. (2016). The Secret of our Success: How Culture is Driving Human\nEvolution, Domesticating our Species, and Making us Smarter. Princeton, NJ:\nPrinceton University Press.\nHerman, L. M. (2010). What laboratory research has told us about dolphin\ncognition. Int. J. Comp. Psychol. 23, 310–330.\nHerrmann, E., Call, J., Hernandez-Lloreda, M. V., Hare, B., and Tomasello,\nM. (2007). Humans have evolved specialized skills of social cognition: the\ncultural intelligence hypothesis. Science 317, 1360–1366. doi: 10.1126/science.\n1146282\nHewes, G. W. (1973). Primate communication and the gestural origin of language.\nCurr. Anthrop. 14, 5–24.\nHill, C. E., Akçay, Ç, Campbell, S. E., and Beecher, M. D. (2011). Extrapair\npaternity, song and genetic quality in song sparrows. Behav. Ecol. 22, 73–81.\ndoi: 10.1093/beheco/arq171\nFrontiers in Psychology | www.frontiersin.org 14 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nHill, K., Walker, R. S., Božicevi ˇ c, M., Eder, J., Headland, T., Hewlett, B., et al. (2011). ´\nCo-residence patterns in hunter-gatherer societies show unique human social\nstructure. Science 331, 1286–1289. doi: 10.1126/science.1199071\nHockett, C. F. (1960). The origin of speech. Sci. Am. 203, 88–111.\nHorning, C. L., Beecher, M. D., Stoddard, P. K., and Campbell, S. E. (1993). Song\nperception in the song sparrow: importance of different parts of the song in song\ntype classification. Ethology 94, 46–58. doi: 10.1111/j.1439-0310.1993.tb00546.x\nHrdy, S. B. (2009). Mothers and Others: The Evolutionary Origins of Mutual\nUnderstanding. Cambridge, MA: Harvard University Press.\nJanik, V. M. (2014). Cetacean vocal learning and communication. Curr. Opin.\nNeurobiol. 28, 60–65. doi: 10.1016/j.conb.2014.06.010\nJohnstone, R. A. (1997). “The evolution of animal signals,” in Behavioural Ecology:\nAn Evolutionary Approach, eds J. Krebs and R. Davies (Oxford: Blackwell),\n155–178.\nKeagy, J., Savard, J.-F., and Borgia, G. (2009). Male satin bowerbird problemsolving ability predicts mating success. Anim. Behav. 78, 809–817. doi: 10.1016/\nj.anbehav.2009.07.011\nKelly, R. (2000). Warless Societies and the Origin of War. Ann Arbor: Uinversity of\nMichigan Press.\nKnight, C. (2018). “Pressure for trust-based efficiency shaped the evolution of\nlanguage,” in The Evolution of Language: Proceedings of the 12th International\nConference (EVOLANGXII), eds C. Christine, F. Molly, L. Hannah, M. Luke, R.\nAndrea, and V. Tessa.\nKnight, C., and Power, C. (2011). “Social conditions for the evolutionary emergence\nof language,” in Handbook of Language Evolution, ed. K. G. M. Tallerman\n(Oxford: Oxford University Press), 346–349.\nKnornschild, M. (2014). Vocal production learning in bats. Curr. Opin. Neurobiol.\n28, 80–85. doi: 10.1016/j.conb.2014.06.014\nKrebs, J. R. (1991). “Animal communication: ideas derived from Tinbergen’s\nactivities,” in The Tinbergen Legacy, eds M. D. Dawkins, T. R. Halliday, and\nR. Dawkins (London: Chapman & Hall), 60–74. doi: 10.1007/978-0-585-35\n156-8_5\nKrebs, J. R., and Davies, R. (1978). Behavioural Ecology: An Evolutionary Approach.\nOxford: Blackwell.\nKrebs, J. R., and Dawkins, R. (1984). “Animal signals: mind-reading and\nmanipulation,” in Behavioural Ecology: An Evolutionary Approach, 2nd Edn, eds\nJ. R. Krebs and R. Davies (Oxford: Blackwell), 380–402.\nKroodsma, D., Hamilton, D., Sánchez, J. E., Byers, B. E., Fandiño-Mariño, H.,\nStemple, D. W., et al. (2013). Behavioral evidence for song learning in the\nsuboscine bellbirds (Procnias spp.; Cotingidae). Wilson J. Ornithol. 125, 1–14.\ndoi: 10.1676/12-033.1\nKroodsma, D. E. (1977). Correlates of song organization among North American\nwrens. Am. Nat. 111, 995–1008. doi: 10.1086/283228\nKroodsma, D. E. (1988). “Contrasting styles of song development and their\nconsequences among passerine birds,” in Evolution and Learning, eds R. C.\nBolles and M. D. Beecher (Hillsdale, NJ: Lawrence Erlbaum Associates),\n157–184.\nLangmore, N. E. (1998). Functions of duet and solo songs of female birds. Trends\nEcol. Evol. 13, 136–140. doi: 10.1016/s0169-5347(97)01241-x\nLee, R. L. (2018). Hunter-gatherers and human evolution: new light on old\ndebates. Annu. Rev. Anthrop. 47, 513–531. doi: 10.1146/annurev-anthro102116-041448\nLevin, R. N. (1996). Song behaviour and reproductive strategies in a duetting\nwren, Thryothorus nigricapillus: II. Playback experiments. Anim. Behav. 52,\n1107–1117. doi: 10.1006/anbe.1996.0258\nLogue, D. M., and Gammon, D. E. (2004). Duet songs and sex roles during territory\ndefence in a tropical bird, the black-bellied wren. Anim. Behav. 68, 721–731.\ndoi: 10.1016/j.anbehav.2003.10.026\nLorenz, K. Z. (1966). The psychobiological approach: methods and results–\nEvolution of ritualization in the biological and cultural spheres. Philos. Trans.\nR. Soc. Lond. B Biol. Sci 251, 273–284. doi: 10.1098/rstb.1966.0011\nMacDougall-Shackleton, S. A. (1997). Sexual selection and the evolution of song\nrepertoires. Curr. Ornithol. 14, 81–124. doi: 10.1007/978-1-4757-9915-6_3\nMarler, P. (1961). The logical analysis of animal communication. J. Theor. Biol. 1,\n295–317. doi: 10.1016/0022-5193(61)90032-7\nMarler, P. (1968). “Visual systems,” in Animal Communication, ed. T. A. Sebeok\n(Bloomington, IN: Indiana University Press), 103–126.\nMarler, P. (1970). Birdsong and speech development: could there be parallels? Am.\nSci. 58, 669–673.\nMarler, P., and Pickert, R. (1984). Species-universal microstructure in the learned\nsong of the swamp sparrow (Melospiza georgiana). Anim. Behav. 32, 673–689.\ndoi: 10.1016/s0003-3472(84)80143-8\nMarler, P., and Tamura, M. (1964). Culturally transmitted patterns of vocal\nbehavior in sparrows. Science 146, 1483–1486. doi: 10.1126/science.146.3650.\n1483\nMaynard Smith, J., and Harper, D. (2003). Animal Signals. Oxford: Oxford\nUniversity Press.\nMcDonald, M. V. (1989). Function of song in Scott’s seaside sparrow. Anim. Behav.\n38, 468–485. doi: 10.1016/s0003-3472(89)80040-5\nMedvin, M. B., Stoddard, P. K., and Beecher, M. D. (1993). Signals for parentoffspring recognition: a comparative analsysis of the begging calls of cliff\nswallows and barn swallows. Anim. Behav. 45, 841–850. doi: 10.1006/anbe.1993.\n1105\nNowicki, S., and Searcy, W. A. (2014). The evolution of vocal learning. Curr. Opin.\nNeurobiol. 28, 48–53. doi: 10.1016/j.conb.2014.06.007\nOtte, D. (1974). Effects and functions in the evolution of signaling systems. Annu.\nRev. Ecol. Syst. 5, 385–417. doi: 10.1146/annurev.es.05.110174.002125\nOtter, K. A., Mckenna, A., LaZerte, S. E., and Ramsay, S. M. (2020). Continentwide shifts in song dialects of white-throated sparrows. Curr. Biol. 30, 1–5.\ndoi: 10.1016/j.cub.2020.05.084\nOwings, D. H., and Morton, E. S. (1997). “The role of information in\ncommunication: an assessment/management approach,” in Communication,\neds D. H. Owings, M. D. Beecher, and N. S. Thompson (New York, NY: Plenum\nPress), 359–390. doi: 10.1007/978-1-4899-1745-4_12\nOwings, D. H., and Morton, E. S. (1998). Animal Vocal Communication: A New\nApproach. Cambridge: Cambridge University Press.\nOwren, M. J., Rendall, D., and Ryan, M. J. (2010). Redefining animal signaling:\ninfluence versus information in communication. Biol. Philos. 25, 755–780. doi:\n10.1007/s10539-010-9224-4 Corpus ID: 13443399,\nPailian, H., Carey, S. E., Halberda, J., and Pepperberg, I. M. (2020). Age and\nspecies comparisons of visual mental manipulation ability as evidence for\nits development and evolution. Sci. Rep. 10:7689. doi: 10.1038/s41598-020-\n64666-1\nPenn, D. J., and Számadó, S. (2018). The handicap principle: how an erroneous\nhypothesis became a scientific principle. Biol. Rev. 95, 267–290. doi: 10.1111/\nbrv.12563\nPepperberg, I. M. (1981). Functional vocalizations by an African grey parrot\n(Psittacus erithacus). Z. Tierpsychol. 55, 139–160. doi: 10.1111/j.1439-0310.\n1981.tb01265.x\nPepperberg, I. M. (1987). Acquisition of the same/different concept by an African\ngrey parrot (Psittacus erithacus): learning with respect to categories of color,\nshape, and material. Anim. Learn. Behav. 15, 423–432. doi: 10.3758/bf032\n05051\nPetrie, M. (1994). Improved growth and survival of offspring of peacocks with more\nelaborate trains. Nature 371, 598–599. doi: 10.1038/371598a0\nPetrie, M., Halliday, T., and Sanders, C. (1991). Peahens prefer peacocks with\nelaborate trains. Anim. Behav. 41, 323–331. doi: 10.1016/s0003-3472(05)\n80484-1\nPetrie, M., and Halliday, T. R. (1994). Experimental and natural changes in the\npeacock’s (Pavo cristatus) train can affect mating success. Behav. Ecol. Sociobiol.\n35, 213–217. doi: 10.1007/bf00167962\nPinker, S., and Jackendoff, R. (2005). The faculty of language: what’s special about\nit? Cognition 95, 201–236. doi: 10.1016/j.cognition.2004.08.004\nPrice, T., Wadewitz, P., Cheney, D., Seyfarth, R., Hammerschmidt, K., and Fischer,\nJ. (2015). Vervets revisited: a quantitative analysis of alarm call structure and\ncontext specificity. Sci. Rep. 5:13220. doi: 10.1038/srep13220\nReid, J. M., Arcese, P., Cassidy, A. L. E. V., Hiebert, S. M., Smith, J. N. M.,\nStoddard, P. K., et al. (2004). Song repertoire size predicts initial mating success\nin male song sparrows, Melospiza melodia. Anim. Behav. 68, 1055–1063. doi:\n10.1016/j.anbehav.2004.07.003\nReid, J. M., Arcese, P., Cassidy, A. L. E. V., Hiebert, S. M., Smith, J. N. M., Stoddard,\nP. K., et al. (2005). Fitness correlates of song repertoire size in free-living\nsong sparrows (Melospiza melodia). Am. Nat. 165, 299–310. doi: 10.2307/34\n73407\nFrontiers in Psychology | www.frontiersin.org 15 March 2021 | Volume 12 | Article 602635\nBeecher Animal Communication and Simple Language\nRendall, D., Owren, M. J., and Ryan, M. J. (2009). What do animal signals mean?\nAnim. Behav. 78, 233–240. doi: 10.1016/j.anbehav.2009.06.007\nRice, J. O., and Thompson, W. L. (1968). Song development in the indigo bunting.\nAnim. Behav. 16, 462–469. doi: 10.1016/0003-3472(68)90041-9\nRyan, M. J., and Rand, A. S. (1990). The sensory basis of sexual selection for\ncomplex calls in the Túngara frog, Physalaemus pustulosus (sexual selection for\nsensory exploitation). Evolution 44, 305–314. doi: 10.1111/j.1558-5646.1990.\ntb05200.x\nSavage-Rumbaugh, E. S., Murphy, J., Sevcik, R. A., Brakke, K. E., Williams, S. L.,\nand Rumbaugh, D. M. (1993). Language comprehension in ape and child.\nMonogr. Soc. Res. Child Dev. 58, 1–222.\nSchürch, R., and Ratnieks, F. W. (2015). The spatial information content of the\nhoney bee waggle dance. Front. Ecol. Evol. 3:22. doi: 10.3389/fevo.2015.00022\nScott-Phillips, T. C. (2008). Defining biological communication. J. Evol. Biol. 21,\n387–395. doi: 10.1111/j.1420-9101.2007.01497.x\nSearcy, W. A. (1984). Song repertoire size and female preferences in song sparrows.\nBehav. Ecol. Sociobiol. 14, 281–286. doi: 10.1007/bf00299499\nSearcy, W. A., Akçay, Ç, Nowicki, S., and Beecher, M. D. (2014). Aggressive\nsignaling in song sparrows and other songbirds. Adv. Study Behav. 46, 89–125.\ndoi: 10.1016/b978-0-12-800286-5.00003-1\nSearcy, W. A., and Beecher, M. D. (2009). Song as an aggressive signal in songbirds.\nAnim. Behav. 78, 1281–1292. doi: 10.1016/j.anbehav.2009.08.011\nSearcy, W. A., and Nowicki, S. (2005). The Evolution of Animal Communication.\nPrinceton, NJ: Princeton University Press.\nSearcy, W. A., and Yasukawa, K. (1996). “Song and female choice,” in Ecology and\nEvolution of Acoustic Communication in Birds, eds D. E. M. Kroodsma and H.\nEdward (London: Cornell University Press), 454–473.\nSeyfarth, R. M., Cheney, D. L., Bergman, T., Fischer, J., Zuberbühler, K., and\nHammerschmidt, K. (2010). The central importance of information in studies\nof animal communication. Anim. Behav. 80, 3–8. doi: 10.1016/j.anbehav.2010.\n04.012\nSeyfarth, R. M., Cheney, D. L., and Marler, P. (1990). Monkey responses\nto three different alarm calls: evidence of predator classification and\nsemantic communication. Science 210, 801–803. doi: 10.1126/science.74\n33999\nSmith, W. J. (1997). “The behavior of communicating, after twenty years,” in\nCommunication, eds D. H. Owings, M. D. Beecher, and N. S. Thompson\n(New York, NY: Plenum), 7–53. doi: 10.1007/978-1-4899-1745-4_2\nStein, A. C., and Uy, J. A. C. (2006). Plumage brightness predicts male mating\nsuccess in the lekking golden-collared manakin, Manacus vitellinus. Behav. Ecol.\n17, 41–47. doi: 10.1093/beheco/ari095\nSterelny, K. (2012). The Evolved Apprentice: How Evolution Made Humans Unique.\nCambridge, MA: The MIT Press.\nStoddard, P. K., and Beecher, M. D. (1983). Parental recognition of offspring in the\ncliff swallow. Auk 100, 795–799. doi: 10.1093/auk/100.4.795\nStoeger, A., and Manger, P. (2014). Vocal learning in elephants: neural bases and\nadaptive context. Curr. Opin. Neurobiol. 28, 101–107. doi: 10.1016/j.conb.2014.\n07.001\nStruhsaker, T. T. (1967). “auditory communication among vervet monkeys\n(Cercopithecus aethiops),” in Social Communication Among Primates, ed. S. A.\nAltmann (Chicago, IL: Chicago University Press), 281–324.\nStuddert-Kennedy, M. (1998). “The particulate origins of language generativity:\nfrom syllable to gesture,” in Approaches to the Evolution of Language: Social\nand Cognitive Bases, eds J. Hurford, M. Studdert-Kennedy, and C. Knight\n(Cambridge: Cambridge University Press), 202–221.\nTakahashi, M., Arita, H., Hiraiwa-Hasegawa, M., and Hasegawa, T. (2008). Peahens\ndo not prefer peacocks with more elaborate trains. Anim. Behav. 75, 1209–1219.\ndoi: 10.1016/j.anbehav.2007.10.004\nTempleton, C. N., Akçay, Ç, Campbell, S. E., and Beecher, M. D. (2012). Soft song\nis a reliable signal of aggressive intent in song sparrows. Behav. Ecol. Sociobiol.\n66, 1503–1509. doi: 10.1007/s00265-012-1405-5\nThompson, W. L. (1969). Song recognition by territorial male buntings (Passerin\nA). Anim. Behav. 17, 658–663. doi: 10.1016/S0003-3472(69)80008-4\nThompson, W. L. (1970). Song variation in a population of indigo buntings. Auk\n87, 58–71. doi: 10.2307/4083658\nTinbergen, N. (1952). Derived activities: their causation, biological significance,\norigin and emancipation during evolution. Q. Rev. Biol. 27, 1–32. doi: 10.1086/\n398642\nTinbergen, N. (1964). “The evolution of signaling devices,” in Social Behavior and\nEvolution Among Vertebrates, ed. W. Etkin (Chicago, IL: University of Chicago\nPress), 206–230.\nTomasello, M., Carpenter, M., Call, J., Behne, T., and Moll, H. (2005).\nUnderstanding and sharing intentions: the origins of cultural cognition. Behav.\nBrain Sci. 28, 675–691. doi: 10.1017/s0140525x05000129\nTomasello, M., Melis, A. P., Tennie, C., Wyman, E., and Herrmann, E. (2012).\nTwo key steps in the evolution of human cooperation: the interdependence\nhypothesis. Curr. Anthropol. 53, 673–692. doi: 10.1086/668207\nvan Heijningen, C. A. A., de Visser, J., Zuidema, W., and ten Cate, C. (2009). Simple\nrules can explain discrimination of putative recursive syntactic structures by a\nsongbird species. Proc. Natl. Acad. Sci. U.S.A. 106, 20538. doi: 10.1073/pnas.\n0908113106\nWacewicz, S., and Zywiczy ˙ nski, P. (2015). Language evolution: why Hockett’s ´\ndesign features are a non-starter. Biosemiotics 8, 29–46. doi: 10.1007/s12304-\n014-9203-2\nWalcott, C., Mager, J. N., and Piper, W. (2006). Changing territories, changing\ntunes: male loons, Gavia immer, change their vocalizations when they change\nterritories. r Anim. Behav. 71, 673–683. doi: 10.1016/j.anbehav.2005.07.011\nWhiten, A. (2017). Social learning and culture in child and chimpanzee. Annu. Rev.\nPsychol. 68, 129–154. doi: 10.1146/annurev-psych-010416-044108\nWiley, R. H. (2013). “Communication as a transfer of information: measurement,\nmechanism, and meaning,” in Animal Communication Theory: Information and\nInfluence, ed. U. Stegmann (Cambridge: Cambridge University Press), 113–129.\ndoi: 10.1017/cbo9781139003551.007\nWilliams, G. C. (1966). Adaptation and Natural Selection. Princeton, NJ: Princeton\nUniversity Press.\nWilson, E. O. (1975). Sociobiology. Cambridge, MA: Harvard University Press.\nWrangham, R. W. (2018). Two types of aggression in human evolution. Proc. Natl.\nAcad. Sci. U.S.A. 115, 245–253. doi: 10.1073/pnas.1713611115\nWrangham, R. W. (2019). Hypotheses for the evolution of reduced reactive\naggression in the context of human self-domestication. Front. Psychol. 10:1914.\ndoi: 10.3389/fpsyg.2019.01914\nWright, T. F., and Dahlin, C. R. (2018). Vocal dialects in parrots: patterns and\nprocesses of cultural evolution. Emu 118, 50–66. doi: 10.1080/01584197.2017.\n1379356\nZahavi, A. (1975). Mate selection- a selection for handicap. J. Theor. Biol. 53,\n205–214. doi: 10.1016/0022-5193(75)90111-3\nZuidema, W., and de Boer, B. (2009). The evolution of combinatorial phonology.\nJ. Phon. 37, 125–144. doi: 10.1016/j.wocn.2008.10.003\nZuk, M., Rotenberry, J. T., and Tinghitella, R. M. (2006). Silent night: adaptive\ndisappearance of a sexual signal in a parasitized population of field crickets.\nBiol. Lett. 2, 521–524. doi: 10.1098/rsbl.2006.0539\nConflict of Interest: The author declares that the research was conducted in the\nabsence of any commercial or financial relationships that could be construed as a\npotential conflict of interest.\nCopyright © 2021 Beecher. This is an open-access article distributed under the terms\nof the Creative Commons Attribution License (CC BY). The use, distribution or\nreproduction in other forums is permitted, provided the original author(s) and the\ncopyright owner(s) are credited and that the original publication in this journal\nis cited, in accordance with accepted academic practice. No use, distribution or\nreproduction is permitted which does not comply with these terms.\nFrontiers in Psychology | www.frontiersin.org 16 March 2021 | Volume 12 | Article 602635", "affiliations": [{"university": "University of Washington", "country": "United States", "discipline": "Biology"}, {"university": "University of Washington", "country": "United States", "discipline": "Psychology"}], "species_categories": ["Bird", "Terrestrial Mammal"], "specialized_species": ["songbirds", "parrots", "dolphins", "vervet monkeys"], "computational_stages": ["Data Collection", "Pre-processing", "Sequence Representation", "Meaning Identification"], "linguistic_features": ["Specialization", "Semanticity", "Learnability", "Cultural Transmission"], "status": "saved", "created_at": "2026-01-13T12:49:59.882680", "updated_at": "2026-01-13T13:59:15.072000", "committed_at": "2026-01-13T13:59:17.790612"}
{"id": "1523cda7-9791-41ca-83c3-d189b767235a", "title": "Dialects in Japanese Monkeys: Vocal Learning and Cultural Transmission of Locale-specific Vocal Behavior?", "authors": ["Green,  Steven"], "year": "1975", "journal": "Zeitschrift f\\", "abstract": "", "doi": "10.1111/j.1439-0310.1975.tb02006.x", "analysis_notes": "Differences were detected by ear in vocalizations made during artificial feeding of Japanese monkey troops at three locations. Tape recording and sound spectrographic analysis confirmed a distinctive vocal pattern specific to each site and used only in the provisioning situation. The 3 different acoustic morphologies are variations on a shared tonal theme. Vocal learning by Macaca fuscata may have occurred separately at each site regulated by species-wide constraints on vocal production.\n\n", "affiliations": [{"university": "The Rockefeller University", "country": "United States", "discipline": "Biology"}], "species_categories": ["Primate"], "specialized_species": ["Japanese monkey"], "computational_stages": ["Data Collection", "Pre-processing", "Meaning Identification"], "linguistic_features": ["Vocal Auditory Channel and Turn-taking", "Semanticity", "Specialization", "Tradition and Cultural Transmission"], "status": "saved", "created_at": "2026-01-13T12:49:59.882685", "updated_at": "2026-01-13T14:01:18.820388", "committed_at": "2026-01-13T14:01:24.790310"}
{"id": "c65da50d-b901-4513-aa6a-bdd11d7a7cf7", "title": "Do common ravens yell because they want to attract others?", "authors": ["Heinrich,  B.", "Marzluff,  J.M."], "year": "1991", "journal": "Behavioral Ecology and Sociobiology", "abstract": "", "doi": "10.1007/bf00172134", "analysis_notes": "Behav Ecol Sociobiol (1991) 28:13-21 Behavioral Ecology\nand Sociobiology\n© Springer-Verlag 1991\nDo common ravens yell because they want to attract others ?\nB. Heinrich and J.M. Marzluff\nDepartment of Zoology, University of Vermont, Burlington, VT 05405, USA\nReceived January 24, 1990 / Accepted July 28, 1990\nSummary. The formation of groups at food bonanzas\nresults from a variety of mechanism, which include recruitment by signalling and information parasitism. Recruitment is distinguished from information parasitism\non functional grounds: attraction of a crowd is termed\nrecruitment if the signaler's fitness is enhanced by the\nattraction of others but termed parasitism if the signaler's fitness is reduced by the attraction of others. We\nhere show, however, that in Common Ravens, Corvus\ncorax, the proximate reasons for giving recruitment signals are probably other than for attracting a crowd. In\nthe forests of the northeastern United States, non-breeding, vagrant ravens commonly aggregate in large\nnumbers at carcasses where they neutralize the defense\nof territorial adults. We attempted to mimic this situation with a captive flock of juveniles and a pair of resident adults in order to determine the proximate factors\ntriggering \"yells\", vocalizations which attract nearby ravens to large animal carcasses. Our experiments indicate\nthat yells are given primarily by hungry birds. However,\nyelling is strongly modified by status. Within the vagrant\ncrowd, status is labile. When successive dominants were\nremoved, replacements immediately took their place.\nFurthermore, when the dominants were re-introduced\nto the flock they always suffered significant losses of\nstatus and ceased yelling. The territorial male has, and\nconstantly maintains, the highest status within (but not\nnecessarily outside) his territory, and here he rarely yells.\nIn sharp contrast, within the vagrant crowd of unmated\nbirds it is the highest-status birds that are the most likely\nto yell when approaching food. Furthermore, the dominant vagrants (as well as adults) suppress yelling in subordinates. We conclude that ravens yell proximately to\nadvertise their status at food, and that recruitment is\nonly one of several ultimate advantages of the behavior.\nIntroduction\nMany species of animals aggregate in large numbers at\nlocalized food bonanzas. Groups can be assembled by\nOffprint request to .' B. Heinrich\na wide varietiy of mechanisms. For example, on the\nplains of Africa thousands of beetles are attracted to\na single dung pile where they compete intensely (Heinrich and Bartholomew 1978). They are presumably attracted by the smell of the dung itself. Vultures are attracted to carcasses by seeing others spiralling down to\nthem (K6nig 1983). A food bonanza may also be discovered by a group, that then feeds as a group (Balda\nand Bateman 1971). In addition, and perhaps most interestingly, animals may be attracted by signals given by\nthose discovering food. Specific attraction signals include the \"chirrup\" calls of House sparrows (Elgar\n1986), the \"whinny\" calls of spider monkeys (Chapman\nand Lefebvre 1990), the undulating flights of ospreys\n(Greene 1987), and the \"yells\" of ravens (Heinrich 1988,\n1989). Sometimes unsuccessful foragers also follow successful ones using subtle and largely unknown cues\n(Ward and Zhahavi 1973; Rabenold 1987 a; Krebs 1974;\nBrown 1986). In the latter cases information may be\nwithheld, and/or following may be suppressed (Waltz\n1983; Rabenold 1987b).\nWhether or not recruitment or information parasitism occurs is often a controversial topic, because to\nmany it raises the question of whether the signallers try\nto call in others or whether instead they are being exploited. But part of the controversy is artificial because\nthe distinction between proximate and ultimate causes\nof attraction signals have commonly not been made.\nMuch confusion has arisen because it is generally assumed that the two coincide. In other words, it is usually\nassumed that when animals recruit they give signals\n\"to\" attract others. Likewise, if information is parasitized it is usually assumed that the signals are not given\n\"to\" attract others. It is not necessary to impart volition\nto animals giving recruitment signals and then use this\nas the basis for distinguishing recruitment from parasitism. Attraction of a crowd can yield a great variety of\ndifferent costs and benefits. But the balance must be\npositive for attraction signals to evolve. Recruitment is\ndistinguished from parasitism on this purely functional\nground by investigating the ultimate consequences of\ngroup formation. Recruitment results when assembled\ngroups increase the signaler's fitness. Parasitism results\nwhen assembled groups decrease the signaler's fitness. \n14\nBy removing volition from the equation of whether\nor not recruitment occurs, we are faced with a second\nquestion: What is/are the proximate reason(s) for giving\nsignal(s) that result in recruitment (or parasitism)? Presumably any of a number of signals originally used and\nperhaps still functional for other purposes can be used\nand even modified to function as recruitment signals.\nTherefore, the proximate reason a recruitment signal is\ngiven is not necessarily to attract others. Recruitment\nsignals evolve because they enhance fitness and it is not\nnecessary for a signaler to realize this function. Only\nwhen the proximate reasons for giving recruitment signals include the attraction of others is the signaler behaving in an apparently purposeful manner that previous\nworkers implied was necessary for one to use the term\n\"recruitment.\" Costa Rican spider monkeys provide a\npossible example of recruitment where proximate and\nultimate reasons for signaling coincide. These monkeys\nadjust their group size to match resource availability\nby uttering \"whinny\" calls when groups are small relative to resource abundance and by withholding calls\nwhen enlarged group size results in heightened competition (Chapman and Lefebvre 1990). In the above and\nmany other cases of documented recruitment, scenarios\nof kin selection or reciprocity could explain the attraction of others, where the animals may proximately\n\"want\" to recruit. However, as discussed elsewhere\n(Heinrich 1988, 1989), this ultimate adaptive advantage\nis unlikely for ravens because of their vagrancy.\nHere we report on an experimental study designed\nto decipher the proximate factors eliciting the \"yell\"\nvocalization of Common Ravens (Corvus corax). This\nvocalization is given by immature vagrants near food\nwhich attracts other nearby vagrants (Heinrich 1988).\nElsewhere we investigated the ultimate function of group\nformation and concluded that it increases the signaler's\nfitness because crowds are able to overpower territorial\nadults and access defended foods unavailable to single\nravens (Heinrich 1988, 1989; Marzluff and Heinrich in\nprep.). Given our functional definitions, group formation by attraction to yells therefore constitutes recruitment, not parasitism. The results reported herein allow\nus to determine the motivation of the signaler thereby\ngaining a broader understanding of recruitment in this\nspecies.\nMethods\nApparatus and subjects. On 29 and 31 December 1988 we captured\n20 immature ravens (6 yearlings and 14 juveniles) and placed them\nin the main aviary of our aviary complex (Fig. 1). These immatures\nwere part of a group of approximately 50 ravens that were foraging\nat a dead cow in the mountains of western Maine within 20 meters\nof the aviary. Birds were captured in a walk-in trap, aged by mouth\nand plumage coloration, and marked with uniquely numbered and\ncolored patagial tags on both wings (see Heinrich, 1988 for details\nof the study area and capture and marking techniques). Laparotomies on December 8, 1989, indicated a sex ratio of 6 c?/14%\nImmatures quickly adapted to their new surroundings. They\nroosted as a communal group in a covered shed, fed as a group\nat liberally supplied carcasses and slaughter house offal (birds were\nf R'OO~T ~ ~\nAWAB¥ )\n, x ~xi x\"\nga\n/ OBBERVATION HUT :=: ;..\\\n?-k\n[ ) TERRITORIAL\n~ ADULTS\nFig. 1. Aviary complex where experiments were conducted. Twenty\nimmatures resided in the main aviary and a pair of adults defended\n1 peripheral aviary. The entire complex is interconnected and arms\ncan be opened or shut off by raising or lowering gates with guy\nwires operated from inside the observation hut. Due to terrain\n(hut is at the apex of a knoll) and vegetation (a spruce thicket\nlies between 2 peripheral aviaries) birds in the peripheral aviaries\ncan only see the lower quarter of the main aviary and vice versa.\nX's mark locations of food during experiments. The aviary complex\nis from 4-7 m in height and contains 50 vertical perches and 26\nhorizontal ones scattered throughout\nnever without food for more than 3 days during the first 3 months),\nbathed in the snow, allopreened and fought. Qualitatively, their\nbehavior was identical to the behavior of free-living immatures\nwe continued to monitor.\nOn 2 January we captured 3 adults and placed them in one\nof the peripheral aviaries of the complex (Fig. 1). 2 of these adults\nallopreened regularly, called in synchrony, and mirrored each\nother's actions suggesting to us (and another experienced with ravens, E. Gwinner) that they were an established pair. The third\nbird was released after 2 days of ostracism by the \"pair\". The\npair was fed ad libitum in their aviary and quickly began to defend\nits boundaries from free-living wild birds and from juveniles wandering in the arm between the main aviary and the adult aviary.\nThe adults asserted their dominant status by giving bowing ceremonies, thick-head postures, and ear-tuft intimidation displays (Gwinher 1964; Heinrich 1988) to any intruders.\nWe allowed our captive immatures to find food bonanzas\n(hunks of meat and carcasses ranging in size from squirrels to\ndeer) randomly located throughout the aviary complex for 3\nmonths prior to the experiment we discuss here. During this time\nwe determined dominance-subordination relationships by ad libirum observations of dyads during foraging (Altman 1974). The\ndominant of an interaction was the bird that forced the other to\nback away from confrontation in the fuzzy-headed submission posture (Heinrich 1988).\nA stable dominance hierarchy quickly developed among our\ncaptive immatures. It was clear from the first few days of captivity\nthat a few immatures consistently dominated all the other birds.\nOver a 3 month period (30 December-16 March) a hierarchy developed which we have broken into 4 categories: 1) 3 dominants who\nrarely deferred to others, 2) 6 subdominants who deferred to a\nminority of birds primarily dominants and other subdominants,\n3) 5 intermediates who won roughly 40% of their encounters, and\n4) 6 subordinates who rarely defeated any other bird. There was\nan obvious alpha male (RB) who only lost 2 encounters (both\nto the beta male) and had uncontested access to food at any time.\nOur method of determining the dominance hierarchy minimizes\nthe number of times individuals lose to lower ranking birds (Appleby 1983), however some nonlinearity was evident as one subordinate consistently defeated an intermediate and a lower ranking \n15\n0 40'\nZ\n~ 35- Z ..j\n~ ~ ~o.\n~ ~ 25- .J\n20.\n>-~m ~5\"\nL~ IL\n0 0 10-\nn,- n\"\nuJ uJ 5-\n~ rn\n:~ ~ 0-\n~ ~)\nz Z\nmmYELLS\n~;3 YELLERS\nZERO TWO FOUR\nDAYS WITHOUT FOOD\nFig. 2. The influence of increasing hunger on the number of yellers\nand yelling rates by immatures. Height of boxes indicate means.\nError bats are 1 SEM. Means are derived from the total response\nof the group to each of 5 food locations during each of 8 group\ncompositions (N=40 for 2 and 4 days without food, N=39 for\nzero days without food because of one missing observation)\nsubdominant defeated a higher ranking one. Despite the circular\ntriads created by these reversals, Appleby's (I 983) method indicates\nthat this hierarchy was significantly linear (linearity coefficient,\nK=0.93, X2 = 157, df= 27, P<0.001).\nExperimentalprotocol. From 23 March until 6 July 1989 we systematically varied composition of the immature group, the location\nof food, and the hunger level of immatures in order to determine\nhow these three factors influenced yelling. Two observers watched\nbirds thyough two-way mirrors with 10-power binoculars from an\nobservation hut 10 meters from the main aviary (Fig. 2). We recorded the identity of yellers and their rate of yelling (number\nof yells in randomly selected i rain intervals), who initially approached and contacted food, how long before food was contacted\nand consumed, and whether food was defended. Vocalizations were\nrecorded on a Sony TCM-5000 cassette recorder using a Senheiser\nME-88 microphone.\nWe could not accurately count yelling rates for all birds in\nan experiment. However, we could easily and unambiguously assign birds to primary or secondary yelling status. In an experiment\nthere were usually 1 or 2 primary yellers who yelled nearly constantly and maintained consistently high rates of yelling throughout\na majority of phases (different locations of food) of the experiment.\nPrimary yellers accounted for a majority of the yells uttered. However, in most experiments 3-10 secondary yellers yelled 1 or a\nfew times.\nWe employed a hierarchical experimental design to test the\ninfluence of the 3 factors. Group composition was the main blocking factor and consisted of 4 independent levels. Hunger was a\nthree-level repeated measure nested within each group. Location\nwas a five-level repeated measure within each hunger level. Each\ngroup was replicated twice resulting in 8 experiments. Each experiment consisted of 3 runs on 3 separate days (one for each hunger\nlevel) and on each day responses were measured in 5 phases (one\nfor each food location). One phase was omitted from analysis\n(\" Split Group\" when birds were satiated and the alpha was removed) because birds could not be segregated in the aviary arm.\nGroup composition was modified by removing dominant birds\nfrom the immature group. The first group composition included\nall birds. Our first manipulation was to remove the alpha male\nand his consort. After 7-10 days of allowing the remaining immatures to establish a dominance hierarchy (all birds simply shifted\nup one position) we tested this new group and then removed its\nalpha male. Again 1 week was allowed for reshuffling (all shifted\none position), birds were retested, and the alpha male removed.\nThus by removing three alpha males in succession we created 4\ngroup compositions. The response of birds in each composition\nwas replicated after a 14 day ad libitum feeding period by reintroducing each male in the reverse order from the order of removal.\nHunger level was repeated within each replicate of group composition. We modified hunger level by removing all food from\nsatiated birds for varying lengths of time. Responses of satiated\nbirds in each group were measured after they had fed ad libitum\nfor 2-6 days and still had food left on the day of the test. This\nresponse was contrasted to the response of birds on the second\nand fourth day without food. The order of application of hunger\nlevel to groups was randomized. In order to reduce accumulation\nof carryover effects between hunger treatments on different groups\nwe repeatedly satiated the birds during the 7-10 day period of\nreshuffling after an alpha male was removed.\nFood (5-15 kg hunks of meat) location was a second repeated\nmeasure because the immatures were exposed to each food location\non every day of the experiment. Each day we measured the calling\nresponse of immatures under 5 conditions. 1) Baseline = Before we\nplaced food in the aviary. 2) Group Approach = As immatures approached freely accessible food in the main aviary or just inside\nthe arm leading to the adult aviary. 3) Behind Screen=As they\napproached inaccessible food behind a lowered screen door separating the main aviary from the arm. 4) With Adults=As they\napproached inaccessible food in the adults' aviary behind a lowered\nscreen door. 5) Split Group=We captured approximately half of\nthe immatures in the arm of the complex by closing the screen\ndoor as the group began to enter. The rest of the group remained\nin the main aviary. Our intent was to separate the alpha male\nfrom the beta male. Food was placed in the main aviary or in\nthe arm so that ~[ group could get to it but the other group could\nonly see the food. After the group with access approached and\nbegan to eat, the meat was removed and placed with the other\ngroup until they approached and began to eat. Behaviors of all\nbirds as either group approached constituted the response for this\ntreatment.\nAdditional observations, as indicated, were made on birds in\nthe wild in the study area near the aviary, and on hand-reared\nbirds.\nResults\nOverall ANO VA\nHunger and location of food significantly influenced\nyelling rate and the number of birds yelling (Table 1).\nImmatures yelled more frequently as their level of\nhunger increased (Fig. 2). Satiated birds rarely yelled.\nOnly 50% of experiments with satiated birds produced\nany yelling. In contrast, after 4 days without food all\nexperiments had yellers, over a third of the group yelled,\nand many of the birds yelled continuously. The rate of\nyelling increased 16-fold every 2 days immatures were\nwithout food. As soon as foraging began, even after\n4 days without food, yelling subsided and was typically\nextinguished 10 min after the onset of eating. The major\ninfluence of food location on yelling was a decline in\nthe number of yellers and their rate of yelling when food\nwas located in the adults' aviary (Fig. 3). This food was\napproximately 0.5 m from the screen partition and when\nimmatures approached it the adults flew up to meet them\nwith ear-tuft intimidation displays and harsh calls that\nare often given by free-living adults when we trespass\nin their territories and when they fly over our captive\nadults. Yelling was most frequent when some or all birds\nhad access to the food and adults were out of view (Split\nApproach and Group Approach). However, inaccessible \n16\nT~ble 1. Analysis of variance results for the influence of 3 factors on yelling rate and\nthe number of yellers in an experiment\nYells/min Number of birds yelling\nF DF P F DF P\nMain effects\nHunger 16.5 2.8 0.001 18.5 2.8 0.001\nPlacement of food 4.8 4,16 0.01 7.9 4,16 0.001\nGroup composition 0.1 3,4 0.98 0.3 3,4 0.80\nInteractions\nHunger x Group 0.3 6,8 0.90 0.2 6,8 0.96\nHunger x Placement 4.5 8,32 0.001 3.8 8.32 0.003\nGroup x Placement 0.2 12,16 0.99 0.3 12,16 0.96\nGroup × Hunger\nx Placement 0.9 24,32 0.57 0.7 24,32 0.80\nSATIATED\n0 o 0\nZ 2 DAYS WITHOUT FOOD 3 25 2 DAYS WITHOUT FOOD\nz 15\n~o ~ 20. _a n-\n.a~ ~-~ ~ lo- ,,o\nO 8 O'\n8~ o ~-~, 50- 4 s u OD 04o. iioiii0 I~1~ ~Z\n~ ~ 30-\n<\n< 20-\n10-\n0 .%=¢,, ~,~,~¢~ .~,% ~ .¢ _~\n• ~.~ ~'4 ......... ~? ~ ~ t,9~,,~ \"'~Ol..~',q -~'O/~'Z~ \"~O,V;~,~]~ _ ~'V ~,~'~ V~ -- ~'~ .~ ~ ~, ,:,~ ~,~%,,%~,~ ~'#\n~'~\nPLACEMENT OF FOOD GROUP COMPOSITION ~/\nFig. 3. The influence of food placement on yelling by immatures.\nFood placements are defined in Methods. Average response+l\nSEM are indicated at 3 hunger levels. Total responses of birds\nat each location for each level of hunger are averaged over N= 8\ngroup compositions (N= 7 for split group, satiated because of missing observation)\nFig. 4. Influence of group composition on yelling by immatures.\n(Responses of immatures are averaged over 5 food locations during\n2 replicates of group composition to give N= 10 for each group\nat each of 3 hunger levels N= 9 for treatment with alpha removed\nand immatures satiated due to missing data). Means ÷ 1 SEM are\nshown\nfood also elicited frequent yelling when adults were not\npresent (compare Behind Screen to With Adults).\nHunger and placement of food had a significant interaction effect (Table 1). As hunger increased, yelling\nincreased, in all locations. However, the relative reduction in yelling when food was placed with adults became\nmore noticable as hunger increased (Fig. 3). After 4 days\nwithout food, yelling was even triggered by the sight\nof the observers entering the observation hut without\nplacing food in the aviary. (We suspect the birds were\nyelling in response to the expectation of food, inasmuch\nas fledged young yell when they see their parents approach.)\nGroup composition was not significantly related to\nyelling (Table 1, Fig. 4). However, dominance did influence yelling and hunger mediated this influence (Fig. 5). \nZ\n~ 100\n~ 80'\n,\n0 60 ~\n~ 40 ¢\nz 20\n~ 0\nSATIATED\n-I -] -\nI\n~ =\nI,I\n,\n-- i O~ ~\n0 •\n:~ 0\no ~'\nB C\n2 DAYS 4 DAYS\nWITHOUT FOOD WITHOUT FOOD\n-- ~\n~ K ~ ~\nFig. 5. Change in yelling and changing composition of yellers (with\nrespect to dominance) as a function of hunger. The percentage\nof all birds in a given class that yelled or were silent at 3 hunger\nlevels are shown. Percentages of yellers in each status class were\ndetermined by summing the number of yellers per dominance class\nacross the 8 tests of group composition and dividing this sum\nby the total birds in each class summed across the 8 experiments.\nChanges in status through the course of experimentation were accounted for which resulted in a total of 144 subordinates, 120\nintermediates, 114 subdominants, and 66 dominants across the 8\nexperiments. Filled bars primary yeller; hatched bars secondary\nyeller; open bars did not yell in experiment\nDuring experiments with satiated birds there were few\nprimary yellers and these were usually low ranking birds\nand never dominants (Fig. 5 A). The few dominants that\ndid yell did so only occasionally. As hunger increased\nnearly all dominants yelled and after four days without\nfood most primary yellers were high ranking birds\n(Fig. 5 B, C).\nDominant immatures suppress yelling by subordinates\nDuring the first replicate of the group composition manipulations subordinates yelled more frequently as dominants were removed (Fig. 6). When the alpha was removed, 4 new birds yelled for the first time including\nthe new dominant who did not yell in experiment 1 and\n2 intermediates who, after the alpha's removal, had risen\nin status. After removing the beta in addition to the\nalpha, 6 additional birds yelled for the first time. These\nwere primarily subordinates. Lastly, after the gamma\nwas removed still 1 more subordinate yelled. In total,\n19 of 20 birds yelled, but over half of these did not\nyell until 1 or more of their superiors were removed\nfrom the group. One subordinate never yelled.\nIt appears that alpha birds actively suppress the yelling of other dominants. A dramatic example of suppression occurred in experiment 2. The birds headed down\nthe arm toward the adults' aviary and the beta male\nyelled 8 times during the first minute. The alpha, who\nhad been silent, then attacked the beta, pinned him to\nthe snow and jabbed him with his bill. For the remainder\nof the experiment the beta was silent and the alpha yelled\nan average of 18.4 times/rain. More typically, suppression by the alpha was not physically forced upon other\ndominants and was only obvious when the dominants\nwere separated as we did in our Split Group treatment.\n17\n--~ 20 m~\n-~-\n01'- 15-\nD\n~_~\n,oil.\no\n.-~\nZ\nDOMINANTS ~\nSUBDOMINANTS ~ -\nINTERMEDIATES ~\nALL ALPHA ALPHA& ALPHA,\nPRESENT REMOVED BETA BETA &\nREMOVED GAMMA\nREMOVED\nGROUP COMPOSITION\n-2O\nO\nOc\n.15 mc\n• 10 ~ -<\nNz\nC g ~-K\nrrl\n0\nFig. 6. Number and status of birds yelling for the first time when\nwe varied group composition in the first replicate (experiments\n1-4). The line graph gives the cumulative number of birds that\nhad yelled at some point during experimentation. The bar graph\nindicates the composition of the new yellers accumulated in the\nline graph each time an alpha was removed. (Status categories\nrefer to status prior to the experiment)\nDuring this phase of experiment 1 we succeeded in getting the alpha and beta on opposite sides of the screen\ndoor. Although these birds were in visual and vocal contact within a few meters of each other the beta's behavior\nimmediately shifted when he was protected from the alpha and assumed the top position within his subgroup.\nInstead of waiting for the alpha to lead the way to food,\nthe beta was the first to eat and he yelled and attacked\nothers as he approached the food (Fig. 7A). However,\nhe never yelled when in the same subgroup with the\nalpha.\nIn experiment 2, the alpha was removed leaving the\nbeta in charge and he yelled regardless of who was in\nhis presence (Fig. 7 B). We were not able to isolate the\ngamma in this experiment and he remained silent in the\npresence of the beta. We did succeed in isolating the\ndelta and he took charge of his subgroup and yelled\nonly when isolated from the beta and gamma.\nIn experiment 3, the alpha and beta were removed\nand the gamma finally yelled as he assumed the alpha\nrole in the group (Fig. 7 C). As in experiment 2, the delta\nyelled only when isolated from the gamma. The delta\ncontinued yelling after the alpha, beta, and gamma were\nremoved (Fig. 7D). In this last experiment, however, the\ndelta and epsilon both yelled and even yelled without\nconflict when perched side by side.\nIn the first 3 experiments we can make 6 independent\ncomparisons of yelling by dominants when in the alpha's\ngroup versus when not in his group. 3 of these are comparisons of yelling within an experiment when dominants never yelled unless they were out of the alpha's\ngroup (then they averaged 8 yells/rain, SD = 5). Yelling\nby dominants in the experiments following removal of\nthe current alpha versus yelling in the previous experiment with the alpha provide three more comparisons.\nDominants did not yell with the alpha, but averaged \n18\nLu\nz\n30-\n25\"\n20\"\n15\"\n10\"\n5\"\n0\n'!t 1\nR\n0 ~\nm ALL BIRDS IN CONTACT 100 ]\n~2~ SCREEN BETWEEN /\nDOMINANT and 801 SUBDOMINANT ~D m i R B, O REMOVED 60[ All Birds Present ~ I-\n(Expo 1) A uJZ ~\no8 40\nn.,,\nul 20 n\n0\nil °\nAlpha Removed\n(Exp. 2)\n11 B\n8\n10-Alpha and Beta\nRemoved\n5-\n0\n10-\n52\n0\n(Exp.3)R R ~\nAlpha, Beta and\nGamma Removed\n5) ~Exp. R R\n7\nc\n13 43\no\n%,°% %o,\nINDIVIDUALS YELLING\nFig. 7. Yelling by (5) most dominant immatures as a function of\ndays without food and who they are in contact with. Lower ranking\ndominants begin to yell as top ranking dominants are removed\nor when they are prevented from contact with the dominant when\nthey are behind screen doors (shaded bars). Yell rates for each\nmale are averages+ ISD for 1 min samples during focal observations of each male. Number of such samples for each male is given\nabove error bar\n6.1 yells/rain (SD=I.9) in the subsequent experiment.\nTogether these 6 comparisons allow us to conclude that\nalphas suppress yelling by the other dominants (Wilcoxon T=0, P=0.031).\nGiven the significant suppression of yelling by alphas\nwe were surprised by the lack of a significant increase\nin the number of yellers as successive dominants were\nI BEFORE REMOVAL\n~ AFTER REINTRODUCTION\nDEFENDED APPROACHED\nFIRST\nBEHAVIOR OF ALPHA MALES\nFig. 8. Defense and first approach to food items by alpha males\n(3) while in the group (filled bars') and when reintroduced (hatched\nbars). Only approaches to food by hungry birds (3-4 days without\nfood) are included. Samples are derived from N= 13 approaches\nbefore removal and N=46 after reintroduction. Note apparent\nloss of both status (defense) and \"bravery\" (willingness to approach first)\nremoved (Table 1, Fig. 4). In the first replicate of group\ncomposition the percentage of immatures yelling increased from 40% when all birds were present, to 50%\nafter the alpha was removed, to 88% after the beta was\nremoved, and it remained at 88% after the gamma was\nremoved. However, when the dominant birds were reintroduced the percentage of the group yelling remained\nhigh varying only between 75% and 83%, rather than\nreturning to previous levels.\nThe continued yelling of many birds after the dominants were reintroduced was likely due to status shifts.\nThe moment we placed previous dominants back into\nthe main aviary they were chased and attacked by the\nnew alpha and beta birds. Evidently, the returnees were\nrecognized as intruders and not as formerly dominant\ngroup members (free-living vagrants that visit the aviary\nare also responded to with threatening postures). Returning dominants did not reclaim their prior status.\nEach dropped at least 3 places in the hierarchy (Table 2).\nTable 2. Changes in the status of top ranking birds after the alpha, beta, and gamma\nwere removed from the main aviary. Each dominant was the alpha bird just prior to\nhis removal and each dropped in status after reintroduction. All 3 dominants were housed\ntogether in a peripheral aviary during removal. RB was the first removed and last reintroduced and was out of the main aviary for 76 days. GB was out for 55 days. GY was\nout for 28 days\nPrior to experiments\n(30 Dec- 16 March)\nDuring reintroduction\n(26 MayM6 July)\nCode N Percentage of Rank Rank Percentage of N\ninteractions won interactions won\nRB 118\nGB 75\nGY 96\nBR 75\nYW 120\nBrW 85\nB 114\nRY 61\n98.3 1 4 67.3 110\n88.0 2 7 38.5 78\n74.0 3 6 55.8 95\n57.3 4 1 100.0 41\n74.2 5 5 75.9 108\n70.6 6 8 50.0 58\n50.0 7 2 95.8 48\n68.9 8 3 72.7 77 \nZ\nuJ\n30\n20\n15\n10\n5.\nO-\n• ll BEFORE REMOVAL\n~ AFTER REINTRODUCTION\nBIRD 1 BIRD 2 BIRD 3\nFig. 9. Yelling rates by 3 alpha males while in the group (filled\nbars) and when reintroduced. Yelling was only measured as hungry\nbirds (4 days without food) approached food. Note elevated zero\nline: none yelled after reintroduction. Yell rates before removal\nare averages (+ I SD) from N=16 (RB), N=13 (GB), and N=8\n(GY) 1 min samples during focal observations of each male\n19\n25-\n20- • l\nz\n~5\nIJJ\nn 10\ncO\n..I\n,,, 5\n>- .. ~\n0 ---\nNO 5MIN 30MIN NO 5MIN 30MIN\nFOOD AFTER AFTER FOOD AFTER AFTER\n2-3 FEEDING FEEDING 2-3 FEEDING FEEDING\nH H\nFig. 10. The influence of hunger on yelling (observer not visible)\nby 10 hand-reared fledglings (approximately 2 months old at start\nof tests). Boxes indicate rates of yelling per 10 birds averaged over\n4 tests spanning 11 days. Each test lasted 1 day and included responses before and after 2 feedings. All 10 nestlings were observed\nyelling during the course of a test. Error bars are +/-1 SD\nEspecially surprising was the change in RB's status. As\nthe former alpha he was extremely dominant but upon\nreintroduction he was consistently displaced by 3 birds\nfor nearly 2 months.\nAssociated with drop in the former alpha birds' stares was a change in their foraging behavior and yelling.\nReintroduced former dominants rarely were the first to\napproach food and were even less frequently defensive\nof food they approached (Fig. 8). Neither of the 3 former\nalpha birds yelled during the experiment following reintroduction (Fig. 9). Only after 2 months did RB occasionally yell when he approached food. Reintroduced\nalphas did not regain the status and did not continue\nto behave tyrannically at food nor suppress others from\nyelling.\nOntogeny of yelling\nYelling in response to hunger develops early in life. We\nhand-reared 10 nestlings in the spring of 1989. On four\noccasions we allowed them to go through a series of\nhunger-feeding cycles (Fig. 10). Nestlings began to make\nyell-like calls (Fig. 11 A) after 2-3 h without food, however, within a few minutes of eating yelling was silenced.\nThe rate of yelling declined significantly as a function\nof time since feeding (Kruskal-Wallis W= 10.2 P=0.006)\nThe transition from hoarse begging calls to clear yells\noccurs over a 3 month period (Fig. 11 A).\nVariation in yelling\n2 sources of variation in yells were apparent. First, individuals produced yells differing in tone (Fig. 11 B). Second, yells given by dominants as they approached food\ndiffered from yells given in less aggressive situations\n(Fig. 1 l C). Aggressive yells were of short duration and\nsharp, sounding like an emphatic \"who!\". Yells given\nby hungry, but non-aggressive birds were more drawn\nKHz\n4\n3\n2\n1\no\nKHz\n4\n3\n2\n1\n0\nKHz\n;\n0\nA Hand-reared Juveniles\n,¸,\n~ ~ r~'~', ~ ~1 ~'~' ~ ~ ~\n,, I i , ~!,., , ,,I ....\n'.~.'\"i \"l'/\"'~' ' \" '~,'. ~ ,4i~,,~.l,i~',,' ~' ~,,'i 1~,~;\n5/9/89 5/21/89\nB Individual Variation\n?i~lli~lll~l/~l ,l~,i,,~,,, ~,, ,\n6/21/89 ~8/2/89\n! ~1~ ~ i 11 II,\n' I\n,,?~\ni~ \"~i '~ I\nI 41 L!~I\n• I~ /~'3t!~I '\nBr Y Br B\nScale: } 1see. ~\nC Contexts\n=~t=\n, = ~,~\n'~ll~\"~ ~'~l!O(('t ~(i~' [~l ~\n~'il'i ~ ~Jl'tl l'~ll =~1 '¢ ~t~tV~l ~,\n~11, tl ~ ,i\ni~' ~ ~'~ t #ll,~ ,~\n~ i'~,\nW54 RYR\nd :~1\n~ ]~!t I,\n\" f\n~t ~\" ~I~ r\nR B Begging R B' ~on\nAggressive Nest\nYell\nFig. 11. Sonograms of yells. (A Ontogeny of the yell. Left=raspy\ncalls on 9 May, I week before fledging. Center=21 June, 7 weeks\nafter fledging. Right=2 August, 9 weeks of age. B Variations in\nthe yell of 4 different individuals. C Left = submissive begging near\ndominant adults. Center=the aggressive \"who!\" yell and (for\nright) female yelling on nest. Right=plaintive begging near\nscreened-in meat\nOutside Birds --\nPlaintive Yells\nout and plaintive sounding. Adults rarely yelled, however, as in many corvids, adult females use calls similar\nto juvenile begging calls when receiving food from their\nmates (Fig. 11 C; Goodwin 1986). Yells given by adult\nfemales on the nest also were drawn out. \n20\n5O\nLL 40\n0\n~..~ 30\nuJ\n~ 20\nD\n~'u~ >- 10\n0\n/ .\n/ \\\n\" ....-O / \"~\n• A ~ \"~\" •\nABCD\nArrival of Perched in First Birds Crowd Crowd\nBirds from Trees Before Approaching Beginning Feeding\nRoost Feeding Food To Feed\nFig. 12. Temporal changes in yelling by\nfree-ranging birds at 4 carcasses in 1989.\nA Approximately 30 birds feeding on a\ncow on 8 April. B Approximately 20\nbirds feeding on a moose on 11 April. C\nApproximately 30 birds feeding on a\ndeer and a bear on 8 Nov. D\nApproximately 50 birds feeding on a\ncow on 23 Nov. These 4 observations\nwere made 1 to 3 days after a large\ncrowd gathered at each bait. Time span\nof observations varies depending on\ndisturbance during arrival (Times from\narrival at feeding site to crowd eating:\nA=37 rain, B=2 h 20 min, C=1 h\n7 rain, D = 53 min)\nComparisons to free-living birds\nBehavior in the aviary closely matched that observed\nin nature. In the field, dominant ravens also were responsine for the marjority of yelling. 8 tagged birds were\nobserved yelling at carcasses. All yelled as they approached food, displaced another bird, and took over\na choice feeding spot. Yelling by wild ravens typically\npeaks as they begin to feed approximately 0.5-1 h after\nbirds arrive from their nocturnal roost (Fig. 12). During\npeak yelling, most yells are short, sharp \"who!\" yells\ngiven by birds claiming prime feeding spots. Prior to\nfeeding, most yells are long, loud and given by unidentified perched birds.\nThe status and behavior of our captive immatures\npersisted after they were reintroduced into the wild 1\nyear after their original capture. We observed 3 dominants, 4 subdominants, 2 intermediates, and 3 subordinates for a 2 week period after their release. These birds\nforaged at carcasses along with 10-30 wild vagrants and\na pair of resident adults. The dominants and subdominants deferred to few birds and held choice feeding positions. The intermediates and subordinates deferred to\nmost birds they encountered, fed infrequently, and rarely\ncontrolled foods.\nDiscussion\nIn the forests of northeastern North America, ravens\ngenerally forage for carcasses and other food (Bruggers\n1988) in the winter by flying singly or in pairs. However,\nlarge numbers of vagrant, nonbreeding birds are commonly found at food bonanzas. One of the vocalizations\ncommonly heard from vagrants at the aggregations in\nthe \"yell,\" and playbacks of this call are highly effective\nin attracting those ravens which are already aggregated\nin the surrounding forest (or passing through) near a\nfood bonanza (Heinrich 1985, 1988). Grouping is adaptive because it allows vagrants, who are subordinate to\nterritorial adults, to access and profitably forage on defended foods (Heinrich 1988 b, 1989; Marzluff and Heinrich in prep.).\nIn nature commonly only one or a few birds of any\nfeeding crowd yell before feeding begins (Heinrich 1989).\nWhich birds yell? Our field and aviary observations suggest that the first yellers of the morning are hungry birds\nthat experienced poor feeding on preceding days. These\nbirds give loud, drawn out yells while perched in trees\naround a carcass. When feeding begins yelling reaches\na peak as many birds descend to the ground and walk\nup to claim their feeding positions. Dominant juveniles\ngive the majority of yells and they walk with erect\nfeathers, spread their shoulders and take the prime feeding spots. Our experiments indicate that these dominant\nimmatures suppress the yelling of the closest subdominant immatures.\nProximate reasons for yelling\nAs we argued in the Introduction it is necessary to understand the proximate basis of recruitment signals, such\nas the yell. Our results suggest that hunger and status\nadvertisement are the primary proximate factors causing\nyelling. Overpowering territorial adults is not a proximate reason for yelling as yells were rarely given by\nimmatures in the presence of adults. In addition, if attraction of others was a proximate reason for yelling\nthen dominant vagrants would not be expected to inhibit\nsubordinates from calling. Instead, dominants should\npunish subordinates for not calling, as occurs in spider\nmonkeys (Chapman and Lefebvre 1990). We conclude\nthat the accumulation of ravens at the site of yelling\nrepresents recruitment because attraction of ravens by\nyelling enhances the fitness of the signaler, but assembling a crowd is not a proximate reason for yelling.\nThe ontogeny of yelling can be traced to the nestling\nstage. Sonograms of individual birds beginning while\nthey are still in the nest indicate a progression from\nrasping disharmonic calls to the loud harmonic yells typically heard at food (Fig. 11). Thus, initially the birds\nyell proximately in response to the presence of food (or\nthe expectation of food), provided they are hungry. The\nfollowing winter (and winters) they continue to do the\nsame, except now the yell is restrained by social superiors.\nAdaptive significance of yelling\nMost species of birds produce begging calls as nestlings\nwhen they are fed. However, these are lost shortly after\nindependence from parental care. We know of no other\ncases where begging calls develop into louder, more easily located calls indicative of food. Presumably this is\nrare because in most cases it is disadvantageous to \n21\nbroadcast the presence of food. On the contrary, it is\nadvantageous for vagrant ravens to advertise the location of food because most foods are defended and therefore unavailable until a crowd assembles and overpowers\nthe defending territory owners.\nYelling may be of further adaptive significance as\na status signal. As has previously been determined\n(Gwinner 1964), captive ravens kept within a flock form\nand maintain a linear dominance hierarchy, and social\ndominance in these birds is an important correlate of\npair formation. At least in part, the food bonanzas on\nwhich ravens specialize serve as a site where dominance\nis established, reinforced and/or maintained by fighting\nand by appropriate signalling of both dominance and\nsubmission. It therefore seems likely that the yelling we\nobserved in our captive birds is related to status signalling that might ultimately translate to mate and food\nacquisition.\nIf yelling by immatures ultimately functions in their\ngaining access to defended foods, it may at first appear\ncurious that they did not yell in the presence of the\nadults. Indeed the adults inhibited, rather than facilitated, yelling. However, the presence of defensive adults\nmay not trigger yelling because adults attack yelling vagrants, thus discouraging their display of status and reducing recruitment to the food the adults try to defend.\nHow can a signal that is proximally given as a show\nof hunger and status be ultimately functional also in\nrecruitment? The show of status ultimately functions in\nsexual selection (Gwinner 1964; Komers and Dhindsa\n1989). However, since the same display yields other advantages related to feeding and gaining access to food,\nit has likely become amplified rather than compromised\nthrough evolution. Presumably this is rare because in\nmost cases it is disadvantageous to broadcast the presence of food. However, it is convenient for ravens to\nadvertise status at food, and it is advantageous for vagrant ravens to advertise the location of food, because\nmost foods are defended and therefore unavailable until\na crowd assembles and overpowers the defending territory owners. As a result, even though yelling is stimulated\nby hunger and status it has likely been embellished by\nnatural selection because of the benefits of group foraging. It seems unlikely that a display audible for several\nkilometers, such as the yell, would have evolved solely\nas a status signal. Quieter vocalizations or posturing,\nseem more likely because they would suffice for the first\nfunction. In typical jury-rigged fashion natural selection\nhas embellished an existing behavior to serve a new function rather than create a new behavior. As a result, the\nproximate and ultimate reasons for yelling do not coincide.\nAcknowledgements. This study was made possible due to the generous help of an \"army\" of volunteers (in Heinrich, 1989) who\nhelped to build the facilities.\nWe thank Henry and Lee Disotto, Bill Adams, Danny Danforth, David Lidstone, the Wojcik clan, Harry Wycoff, Steve Freschett, and Rick Ashton for their assistance capturing and marking\nravens. Patty Rabenold generously aided in one raven roundup\nand helped us do the laparotomies. Mike Pratt, Dwight Cram,\nFrank and Don Castonguay, Peter Cross, and Wendy Howes\nhelped supply raven food. David Perez tended the captive ravens\nin our absence. Jeffrey Cynx provided sonograms. We are indebted\nto David Lidstone, and Bill and Butch Adams, for cutting and\nhauling carcasses when temperatures dipped below zero. Colleen\nMarzluff aided in all phases of this research making it both possible\nand enjoyable. Joseph Schall, Kevin McGowan, and 3 anonymous\nreviewers provided helpful comments on a draft of the manuscript.\nFunding was provided by the National Science Foundation, Grant\nBNS-8819705.\nReferences\nAltman J (1974) Observational study of behavior: sampling methods. Behaviour 49 : 227-267\nAppleby MC (1983) The probability of linearity in hierarchies.\nAnim Behav 31:600-608\nBalda RP, Bateman GC (1974) Flocking and annual cycle of the\npinon jay, Gymnorhinus cyanocephalus. Condor 73 : 287-302\nBrown C (1986) Cliff swallow colonies as information centers. Science NY 234:83-85\nBruggers JD (1988) The behavior and ecology of the common\nraven in Northeastern Minnesota. Ph D thesis, University of\nMinnesota, Minneapolis\nChapman CA, Lefebvre L (1990) Manipulating foraging group\nsize: spider monkey food calls at fruiting trees. Anim Behav\n39 : 891-896\nElgar MA (1986) House sparrows establish foraging flocks by giving chirrup calls if the resource is divisible. Anita Behav 34:169-\n174\nGreene E (1987) Individuals in an osprey colony discriminate between high and low quality information. Nature (Lond)\n329: 239-241\nGwinner E (1964) Untersuchungen fiber das Ausdrucks- und Sozialverhalten des Kolkraben (Corvus corax corax L.). Z Tierpsychol 21 : 657-748\nHeinrich B (1988) Winter foraging at carcasses by three sympatric\ncorvids, with emphasis on recruitment by the raven, Corvus\ncorax. Behav Ecol Sociobiol 23:141-156\nHeinrich B (1989) Ravens in winter. Summit Books of Simon &\nSchuster, New York\nHeinrich B, Bartholomew GA (1978) Roles of endothermy and\nsize in inter- and intraspecific composition for elephant dung\nin an African dung beetle, Searabaeus laevistriatus. Physiol Zool\n52: 484~494\nK6nig C (1983) Interspecific and intraspecific competition for food\namong old world vultures. In: Wilbur SR, Jackson JA (eds)\nVulture biology and management. University of California\nPress Berkeley, pp 153-171\nKomers PE, Dhindsa MS (1989) Influence of dominance and age\non mate choice in black-billed magpies: an experimental study.\nAnim Behav 37: 645-655\nKrebs JR (1974) Colonial nesting and social feeding as strategies\nfor exploiting food resources in the great blue heron (Ardea\nherodios). Behaviour 51 : 99-134\nRabenold PP (1987a) Recruitment to food in black vultures: evidence for following from communal roosts. Anim Behav\n35:1775-1785\nRabenold PP (1987b) Roost attendance and aggression in black\nvultures. The Auk 104: 647-653\nStiehl RB (1978) Aspects of the ecology of the common raven\nin Harney Basin, Oregon. Ph D thesis. Portland State University, Portland\nWaltz EC (1983) On tolerating followers in information-centers,\nwith comments on testing the adaptive significance of coloniality. Colonial Waterbirds 6:31-36\nWard P, Zahavi A (1973) The importance of certain assemblages\nof birds as \"information centers\" for food finding. Ibis\n115:517-534 ", "affiliations": [{"university": "University of Vermont", "country": "United States", "discipline": "Zoology"}], "species_categories": ["bird"], "specialized_species": ["common raven"], "computational_stages": ["Data Collection", "Pre-processing", "Meaning Identification"], "linguistic_features": ["Vocal Auditory Channel and Turn-taking", "Semanticity", "Specialization", "Openness", "Tradition and Cultural Transmission"], "status": "saved", "created_at": "2026-01-13T12:49:59.882689", "updated_at": "2026-01-13T14:02:12.730316", "committed_at": "2026-01-13T14:02:22.706836"}
{"id": "051f43fe-c2f3-46c2-a04d-fe5aef700e0b", "title": "The Chick-a-Dee Call System of the Mexican Chickadee", "authors": ["Ficken,  Millicent Sigler", "Hailman,  Elizabeth D.", "Hailman,  Jack P."], "year": "1994", "journal": "The Condor", "abstract": "", "doi": "10.2307/1369065", "analysis_notes": "The Chick-a-Dee Call System of the Mexican Chickadee\nAuthor(s): Millicent Sigler Ficken, Elizabeth D. Hailman and Jack P. Hailman\nSource:\nThe Condor , Feb., 1994, Vol. 96, No. 1 (Feb., 1994), pp. 70-82\nPublished by: Oxford University Press\nStable URL: https://www.jstor.org/stable/1369065\nREFERENCES\nLinked references are available on JSTOR for this article:\nhttps://www.jstor.org/stable/1369065?seq=1&cid=pdf-\nreference#references_tab_contents\nYou may need to log in to JSTOR to access the linked references.\nJSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide\nrange of content in a trusted digital archive. We use information technology and tools to increase productivity and\nfacilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.\nYour use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at\nhttps://about.jstor.org/terms\nOxford University Press is collaborating with JSTOR to digitize, preserve and extend access to\nThe Condor\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/termsThe Condor 96:70-82\n? The Cooper Ornithological Society 1994\nTHE CHICK-A-DEE CALL SYSTEM OF THE MEXICAN CHICKADEE1\nMILLICENT SIGLER FICKIEN\nDepartment of Biological Sciences, University of Wisconsin-Milwaukee, Milwaukee, WI 53211\nELIZABETH D. HAILMAN\nDepartment of Dairy Science, University of Wisconsin-Madison, Madison, WI 53706\nJACK P. HAILMAN\nDepartment of Zoology, University of Wisconsin-Madison, Madison, WI 53706\nAbstract. Chick-a-dee calls of the Mexican Chickadee (Parus sclateri) are composed of\ncombinations of three common note types (A, C and D) and one very rare type (B). Calls\nhave the invariant sequence of notes A-B-C-D, where any note type may be omitted, given\nonce or repeated a variable number of times before transiting to the next type. The B and\nC notes are phonologically similar to the B and C notes of chick-a-dee calls of the Black-\ncapped Chickadee (P. atricapillus), but the A note is markedly different and the D note\nsomewhat different from equivalent notes of the congener. A total of 2,071 calls recorded\nyielded 60 different call types, and Zipf-Mandelbrot plots show that the call system is \"open\";\nas the sample size is increased new call types will be found without demonstrable bound.\nIn relatively undisturbed contexts (with mate on territory, in fall flocks, alone in fall) birds\ngave mainly [A][D] calls with lesser numbers of [A] and [C] calls, where brackets indicate\nvariable repetition of note types. In disturbed contexts (mobbing plastic Great Horned Owl,\nmobbing speaker playing calls of the Northern Pygmy-Owl, observer sitting under the nest\ncavity) the birds gave more [C] calls with [A][C] as well. In the longest mobbing session to\nowl calls, birds gave mainly [A] calls when approaching, switched to [C] calls while flying\nabout the speaker, and then resumed [A] calls and moved offwhen the playback was stopped.\nOutside of human language, this is the second truly combinatorial system of vocal com-\nmunication found in animals, the first being chick-a-dee calls of the Black-capped Chickadee.\nThis study provides the first data substantiating quantitative differences in calls from different\ncontexts, an important step toward understanding what kinds of information combinatorial\nchick-a-dee calls encode.\nKey words: Parus sclateri; Mexican Chickadee; vocalizations; calls; syntax; mobbing;\nflocks.\nINTRODUCTION\nAll tit species of the subgenus Poecile (genus Par-\nus) appear to give combinatorial \"chick-a-dee\"\ncalls (Hailman 1989), but this complicated call\nsystem has heretofore been analyzed quantita-\ntively only in the Black-capped Chickadee, P.\natricapillus (Hailman et al. 1985, 1987; Hailman\nand Ficken 1986). Chick-a-dee calls of the Mex-\nican Chickadee (P. sclateri) were first recorded\nby Dixon and Martin (1979), whose spectro-\ngrams (their figures la and b, p. 422) show two\nkinds of notes. Ficken (1990a) found that in a\nsample of over 1,000 recorded calls, four types\nof notes were given but one of them only rarely\n(see also Ficken and Nocedal 1992). The present\nstudy, based on further field work including\nmobbing experiments, (1) describes the acous-\ntical features and analyzes the syntactical struc-\nture of the Mexican Chickadee's calls, (2) co\npares these results with those from Black-cap\nChickadees, and (3) documents differences\ncalling in different contexts.\nChick-a-dee calls are of special interest be\ncause these compose the only presently doc\nmented combinatorial system of animal com\nmunication outside of human language.\nBlack-capped Chickadee uses four note ty\n(Ficken et al. 1978) in combination to create hu\ndreds of different call types (Hailman et al. 19\nThe note types (designated A, B, C and D) oc\nin the fixed sequence A-B-C-D, where any n\ntype may be omitted, given once, or repeat\nvariable number of times before transiting to\nnext note type within the call. This straight\nward phonological syntax was shown to be l\nically explicit by writing a \"characteristic fu\ntion\" (correctly working algorithm) for a Tur\nmachine (Hailman and Ficken 1986). Howeve\nsystematic departures from first-order MarReceived 12 March 1993. Accepted 9 August 1993.\n[70]\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/termsMEXICAN CHICKADEE CALLS 71\nchains of transitions between note types show\nthat far more complicated rules underlie the pre-\ncise structure of calls (Hailman et al. 1987). De-\nspite extensive analyses of syntactical structure,\nthe communicative significance of chick-a-dee\ncalls remains elusive. The present study of the\nMexican Chickadee provides not only the first\ncomparative data on syntax in another species\nbut also an important first step toward under-\nstanding differences in calls in different behav-\nioral contexts.\nMETHODS\nAll recordings were made at or in the vicinity of\nRustler Park in the Chiricahua Mountains (Co-\nchise County) in southeastern Arizona. Record-\nings were made by M.S.F. in October 1985, and\nMay and October 1986 (Ficken 1990a); further\nfield work by the three authors together was con-\nducted in April 1990 and March 1991, by M.S.F.\nin May 1991, and by the three authors together\nin April 1992.\nCalls were divided into four principal contexts\nof recording: (1) with mate on the breeding ter-\nritory, (2) in mixed-species flocks in the fall, (3)\nwhen disturbed by the presence of an observer\nnear the nest, and (4) when mobbing a speaker\nplaying the call of a Northern Pygmy-Owl, Glau-\ncidium gnoma. In addition, small samples of vo-\ncalizations were taped in two other contexts: (5)\nlone birds in the fall and (6) mobbing a plastic\nmodel of a Great Horned Owl, Bubo virginianus.\nThe mobbing experiments using owl tapes were\nconducted at five different sites. The partially\ndiurnal Northern Pygmy-Owl takes small birds\nand is common in the study site; during one ex-\nperiment, an owl answered the playback from\nacross a canyon. In two experiments tapes were\nplayed back from a Marantz PMD 430 Profes-\nsional cassette recorder through a Realistic Min-\nimus-0.6 amplified speaker near the ground on\na log or stump. In the other three experiments,\ntapes were played back from a Sony Walkman\nProfessional cassette recorder through a Sony\nSRS-27 amplified speaker. All playbacks attract-\ned a number of small forest species besides Mex-\nican Chickadees.\nMost of the recordings were made on a Sony\nWalkman Professional cassette tape recorder\n(details in Ficken 1990a, 1990b), with further\nrecordings of mobbing experiments made with\na Sony 8 mm camcorder. Tapes were analyzed\nby M.S.F. with a Kay 7800 Digital Sona-Graph\n(150 Hz filter band width). Note types (described\nin the Results) were classified by eye and written\non data sheets. In all, 2,045 calls were analyzed\n(a few occurred in other contexts with insufficient\nsample sizes for analysis). Data on the note com-\nposition and context of each call were read into\na database and made accessible to mainframes\nand microcomputers for analyses. Analyses were\nbasically a subset of those performed on calls of\nBlack-capped Chickadees by Hailman et al.\n(1985), designed by J.P.H. and programmed by\nE.D.H. Most of the special analytical programs\nwere written in PASCAL and run on microcom-\nputers.\nContingency tables were analyzed statistically\nusing \"computer-intensive\" methods of the pub-\nlicly available software MONTE CARLO RXC\nwritten for the Apple Macintosh by W. R. Engels\nof the University of Wisconsin-Madison, De-\npartment of Genetics. Such analyses provide a\nmore sensitive and accurate assessment than Chi-\nsquare methods often applied to such data (En-\ngels 1988). At least 1,000 trials were run for a\ngiven test to create the distribution against which\nthe data were compared. Sequential analysis by\nthe method of Markov chains was done with\nUNCERT, publicly available software for DOS\ncomputers copyrighted by E.D.H. and J.P.H.\nRESULTS\nPHONOLOGY\nChick-a-dee call systems as a whole are doubtle\nhomologous among species possessing them. Tw\nof the Mexican Chickadee's note types differ\nsomewhat in acoustical structure from those of\nthe Black-capped Chickadee, so they might not\nbe homologous. Unlike the basically chevron-\nshaped A note of the Black-capped Chickadee,\nthe Mexican Chickadee's A note (Fig. 1) is fre-\nquency modulated at a high rate, giving it a buzz-\ning aspect to the human ear. Buzzing notes char-\nacterize other vocalizations of the Mexican\nChickadee as well (Dixon and Martin 1979; Fick-\nen 1990a, 1990b). The A note usually sweeps\ndownward in frequency during its duration (Fig.\n1) but the magnitude of the decline is variable.\nDixon and Martin (1979, Fig. la) showed a call\nwith three A notes that lacked this frequency\ndownsweep completely, and there seems to be a\ncontinuum in the slope of the downsweep. The\nA note was common (1965 notes recorded), com-\nprising 28.4% of all notes.\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/terms72 M. S. FICKEN, E. D. HAILMAN AND J. P. HAILMAN\na b\n8r\n?~iC . . ,\"?. i :'\nI ... . ....V4\n. 2A\n0\n0 0.5 1 1.5Time (s)\nc d\n8-\nZiM 6 V~i?t~:\nYC i\n\"P . ,.:. . . . .:,\nTime (s)\nFIGURE 1. Sound spectrograms illustrating the A and D notes of chick-a-dee calls of the Mexican\nselected to illustrate some of the phonological variation found in notes. The frequency and time sca\nsame for all spectra. (a) Commonest of all calls is the call type AD, with typical down-slurred A n\nthe banded structure of the D note. (b) A call showing the typical A/D contraction of the final A\nnotes of calls containing both types. (c) The contracted A/D may begin a call; notice the noisy\nstructure of the D notes. (d) Rare phonological structures occur, as in this call with an A-like introd\nfollowed by an A/D-like structure of long A component and short D component.\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/termsMEXICAN CHICKADEE CALLS 73\nTABLE 1. Durations, highest frequencies, and lowest frequencies of note types in ch\nMexican Chickadee. Entries are mean + standard deviation (sample size).\nCall type: duration Fig. No. of Duration Lowest Highest\nNote(s) call type (msec) frequency (kHz) frequency (kHz)\nSingle A - 151 + 35 (13) 5.7 + 0.7 (13) 8.0 + 0.5 (13)\n\"Low frequency\" A 2a 208 ? 28 (14) 3.4 + 0.2 (14) 6.3 ? 0.6 (14)\nAA: 331 + 92 (16) -\nFirst A 127 + 53 (16) 5.5 + 0.6 (16) 8.1 + 0.6 (16)\nSecond A 120 + 35 (16) 5.7 ? 0.8 (15) 7.3 + 1.8 (16)\nAA/D: 521 ? 118 (20) lb\nFirst A 170 ? 32 (20) 5.8 + 0.4 (20) 8.4 ? 0.2 (20)\nSecond A (of A/D) 50 ? 31 (20) 4.2 + 0.7 (18) 8.2 ? 0.3 (20)\nD (of A/D) 212 ? 67 (20) 3.2 + 0.3 (19) 6.2 + 0.5 (18)\nA/D: 425 ? 107 (17) -\nA (of A/D) 100 ? 76 (17) 4.4 ? 0.6 (16) 8.3 ? 0.2 (16)\nD (of A/D) 319 + 95 (17) 3.2 + 0.2 (17) 6.4 ? 0.5 (12)\nA/D D: 690 ? 181 (18) Ic\nA (of A/D) 87 ? 59 (18) 4.3 ? 0.7 (15) 8.2 ? 0.3 (16)\nFirst D (of A/D) 245 ? 117 (18) 3.1 + 0.3 (14) 5.6 + 0.3 (15)\nSecond D 270 ? 85 (18) 2.9 ? 0.4 (14) 5.5 + 0.3 (15)\n[C] (First of a series) 2e 23 ? 2 (14) 2.8 ? 0.3 (14) 7.6 + 0.1 (13)\nThe Mexican Chickadee's D note more closely\nresembles that of the Black-capped Chickadee,\nand is typically the longest note-type in duration\n(Fig. 1)-even longer than that of the Black-\ncapped Chickadee. The D note varies from hav-\ning a distinctly banded frequency structure (Fig.\nla) to being nearly uniformly noisy (Fig. Ic). The\nMexican Chickadee's D note often has an onset\n\"spike\" of higher frequency (also evident in spec-\ntrograms of Dixon and Martin 1979, and Ficken\n1990a). The 1,080 D notes recorded comprise\n15.6% of the sample. When A and D notes occur\nin the same call (always in the order A-D), they\nmay be independently uttered (Fig. 1 a), or more\ntypically the last A in a sequence merges with\nthe first D note to form a sort of contracted note\nwe designate by \"A/D\" (Figs. ib, c). The A part\nof this contraction is variable in duration, but\nusually much shorter than a typical A. Rarely,\npeculiar phonations occur in recordings, as in\nFigure 1 d where an A-like note occurs before an\nA/D-like contraction.\nIt might be that some variations in A notes\nare communicatively significant. Single A notes,\nnot combined with other As or other note types,\nare particularly variable. Among the 6,918 notes\nrecorded, 34 single A notes of unusually low\nacoustical frequency were found (Fig. 2a). These\nmay represent a rare but distinct subtype of A\nnotes, treated here as A notes but deserving of\nfurther study. The Mexican Chickadee also com-\nmonly utters high frequency notes that resemble\nbrief A notes (Fig. 2b) but are much shorter and\nnever combined with other notes into multi-note\ncalls. These notes appear to be related to or var-\niants of \"tseets,\" of which Ficken (1990a) re-\ncorded only six examples. These A-like notes are\nnot included in the analyses below.\nThe B note, which was very rare (3 occurrences\nin 6,918 notes), is chevron-shaped (Fig. 2d) and\nsimilar to the B note of the Black-capped Chick-\nadee. The C note, which was the commonest type\n(3,870 recorded, comprising 55.9% of all notes),\nis a \"noisy chevron\" (Figs. 2c, d), again similar\nto the C note of the Black-capped Chickadee.\nThe C notes were usually uttered in distinct cou-\nplets (Fig. 2e), another variation of phonology\nworthy of further study.\nThe duration of a given type of note varies, in\nsome cases markedly (Table 1), depending upon\nthe type of call and where the note falls within\nthe call. Internote intervals vary from about 50\nto 90 msec for various types, but the only sys-\ntematic variation found was a bimodal distri-\nbution of C-C intervals (shorter intervals within\nthan between couplets). The interval between the\nlast note of one call and the first note of the next\nis one to two orders of magnitude longer than\ninternote intervals within calls, thus defining the\ncall as a natural unit of phonation.\nNotes of the Mexican Chickadee (especially\nthe A and D notes) are noticeably longer than\nthose of the Black-capped Chickadee (Table 1)\nwhereas the number of notes in a call is notice-\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/terms74 M. S. FICKEN, E. D. HAILMAN AND J. P. HAILMAN\na b\n8\nN%\n0 --~\nC 4 . -r . _i\"..a)\n0 0.5 1 1.5\nTime (s)\n8 c d e\n__ fill it ( ?i, ~if | , , \"\" . K4 i\"\n)21 \" '' .. ,\n-L 2-\n0~\nu.2 AGOC B 000000 cc cc cc cc\n. , : i ? . ! \"' ? ?::- ?'?\" ? . ,.??: ? '.: ?. ' . . : .. . ' . i\n0 0.5 1 1.5\nTime (s)\nFIGURE 2. Spectrograms illustrating other notes of chick-a-dee calls (cf. Fig. 1). The frequency and time scales\nare the same for all spectra. (a) A low-frequency A note that may represent a special subtype. (b) High-frequency\nnotes resembling abbreviated A notes, which appear to be \"tseet\" notes related to flight intention. (c) An A note\nwith two C notes. (d) The rare B note followed by a series of C notes. (e) The couplet structure of C notes, found\nonly in calls composed of this note type alone.\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/termsMEXICAN CHICKADEE CALLS 75\nAD\nBEGIN END\nCALL CALL\n(silence) (silence)\nB c\nFIGURE 3. Kinematic graph of first-or\nletters A, B, C and D are approximatel\n(except that the rare B would not be\nproportional to the probabilities of tra\nwould have invisibly thin lines are om\nrepetitions of the same note type.\nably fewer than in the other speci\na-dee calls of the Mexican Chickadee are from\none to at least 14 notes in length, which is a\nsmaller maximum note-length than in calls of\nthe Black-capped Chickadee. The longest in\nHailman et al. (1985) was 24 notes, and that\nsample did not include predator contexts, where\ncalls are apparently often longer (Apel 1985).\nTable 1 contains several examples of how the\nplacement of a note affects its duration. For ex-\nample, the A note of single-A calls is longer than\neither note of an AA call, but not as long as the\nfirst A that is followed by an A/D contraction to\ncompose a call. Similarly the D part of an A/D\ncontraction is shorter if the contraction is pre-\nceded by an A note than if the contraction is\nitself the entire call. The frequency characteris-\ntics of notes may also vary somewhat according\nto their placement within calls (Table 1).\nSYNTAX\nMarkov chain analysis showed that the sequence\nof notes within a call follows the same rule as in\nthe Black-capped Chickadee: note types always\noccur in the order A-B-C-D (no exceptions in\n2,071 calls), where any note type may be omitted\nentirely, given once, or repeated a variable num-\nber of times. The first-order Markov chain (Fig.\n3) reveals the principal syntactic structure. Calls\nbegin with A or C (less than 1% begin with B or\nD). If beginning with A, that note-type repeats,\ntransits to D, or ends the call; if transiting to D,\nthat note repeats or ends the call. If the call begins\nwith C, that note most commonly repeats, or\nends the call.\nFrom Figure 3 one may infer that calls will\ncommonly have the structure [A], [C] or [A][D],\nwhere the enclosure of the note type in square\nbrackets denotes possible repetition. Other call\nstructures should be much rarer. Categories such\nas [A][D] are termed sequence types, and each\nmay include many distinct call types such as AD,\nAAD, AAAAD, ADD, ADDDD, and so on.\nThere are 15 such sequence types possible, but\nbecause of the rarity of B notes five of these did\nnot occur in the record: [B], [B][D], [A][B][C],\n[B][C][D], and [A][B][C][D]. The commonest se-\nquence types were: 735 [A][D] calls, 664 [C] and\n517 [A], as expected from Figure 3. Two se-\nquence types occurred with sufficient frequency\nto be of interest: 88 [A][C] and 30 [C][D]. The\nremaining types were very rare: 7 [D], 2 [A][C][D],\nand 1 each [A][B], [B][C], and [A][B][D].\nThe departure of Mexican Chickadee calls from\na first-order Markov process was small. The\nmaximum uncertainty possible with four note-\ntypes is log2 4 = 2 bits/note, but the unequal\nfrequencies of note types yielded a zero-order\nMarkov chain of only 1.21 bits/note. First-order\nanalysis reveals a large drop to only 0.36 bit/\nnote, which would be a true first-order Markov\nprocess only if all uncertainty were removed.\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/terms76 M. S. FICKEN, E. D. HAILMAN AND J. P. HAILMAN\nSecond-order analysis showed a small drop to\n0.21 bit/note, meaning that long-range con-\nstraints are relatively unimportant in the calls.\nPut differently, the next note to occur can be\npredicted well from the previous one but know-\ning longer ordered-strings of previous notes hardly\nimproves predictability.\nOPENNESS\nA major finding in the Black-capped Chickadee\nwas that the call system is \"open\" (Hailman et\nal. 1985): as the sample size of calls is increased,\nthe number of different call types increases with-\nout demonstrable bound. A call type is defined\nas any different combination of notes; thus A,\nCC, AC, AAACC and ACCCCCC are all differ-\nent call types. In this study we found 60 different\ncall types in the 2,071 calls recorded, and as in\nthe Black-capped Chickadee these call types var-\nied widely in frequency of occurrence. For ex-\nample, there were 783 calls of the type AD but\nonly one of the type ADDDD.\nThe test for openness is made by a Zipf-Man-\ndelbrot plot of the probability of occurrence (P)\nas a function of occurrence rank (r) on log axes.\nMandelbrot's (1953) formula P = i(r + k)s for\na fitted function has three constants: the intercept\n(i), curvature factor (k) and asymptotic slope (s),\nwhich is always negative. To show openness it\nis not necessary to provide a fitted function but\nmerely to show that the plotted curve reaches a\nnon-zero asymptotic slope. Figure 4 shows this\nproperty for the Mexican Chickadee, both with\nall data combined (top curve) and plotted sep-\narately by the six contexts. Even the curves them-\nselves for the different contexts are remarkably\nsimilar, reaching about the same asymptotic\nslopes at their right-hand sides. The curves dem-\nonstrate that the call system of the Mexican\nChickadee, like that of the Black-capped Chick-\nadee, is open and hence more new call types will\nbe found if the sample is increased.\nDIFFERENCES AMONG CONTEXTS\nThe distribution of calls among the sequence types\ndiffered according to context of recording (Fig.\n5). For purposes of statistical comparison among\ncontexts, calls were considered in five categories:\n[A], [C], [A][D], [A][C] and \"other\" (all the re-\nmaining sequence types combined). As each re-\ncording context was independent of the others,\nevery possible pair of contexts was compared in\na 2 x 5 contingency table using the Monte Carlo\ntechnique (see Methods). All pairs of contexts\ndiffered significantly (P's < 0.001) except two\ninvolving the small sample size of lone birds: th\ndifference from \"with mate\" is marginal (P\n0.06) and from \"fall flocks\" non-significant (P\n0.19).\nWhen undisturbed on territory with the mate,\nalmost three-quarters of calls were of the [A][D]\nsequence type (Fig. 5a). When in mixed-species\nfall flocks (Fig. 5b) or alone (Fig. 5c), only about\nhalf of calls were [A][D], with [A] and [C] calls\nmaking up most of the other half. In the former\ncase, there were also a few calls of the [A][C] type\nand other types. When there was an external dis-\nturbance (right column of diagrams in Fig. 5),\nthe distribution of sequence types was markedly\ndifferent, with [C] and [A][C] together always\nmaking up at least half of all calls. When dis-\nturbed by a Great Horned Owl model, birds gave\nmany [A][C] calls (Fig. 5d). When mobbing a\nNorthern Pygmy-Owl tape, [A] and [C] calls were\ncommoner (Fig. 5e), and when disturbed by the\nobserver sitting about 6 m from the nest, [C] calls\npredominated (Fig. 5f).\nOWL-TAPE PLAYBACK EXPERIMENTS\nMobbing experiments provide a means of study\ning calls in detail because there is a specific locu\nin space to which the mobbing birds attend an\nall individuals present behave similarly. Exper-\niments were conducted at five sites from March\nto May of various years. The experiments, using\nplayback tapes of Northern Pygmy-Owl calls,\ngenerally attracted one pair of Mexican Chick-\nadees, often with small songbirds of other spe-\ncies. The chickadees gave 20 to 50 calls and then\ndeparted with the playback still running. One\nexperiment brought in several chickadees for a\nlong period, and the results are analyzed sepa-\nrately below. The results of the other four ex-\nperiments are shown in Figure 6, which plots the\ncumulative frequency of each call sequence type\nthrough the experiment.\nEvery experiment shown in Figure 6 yielded\na similar but unique set of results. The experi-\nment of 22 March was made about half-way on\nthe road to Rustler Park. This playback probably\nevoked calls from a single bird, which did not\nclosely approach the speaker. In all four exper-\niments one sequence type predominated, and that\ntype of call was given at an approximately equal\nrate (constant slopes in Fig. 6) throughout the\nexperiment. The predominant type was [A][D]\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/termsMEXICAN CHICKADEE CALLS 77\n1\nALL CALLS\n1.---....... \"(N=2071)\n\" .001 C~ bmate\nS... . o..0 .0001\n. \"O\" -. flock0 'O(N=1013)alone\nt (N=35)\nI)\n0 observer\n\" r (N=443)O\n-.. pygmyA horned (N=263)(N=36)\n1 10 100\nr (occu\nFIGURE 4. Zipf-Mand\nvalues are labeled only\nthe curves for clarity\nthat curves reach an\nfound as the sample s\nin the March and A\nthe experiments con\n[C] calls were comm\nand [A][D] occurred\nand three sequenc\nexperiment: [C][D]\n[A][C][D] in April. O\nsuggested that the [C\nwere most agitated\nto the speaker. The\nments may therefo\nduring the nestin\n(March and April).\nOne exper\nrespects. Fi\nMexican Ch\nof the prec\ninstead of\nbirds conti\nthat we co\nstill presen\nAnd third,\nwas record\nBirds bega\nmainly [A]\nbegan givin\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/terms78 M. S. FICKEN, E. D. HAILMAN AND J. P. HAILMAN\nUNDISTURBED DISTURBED\na. d. [A]\n[A] ~ [A][D] [C]i\nplastic\nwith mate [A][D] greatwith mate horned [A][C]\non territory owl\n(N=238) (N=36)\nb. e.\n[A]` [A][D]~ [A]\n[A][D]\n[A][C]\nc. f. [A][D] otherm d A[A][C]\nsC [A]]\nin fall [C] at the nest\n(N=35) (N=443)\nFIGURE 5. Pie diagrams of the distributions of call\nsample sizes over all diagrams is less than the total nu\nin any of the six contexts shown.)\nat approximately the same rate (same slopes i\nFig. 7). As they moved still closer, birds bega\ngiving [A][D] calls as well, until assembled at th\nspeaker site. Here they stopped giving [A] cal\naltogether (zero slope in figure) and soon afte\nstopped giving [A][D], while abruptly increasin\nthe rate of [C] calls (steep slope in Fig. 7). Whe\nplayback was terminated, calling immediately\nchanged again, with [A] calls immediately re-\nsuming (steep slope) and [C] calls becoming rar\n(changing to a noticeably\ning ceased as the birds we\nThese results were consistent with those re-\ncorded when birds were intensely disturbed by\nan observer sitting under the nest hole (Fig. 5f,\nabove), where they also gave mainly [C] calls.\nVideotapes of the owl-playback experiment of\nFigure 7 show that [C] calls were given primarily\nby perched birds that were pivoting on a branch,\nalthough they were sometimes given in flight. (It\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/termsMEXICAN CHICKADEE CALLS 79\n> 15 20\nZ 22 March [A[D 14 April [A[D]\nsite 2\nw\nuJ 10 PU- I d [C][D]\n10 ? [A], [\nFa 0o\n[C]\n1 20' '[A ] 0 [A][B][D] A [A][C]Dl ]\n0 10 20 0 10 20 30 40\nS30 15\nz 15 May [C] 15 May [C]site 1 site 2\na\nu 20 10 -\nu! I[A][C]\nF 10 ~ [A], [AI[C] & [AI[D] 5\n? - 0[A]\n0 0\n0 10 20 30 40 0 10 20\nSUCCESSIVE CALLS SUCCESSIVE CALLS\nFIGURE 6. Cumulative frequency plots of call sequence types\non 22 March) while mobbing a speaker playing back sounds\nexperiments were dominated by [A][D] calls whereas May trial\nis often difficult to tell from the videotape which\ncall is being given by which bird.) Calls contain-\ning the A note were given during approach (and\nretreat) from the site, but there is a difference\nbetween [A] and [A][D] calls. In mobbing trials,\n[A] calls were most often strings such as AA,\nAAA, and AAAA, unlike single A notes given\nin other contexts; furthermore, A-strings given\nin mobbing trials seemed sometimes to be given\nin flight. By contrast, [A][D] calls were given only\nby perched birds. These birds were moving to-\nward the site in short flights, but seemed never\nto utter any call containing a D note while ac-\ntually in flight.\nDISCUSSION\nCOMPARISONS WITH THE BLACK-CAPPED\nCHICKADEE\nThe overall structure of chick-a-dee calling in the\nMexican Chickadee is similar to that of the Black-\ncapped Chickadee (Hailman et al. 1985). Ther\nare four note types, they are combined into unit\ncalls with very short internote intervals com\npared with intervals between calls, and they oc\ncur in the invariable sequence A-B-C-D within\ncalls, where any note type may be omitted en-\ntirely, given once, or repeated a variable numbe\nof times. The note types are structurally simila\nin the two chickadees, with B and C notes bein\nnearly identical. The calls are given by both sex\nes, throughout the year, and in a wide variety of\ncontexts. In both species the [A][D] sequence typ\nis the commonest overall. Both call systems ar\nopen to new call types as the sample size increas\nes, and are the only communication systems out-\nside of human language to be proven open.\nThe chick-a-dee calls of the two species do\ndiffer, however, in many ways. The A notes o\nthe Mexican Chickadee are markedly differen\nfrom those of the Black-capped Chickadee, and\neven the D notes are dissimilar (as noted by Dix\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/terms80 M. S. FICKEN, E. D. HAILMAN AND J. P. HAILMAN\nplayback\nbirds birds moving mobbing stopped\napproaching i closer i speaker\n50\n14 April I I [C]site 1\nU\nZ 40 I\no? Ei ! [A]OII I I A\nS30 I I\n20 '\n[A][D]\n10I I I----lE\n[A][C]\n20 40 60 80 100\nSUCCESSIVE CALLS\nFIGURE 7. Cumulative frequency plots (as in Fig. 6) of a particularly long mobbing session with at l\nMexican Chickadees calling. Birds first gave [A] calls at a distance, then added [C] calls when drawin\nand finally abandoned the former entirely when near the speaker. When the owl tape was turned off\nabruptly resumed.\non and Martin 1979). In both cases, though, the\nnote types may reflect origins from a common\nancestral source: the Black-capped Chickadee's\nA note is a single chevron, the Mexican Chick-\nadee's a continuous chain of short chevrons run\ntogether (Fig. 1, above) so as to sound distinctly\nbuzzing to the human ear. Similarly, the D notes\nof both species are the longest note type, cover\na wide frequency spectrum, and may be distinctly\nbanded into emphasized frequency components\nor more uniformly distributed to compose a noisy\nnote.\nMajor differences are in duration, with the A\nnotes of the Mexican Chickadee being highly\nvariable in duration according to the other note\ntypes with which they occur (means from 120 to\n170 msec in Table 1, for A notes not contracted\nwith D's). By comparison, the A notes of the\nBlack-capped Chickadee are uniformly shorter\nat about 45 msec (Hailman et al. 1985: 195, Ta-\nble 1). Similarly, the Mexican Chickadee's un-\ncontracted D note averages 270 msec (Table 1,\nabove), whereas the Black-capped Chickadee's\nD notes average about 130 msec (Hailman et al.\n1985: 195, Table 1).\nThe rarity of B notes renders the Mexican\nChickadee's calls less diverse than those of its\ncongener, and the total note length of calls is\nmuch shorter. The more variable duration of the\nMexican Chickadee's notes may in some sense\nreplace the more variable number of note repe-\ntitions given by the Black-capped Chickadee. The\nrarity of B notes and the shorter note length of\ncalls means that the Mexican Chickadee's utter-\nances tend to be syntactically simpler, although\nnot necessarily semantically simpler (especially\nif duration of notes encodes useful information).\nGood contextual analyses of Black-capped\nChickadee calls have not been published, but\nsome comparison of mobbing situations between\nthe two species may be made preliminarily. The\nMexican Chickadee utters primarily [C] or [A][C]\ncalls when greatly disturbed or mobbing (Figs.\n5-7, above). An unpublished thesis of Apel (1985)\nshowed that Black-capped Chickadees give pri-\nmarily [D] calls or other sequence types contain-\ning D notes. Similarly, Lambrechts and J. P.\nHailman (in prep.) found that the Black-capped\nChickadees mobbing a stuffed owl near the nest\ngave elevated proportions of [D] and [A][B][D]\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/termsMEXICAN CHICKADEE CALLS 81\ncalls, with hardly any calls of any kind containing\nC notes. There are important differences in the\ndetailed contexts among these three studies, but\nthe difference in results is so striking as to suggest\na fundamentally different semantic structure in\nthe two chickadee species. By Marler's (1955)\nclassic criterion that mobbing calls should con-\nsist of short, broad frequency notes repeated, both\nchickadee species qualify. The Mexican Chick-\nadee's D notes may be too long for effectively\nrapid repetition, whereas its C note more nearly\nmeets the required characteristics. However, the\nC note of the Black-capped Chickadee is very\nsimilar to that of the Mexican Chickadee, so it\nremains unclear why the former species do not\nseem to employ C notes in its mobbing calls. In\nany case, the former impression that D and\nD-like notes were the \"mobbing calls\" of Parus\n(e.g., Thielcke 1968) is too simple a view of the\ncomplex vocalizations of tits.\nSEMANTICS\nWhat chick-a-dee calls \"mean\" (what kind of\ninformation they encode) is a formidable prob-\nlem because of the complexity of the calling sys-\ntem and the fact that chick-a-dee calls occur in\nvirtually every definable context. At one extreme\nis the possibility that chick-a-dee syntax is sim-\nply some by-product of phonation, such that all\ncalls are semantically equivalent, carrying little\nmore information than the species identity and\nposition in space of the caller. At the other ex-\ntreme is the possibility that each of the hundreds\nof call types has its own distinct meaning, like\nwords in a dictionary. Hailman et al. (1985, 1987)\nfavored an intermediate view for the Black-\ncapped Chickadee, suggesting that each note type\nmeans something different and repetitions of note\ntypes within calls serve as modifiers denoting\n\"intensity.\" The present results on the Mexican\nChickadee support this hypothesis by the find-\nings of distinctly different suites of sequence types\nin different contexts (Fig. 5, above) and details\nof calls given during mobbing situations (e.g.,\nFig. 7, above). Comparable contextual differ-\nences have yet to be reported for the Black-\ncapped Chickadee.\nEven though different suites of sequence types\ncharacterize different contexts, the fact that the\nsame note types, sequence types and call types\ncan occur in different contexts suggests that note\ntypes encode information common to a variety\nof situations. Hailman et al. (1985, 1987) sug-\ngested that this encoded information related pri-\nmarily to the position and movements of the\ncaller in space, especially relative to external ob-\njects such as conspecifics, the nest, a human ob-\nserver, or a potential predator. The present re-\nsults on the Mexican Chickadee support that view\nand give it further substance. In contexts where\nbirds are moving through the environment with\nlittle obvious disturbance except that caused by\nconspecifics, Mexican Chickadees utter primar-\nily [A] and [A][D] calls (Fig. 5a-c, above). These\nsame two sequence types characterize birds mov-\ning toward the site of an owl playback (Figs. 6\nand 7, above), and sometimes when leaving the\nsite (Fig. 7). The D note is always given while\nperched whereas the A note seems to be given\nsometimes in flight as well.\nA preliminary interpretation may therefore be\nformulated as follows. A notes indicate a rest-\nlessness (a bird ready to fly or already in flight)\nwhereas a D note indicates that the caller is\nperched and hence indexes its location in space.\nThe [A] sequence type in undisturbed birds con-\nsists mainly of single A notes, but in birds ap-\nproaching a mobbing site AA and AAA call types\nare especially common. Thus, the repetition of\nA might indicate speed of movement or high\n\"intensity\" of restlessness. The [C] calls of the\nMexican Chickadee are given most commonly\nin disturbed situations (Fig. 5d-f, above), where\n[A][C] calls are also common. Furthermore, when\nintensely mobbing an owl playback, the birds\ngave nearly pure [C] calls (Fig. 7), as they did to\nthe human observer at the nest (Fig. 5f). The C\nnote is most often given by a perched bird that\nis pivoting while it calls, but may sometimes be\ngiven in flight. In the Black-capped Chickadee a\nC note given in flight has been correlated prelim-\ninarily with a sudden swerve or change in direc-\ntion (Hailman et al. 1985), but our videotapes\ncannot document the same phenomenon for cer-\ntain in the Mexican Chickadee. That [C] calls,\nand even a few [A][C] calls, occur in relatively\nundisturbed situations (Figs. 5a-c) may indicate\nmild disturbance by conspecifics. Therefore, C\nnotes seem to indicate a disturbing stimulus and\nthe tendency to change direction, so that when\ncombined with A notes in [A][C] calls further\nindicate a tendency to move some distance, as\nwhen flying past a playback speaker or owl mod-\nel.\nIn summary, the relatively simple structure of\nMexican Chickadee calls provides a clearer pic-\nture of the information encoded by note types\nthan is currently available from data on Black-\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/terms82 M. S. FICKEN, E. D. HAILMAN AND J. P. HAILMAN\ncapped Chickadees. Basically, A notes indicate\na tendency to move some distance in space, C\nnotes denote a disturbing stimulus and a ten-\ndency to alter direction, and D notes connote a\nperched bird. (The B note of the Mexican Chick-\nadee cannot be analyzed because of its rarity).\nRepetitions of notes within calls indicate the \"in-\ntensity\" of the behavior that correlates with the\nnote types. Whether or not combinations of these\nnotes into sequence types such as [A][C], [A][D]\nor [C][D] encode something more than the con-\ncatenation of the separate meanings of the notes\nis a derivative question of considerable difficulty\nthat cannot be addressed with present data.\nACKNOWLEDGMENTS\nWe thank Sally Spofford for suggesting study sites. We\nused the facilities of the Southwest Research Station\nof the American Museum of Natural History and thank\nthe staff of that institution.\nLITERATURE CITED\nAPEL, K. M. 1985. Antipredator behavior in the Black-\ncapped Chickadee (Parus atricapillus). Ph.D.diss.,\nUniversity of Wisconsin, Milwaukee, WI.\nDIXON, K. L., AND D. J. MARTIN. 1979. Notes on the\nvocalizations of the Mexican Chickadee. Condor\n81:421-423.\nENGELS, W. R. 1988. Monte Carlo RxC contingency\ntable test. Computer software for Apple Macintosh\nwith on-line documentation. Dept. of Genetics,\nUniv. of Wisconsin, Madison, WI.\nFICKEN, M. S. 1990a. Vocal repertoire of the Mexican\nChickadee. I. Calls. J. Field Ornithol. 61:380-387.\nFICKEN, M. S. 1990b. Vocal repertoire of the Mexican\nChickadee. II. Song and song-like vocalizations.\nJ. Field Ornithol. 61:388-395.\nFICKEN, M. S., R. W. FICKEN, AND S. R. WITKIN. 1978.\nThe vocal repertoire of the Black-capped Chick-\nadee. Auk 95:34-48.\nFICKEN, M. S., AND J. NOCEDAL. 1992. Mexican\nChickadee, p. 1-11. In A. Poole, P. Stettenheim,\nand F. Gills [eds.], The birds of North America,\nno. 8. American Ornithologists' Union and Acad-\nemy of Natural Sciences, Washington, DC and\nPhiladelphia.\nHAILMAN, J. P. 1989. The organization of major vo-\ncalizations in the Paridae. Wilson Bull. 101:305-\n343.\nHAILMAN, J. P., AND M. S. FICKEN. 1986. Combi-\nnatorial animal communication with computable\nsyntax: chick-a-dee calling qualifies as 'language'\nby structural linguistics. Anim. Behav. 34:1899-\n1901.\nHAILMAN, J. P., M. S. FICKEN, AND R. W. FICKEN.\n1985. The 'chick-a-dee' calls ofParus atricapillus:\na recombinant system of animal communication\ncompared with written English. Semiotica 56:191-\n224.\nHAILMAN, J. P., M. S. FICKEN, AND R. W. FICKEN.\n1987. Constraints on the structure of combina-\ntorial \"chick-a-dee\" calls. Ethology 75:62-80.\nMANDELBROT, B. 1953. Contribution A la thborie\nmath6matique des jeux de communication. Publ.\nL'Inst. Stat. L'Univ. Paris 2:5-50.\nMARLER, P. 1955. Characteristics of some animal\ncalls. Nature 176:6-8.\nTHIELCKE, G. 1968. Gemeinsames der Gattung Par-\nus. Ein bioakustischer Beitrag zur Systematik. Vo-\ngelwelt (Beihefte) 1:147-164.\nThis content downloaded from\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n129.2.89.26 on Tue, 13 Jan 2026 19:02:43 UTC��������������\nAll use subject to https://about.jstor.org/terms", "affiliations": [{"university": "University of Wisconsin-Milwaukee", "country": "United States", "discipline": "Biology"}, {"university": "University of Wisconsin-Milwaukee", "country": "United States", "discipline": "Dairy Science"}, {"university": "University of Wisconsin-Milwaukee", "country": "United States", "discipline": "Zoology"}], "species_categories": ["Bird"], "specialized_species": ["Mexican Chickadee", "Black-capped Chickadee"], "computational_stages": ["Data Collection", "Pre-processing", "Sequence Representation", "Meaning Identification"], "linguistic_features": ["Vocal Auditory Channel and Turn-taking", "Broadcast and Direct Reception", "Reference and Displacement", "Specialization", "Arbitrariness and Duality of Patterns", "Discreteness and Syntax", "Semanticity", "Tradition and Cultural Transmission"], "status": "saved", "created_at": "2026-01-13T12:49:59.882698", "updated_at": "2026-01-13T14:03:49.395112", "committed_at": "2026-01-13T14:03:58.572177"}
{"id": "177fb068-8599-4697-9230-2fd9bfc42901", "title": "Pygmy marmosets,  Cebuella pygmaea,  modify vocal structure in response to changed social environment", "authors": ["Elowson,  A.Margaret", "Snowdon,  Charles T."], "year": "1994", "journal": "Animal Behaviour", "abstract": "", "doi": "10.1006/anbe.1994.1175", "analysis_notes": "Abstract. Pygmy marmosets displayed plasticity in vocal structure over a 5-month study of nine individuals. Changes in acoustical structure of a contact call, the trill, were investigated when two unfamiliar, captive populations of monkeys were placed together in a common acoustical environment. Monkeys representing four age categories (infant, juvenile, subadult and adult) were studied in three time blocks: 9 weeks of no acoustical interaction, the first 4 weeks after contact and 6-10 weeks after contact. Analyses of variance showed that the monkeys made coextensive shifts in two frequency measures (bandwidth and peak frequency) making parallel vocal changes rather than converging on or diverging from one another's call form. Call duration initially showed distinct population differences that disappeared after acoustic contact between the populations. Comparable analyses of trills given by pygmy marmosets that did not experience novel social companions did not show significant changes. These results suggest greater vocal plasticity across age ranges than has been hitherto described for a non-human primate and suggest the importance of social factors in vocal architecture.\n", "affiliations": [{"university": "University of Wisconsin–Madison", "country": "United States", "discipline": "Psychology"}, {"university": "", "country": "", "discipline": ""}], "species_categories": ["Primate"], "specialized_species": ["Pygmy marmoset"], "computational_stages": ["Data Collection", "Pre-processing", "Meaning Identification"], "linguistic_features": ["Vocal Auditory Channel and Turn-taking", "Semantics", "Tradition and Cultural Transmission"], "status": "saved", "created_at": "2026-01-13T12:49:59.882703", "updated_at": "2026-01-13T14:05:05.179524", "committed_at": "2026-01-13T14:05:13.799884"}
{"id": "28e069de-ac2d-4fe9-b14d-147c3ca10bc7", "title": "The faculty of language: what is it, who has it, and how did it evolve?", "authors": ["Hauser, Marc D", "Chomsky, Noam", "Fitch, W Tecumseh"], "year": "2002", "journal": "science", "abstract": "", "doi": "", "analysis_notes": "R E V I E W : N E U R O S C I E N C E\nThe Faculty of Language: What Is It, Who Has\nIt, and How Did It Evolve?\nMarc D. Hauser,1 * Noam Chomsky,2 W. Tecumseh Fitch1\nWe argue that an understanding of the faculty of language requires substantial\ninterdisciplinary cooperation. We suggest how current developments in linguistics can\nbe profitably wedded to work in evolutionary biology, anthropology, psychology, and\nneuroscience. We submit that a distinction should be made between the faculty of\nlanguage in the broad sense (FLB)and in the narrow sense (FLN). FLB includes a\nsensory-motor system, a conceptual-intentional system, and the computational\nmechanisms for recursion, providing the capacity to generate an infinite range of\nexpressions from a finite set of elements. We hypothesize that FLN only includes\nrecursion and is the only uniquely human component of the faculty of language. We\nfurther argue that FLN may have evolved for reasons other than language, hence\ncomparative studies might look for evidence of such computations outside of the\ndomain of communication (for example, number, navigation, and social relations).\nIf a martian graced our planet, it would be\nstruck by one remarkable similarity among\nEarth’s living creatures and a key difference.\nConcerning similarity, it would note that all\nliving things are de-\nsigned on the basis of\nhighly conserved de-\nvelopmental systems\nthat read an (almost)\nuniversal language en-\ncoded in DNA base\npairs. As such, life is\narranged hierarchical-\nly with a foundation\nof discrete, unblend-\nable units (codons, and,\nfor the most part,\ngenes) capable of com-\nbining to create increas-\ningly complex and vir-\ntually limitless varieties\nof both species and in-\ndividual organisms. In\ncontrast, it would notice\nthe absence of a univer-\nsal code of communi-\ncation (Fig. 1).\nIf our martian nat-\nuralist were meticu-\nlous, it might note\nthat the faculty medi-\nating human communication appears remark-\nably different from that of other living crea-\ntures; it might further note that the human\nfaculty of language appears to be organized\nlike the genetic code— hierarchical, genera-\ntive, recursive, and virtually limitless with\nrespect to its scope of expression. With these\npieces in hand, this martian might begin to\nwonder how the genetic code changed in such\na way as to generate a vast number of mutu-\nally incomprehensible communication sys-\ntems across species while maintaining clarity\nof comprehension within a given species. The\nmartian would have stumbled onto some of\nthe essential problems surrounding the\nquestion of language evolution, and of how\nhumans acquired the faculty of language.\nIn exploring the problem of language evo-\nlution, it is important to distinguish between\nquestions concerning language as a commu-\nnicative system and questions concerning the\ncomputations underlying this system, such as\nthose underlying recursion. As we argue be-\nlow, many acrimonious debates in this field\nhave been launched by a failure to distinguish\nbetween these problems. According to one\nview (1), questions concerning abstract com-\nputational mechanisms are distinct from\nthose concerning communication, the latter\ntargeted at problems at the interface between\nabstract computation and both sensory-motor\nand conceptual-intentional interfaces. This\nview should not, of course, be taken as a\nclaim against a relationship between compu-\ntation and communication. It is possible, as\nwe discuss below, that key computational\ncapacities evolved for reasons other than\ncommunication but, after they proved to have\nutility in communication, were altered be-\ncause of constraints imposed at both the pe-\nriphery (e.g., what we can hear and say or see\nand sign, the rapidity with which the auditory\ncortex can process rapid temporal and spec-\n1 Department of Psychology, Harvard University,\nCambridge, MA 02138, USA. 2 Department of Linguis-\ntics and Philosophy, Massachusetts Institute of Tech-\nnology, Cambridge, MA 02138, USA.\n*To whom correspondence should be addressed. E-\nmail: mdhauser@wjh.harvard.edu\nFig. 1. The animal kingdom has been designed on the basis of highly conserved developmental systems that read an almost\nuniversal language coded in DNA base pairs. This system is shown on the left in terms of a phylogenetic tree. In contrast, animals\nlack a common universal code of communication, indicated on the right by unconnected animal groups. [Illustration: John Yanson]\nS C I E N C E ’ S C O M P A S S ● R E V I E W\nwww.sciencemag.org SCIENCE VOL 29822 NOVEMBER 2002 1569\ntral changes) and more central levels (e.g.,\nconceptual and cognitive structures, pragmat-\nics, memory limitations).\nAt least three theoretical issues cross-cut\nthe debate on language evolution. One of the\noldest problems among theorists is the\n“shared versus unique” distinction. Most cur-\nrent commentators agree that, although bees\ndance, birds sing, and chimpanzees grunt,\nthese systems of communication differ qual-\nitatively from human language. In particular,\nanimal communication systems lack the rich\nexpressive and open-ended power of human\nlanguage (based on humans’ capacity for re-\ncursion). The evolutionary puzzle, therefore,\nlies in working out how we got from there to\nhere, given this apparent discontinuity. A sec-\nond issue revolves around whether the evo-\nlution of language was gradual versus salta-\ntional; this differs from the first issue because\na qualitative discontinuity between extant\nspecies could have evolved gradually, involv-\ning no discontinuities during human evolu-\ntion. Finally, the “continuity versus exapta-\ntion” issue revolves around the problem of\nwhether human language evolved by gradual\nextension of preexisting communication sys-\ntems, or whether important aspects of lan-\nguage have been exapted away from their\nprevious adaptive function (e.g., spatial or\nnumerical reasoning, Machiavellian social\nscheming, tool-making).\nResearchers have adopted extreme or in-\ntermediate positions regarding these basically\nindependent questions, leading to a wide\nvariety of divergent viewpoints on the evo-\nlution of language in the current literature.\nThere is, however, an emerging consensus\nthat, although humans and animals share a\ndiversity of important computational and\nperceptual resources, there has been sub-\nstantial evolutionary remodeling since we\ndiverged from a common ancestor some 6\nmillion years ago. The empirical challenge\nis to determine what was inherited un-\nchanged from this common ancestor, what\nhas been subjected to minor modifications,\nand what (if anything) is qualitatively new.\nThe additional evolutionary challenge is to\ndetermine what selectional pressures led to\nadaptive changes over time and to under-\nstand the various constraints that channeled\nthis evolutionary process. Answering these\nquestions requires a collaborative effort\namong linguists, biologists, psychologists,\nand anthropologists.\nOne aim of this essay is to promote a\nstronger connection between biology and\nlinguistics by identifying points of contact\nand agreement between the fields. Al-\nthough this interdisciplinary marriage was\ninaugurated more than 50 years ago, it has\nnot yet been fully consummated. We hope\nto further this goal by, first, helping to\nclarify the biolinguistic perspective on lan-\nguage and its evolution (2–7). We then\nreview some promising empirical ap-\nproaches to the evolution of the language\nfaculty, with a special focus on\ncomparative work with non-\nhuman animals, and conclude\nwith a discussion of how in-\nquiry might profitably advance,\nhighlighting some outstanding\nproblems.\nWe make no attempt to be\ncomprehensive in our coverage of\nrelevant or interesting topics and\nproblems. Nor is it our goal to\nreview the history of the field.\nRather, we focus on topics that\nmake important contact between\nempirical data and theoretical po-\nsitions about the nature of the lan-\nguage faculty. We believe that if\nexplorations into the problem of\nlanguage evolution are to progress,\nwe need a clear explication of the\ncomputational requirements for\nlanguage, the role of evolutionary\ntheory in testing hypotheses of\ncharacter evolution, and a research\nprogram that will enable a produc-\ntive interchange between linguists\nand biologists.\nDefining the Target: Two\nSenses of the Faculty of\nLanguage\nThe word “language” has highly divergent\nmeanings in different contexts and disci-\nplines. In informal usage, a language is un-\nderstood as a culturally specific communica-\ntion system (English, Navajo, etc.). In the\nvarieties of modern linguistics that concern\nus here, the term “language” is used quite\ndifferently to refer to an internal component\nof the mind/brain (sometimes called “internal\nlanguage” or “I-language”). We assume that\nthis is the primary object of interest for the\nstudy of the evolution and function of the\nlanguage faculty. However, this biologically\nand individually grounded usage still leaves\nmuch open to interpretation (and misunder-\nstanding). For example, a neuroscientist\nmight ask: What components of the human\nnervous system are recruited in the use of\nlanguage in its broadest sense? Because any\naspect of cognition appears to be, at least in\nprinciple, accessible to language, the broadest\nanswer to this question is, probably, “most of\nit.” Even aspects of emotion or cognition not\nreadily verbalized may be influenced by lin-\nguistically based thought processes. Thus,\nthis conception is too broad to be of much\nuse. We therefore delineate two more restrict-\ned conceptions of the faculty of language, one\nbroader and more inclusive, the other more\nrestricted and narrow (Fig. 2).\nFaculty of language— broad sense\n(FLB). FLB includes an internal computa-\ntional system (FLN, below) combined with\nat least two other organism-internal sys-\nFig. 2. A schematic representation of organism-external and -internal factors related to the faculty of language.\nFLB includes sensory-motor, conceptual-intentional, and other possible systems (which we leave open); FLN\nincludes the core grammatical computations that we suggest are limited to recursion. See text for more\ncomplete discussion.\nS C I E N C E ’ S C O M P A S S\n22 NOVEMBER 2002 VOL 298SCIENCE www.sciencemag.org1570\ntems, which we call “sensory-motor” and\n“conceptual-intentional.” Despite debate on\nthe precise nature of these systems, and\nabout whether they are substantially shared\nwith other vertebrates or uniquely adapted\nto the exigencies of language, we take as\nuncontroversial the existence of some bio-\nlogical capacity of humans that allows us\n(and not, for example, chimpanzees) to\nreadily master any human language without\nexplicit instruction. FLB includes this ca-\npacity, but excludes other organism-\ninternal systems that are necessary but not\nsufficient for language (e.g., memory, res-\npiration, digestion, circulation, etc.).\nFaculty of language—narrow sense\n(FLN). FLN is the abstract linguistic compu-\ntational system alone, independent of the oth-\ner systems with which it interacts and inter-\nfaces. FLN is a component of FLB, and the\nmechanisms underlying it are some subset of\nthose underlying FLB.\nOthers have agreed on the need for a\nrestricted sense of “language” but have sug-\ngested different delineations. For example,\nLiberman and his associates (8) have argued\nthat the sensory-motor systems were specifi-\ncally adapted for language, and hence should\nbe considered part of FLN. There is also a\nlong tradition holding that the conceptual-\nintentional systems are an intrinsic part of\nlanguage in a narrow sense. In this article, we\nleave these questions open, restricting atten-\ntion to FLN as just defined but leaving the\npossibility of a more inclusive definition\nopen to further empirical research.\nThe internal architecture of FLN, so con-\nceived, is a topic of much current research\nand debate (4). Without prejudging the is-\nsues, we will, for concreteness, adopt a par-\nticular conception of this architecture. We\nassume, putting aside the precise mecha-\nnisms, that a key component of FLN is a\ncomputational system (narrow syntax) that\ngenerates internal representations and maps\nthem into the sensory-motor interface by the\nphonological system, and into the conceptu-\nal-intentional interface by the (formal) se-\nmantic system; adopting alternatives that\nhave been proposed would not materially\nmodify the ensuing discussion. All approach-\nes agree that a core property of FLN is recur-\nsion, attributed to narrow syntax in the con-\nception just outlined. FLN takes a finite set of\nelements and yields a potentially infinite ar-\nray of discrete expressions. This capacity of\nFLN yields discrete infinity (a property that\nalso characterizes the natural numbers). Each\nof these discrete expressions is then passed to\nthe sensory-motor and conceptual-intentional\nsystems, which process and elaborate this\ninformation in the use of language. Each\nexpression is, in this sense, a pairing of sound\nand meaning. It has been recognized for thou-\nsands of years that language is, fundamental-\nly, a system of sound-meaning connections;\nthe potential infiniteness of this system has\nbeen explicitly recognized by Galileo, Des-\ncartes, and the 17th-century “philosophical\ngrammarians” and their successors, notably\nvon Humboldt. One goal of the study of FLN\nand, more broadly, FLB is to discover just\nhow the faculty of language satisfies these\nbasic and essential conditions.\nThe core property of discrete infinity is\nintuitively familiar to every language user.\nSentences are built up of discrete units: There\nare 6-word sentences and 7-word sentences,\nbut no 6.5-word sentences. There is no long-\nest sentence (any candidate sentence can be\ntrumped by, for example, embedding it in\n“Mary thinks that . . .”), and there is no non-\narbitrary upper bound to sentence length. In\nthese respects, language is directly analogous\nto the natural numbers (see below).\nAt a minimum, then, FLN includes the ca-\npacity of recursion. There are many organism-\ninternal factors, outside FLN or FLB, that im-\npose practical limits on the usage of the system.\nFor example, lung capacity imposes limits on\nthe length of actual spoken sentences, whereas\nworking memory imposes limits on the com-\nplexity of sentences if they are to be under-\nstandable. Other limitations—for example, on\nconcept formation or motor output speed—\nrepresent aspects of FLB, which have their own\nevolutionary histories and may have played a\nrole in the evolution of the capacities of FLN.\nNonetheless, one can profitably inquire into the\nevolution of FLN without an\nimmediate concern for these\nlimiting aspects of FLB. This\nis made clear by the observa-\ntion that, although many\naspects of FLB are shared\nwith other vertebrates, the\ncore recursive aspect of FLN\ncurrently appears to lack any\nanalog in animal communi-\ncation and possibly other do-\nmains as well. This point,\ntherefore, represents the\ndeepest challenge for a com-\nparative evolutionary ap-\nproach to language. We be-\nlieve that investigations of\nthis capacity should include\ndomains other than commu-\nnication (e.g., number, social\nrelationships, navigation).\nGiven the distinctions\nbetween FLB and FLN and\nthe theoretical distinctions\nraised above, we can define\na research space as sketched\nin Fig. 3. This research\nspace identifies, as viable,\nproblems concerning the\nevolution of sensory-motor\nsystems, of conceptual-in-\ntentional systems, and of\nFLN. The comparative ap-\nproach, to which we turn\nnext, provides a framework\nfor addressing questions\nabout each of these com-\nponents of the faculty of\nlanguage.\nThe Comparative\nApproach to Language\nEvolution\nThe empirical study of the\nevolution of language is be-\nset with difficulties. Lin-\nguistic behavior does not\nfossilize, and a long tradi-\nFig. 3. Investigations into the evolution of the faculty of language\nare confronted with a three-dimensional research space that\nincludes three comparative-evolutionary problems cross-cut by\nthe core components of the faculty of language. Thus, for each\nproblem, researchers can investigate details of the sensory-motor\nsystem, the conceptual-intentional system, FLN, and the interfac-\nes among these systems.\nS C I E N C E ’ S C O M P A S S\nwww.sciencemag.org SCIENCE VOL 29822 NOVEMBER 2002 1571\ntion of analysis of fossil skull shape and\ncranial endocasts has led to little consensus\nabout the evolution of language (7, 9). A\nmore tractable and, we think, powerful ap-\nproach to problems of language evolution is\nprovided by the comparative method, which\nuses empirical data from living species to\ndraw detailed inferences about extinct ances-\ntors (3, 10 –12). The comparative method was\nthe primary tool used by Darwin (13, 14) to\nanalyze evolutionary phenomena and contin-\nues to play a central role throughout modern\nevolutionary biology. Although scholars in-\nterested in language evolution have often ig-\nnored comparative data altogether or focused\nnarrowly on data from nonhuman primates,\ncurrent thinking in neuroscience, molecular\nbiology, and developmental biology indicates\nthat many aspects of neural and developmen-\ntal function are highly conserved, encourag-\ning the extension of the comparative method\nto all vertebrates (and perhaps beyond). For\nseveral reasons, detailed below, we believe\nthat the comparative method should play a\nmore central role in future discussions of\nlanguage evolution.\nAn overarching concern in studies of lan-\nguage evolution is with whether particular\ncomponents of the faculty of language\nevolved specifically for human language and,\ntherefore (by extension), are unique to hu-\nmans. Logically, the human uniqueness claim\nmust be based on data indicating an absence\nof the trait in nonhuman animals and, to be\ntaken seriously, requires a substantial body of\nrelevant comparative data. More concretely,\nif the language evolution researcher wishes to\nmake the claim that a trait evolved uniquely\nin humans for the function of language pro-\ncessing, data indicating that no other animal\nhas this particular trait are required.\nAlthough this line of reasoning may ap-\npear obvious, it is surprisingly common for a\ntrait to be held up as uniquely human before\nany appropriate comparative data are avail-\nable. A famous example is categorical per-\nception, which when discovered seemed so\nfinely tuned to the details of human speech as\nto constitute a unique human adaptation (15,\n16). It was some time before the same under-\nlying perceptual discontinuities were discov-\nered in chinchillas and macaques (17, 18),\nand even birds (19), leading to the opposite\nconclusion that the perceptual basis for cate-\ngorical perception is a primitive vertebrate\ncharacteristic that evolved for general audito-\nry processing, as opposed to specific speech\nprocessing. Thus, a basic and logically in-\neliminable role for comparative research on\nlanguage evolution is this simple and essen-\ntially negative one: A trait present in nonhu-\nman animals did not evolve specifically for\nhuman language, although it may be part of\nthe language faculty and play an intimate role\nin language processing. It is possible, of\ncourse, that a trait evolved in nonhuman an-\nimals and humans independently, as analogs\nrather than homologs. This would preserve\nthe possibility that the trait evolved for lan-\nguage in humans but evolved for some other\nreason in the comparative animal group. In\ncases where the comparative group is a non-\nhuman primate, and perhaps especially chim-\npanzees, the plausibility of this evolutionary\nscenario is weaker. In any case, comparative\ndata are critical to this judgment.\nDespite the crucial role of homology in\ncomparative biology, homologous traits are not\nthe only relevant source of evolutionary data.\nThe convergent evolution of similar characters\nin two independent clades, termed “analogies”\nor “homoplasies,” can be equally revealing\n(20). The remarkably similar (but nonhomolo-\ngous) structures of human and octopus eyes\nreveal the stringent constraints placed by the\nlaws of optics and the contingencies of devel-\nopment on an organ capable of focusing a sharp\nimage onto a sheet of receptors. Detailed anal-\nogies between the parts of the vertebrate and\ncephalopod eye also provide independent evi-\ndence that each component is an adaptation for\nimage formation, shaped by natural selection.\nFurthermore, the discovery that remarkably\nconservative genetic cascades underlie the de-\nvelopment of such analogous structures pro-\nvides important insights into the ways in\nwhich developmental mechanisms can\nchannel evolution (21). Thus, although po-\ntentially misleading for taxonomists, anal-\nogies provide critical data about adaptation\nunder physical and developmental con-\nstraints. Casting the comparative net more\nbroadly, therefore, will most likely reveal\nlarger regularities in evolution, helping to\naddress the role of such constraints in the\nevolution of language.\nAn analogy recognized as particularly rele-\nvant to language is the acquisition of song by\nbirds (12). In contrast to nonhuman primates,\nwhere the production of species-typical vocal-\nizations is largely innate (22), most songbirds\nlearn their species-specific song by listening to\nconspecifics, and they develop highly aberrant\nsong if deprived of such experience. Current\ninvestigation of birdsong reveals detailed and\nintriguing parallels with speech (11, 23, 24).\nFor instance, many songbirds pass through a\ncritical period in development beyond which\nthey produce defective songs that no amount of\nacoustic input can remedy, reminiscent of the\ndifficulty adult humans have in fully mastering\nnew languages. Further, and in parallel with the\nbabbling phase of vocalizing or signing human\ninfants (25), young birds pass through a phase\nof song development in which they spontane-\nously produce amorphous versions of adult\nsong, termed “subsong” or “babbling.” Al-\nthough the mechanisms underlying the acquisi-\ntion of birdsong and human language are clear-\nly analogs and not homologs, their core com-\nponents share a deeply conserved neural and\ndevelopmental foundation: Most aspects of\nneurophysiology and development—including\nregulatory and structural genes, as well as neu-\nron types and neurotransmitters—are shared\namong vertebrates. That such close parallels\nhave evolved suggests the existence of impor-\ntant constraints on how vertebrate brains can\nacquire large vocabularies of complex, learned\nsounds. Such constraints may essentially force\nnatural selection to come up with the same\nsolution repeatedly when confronted with sim-\nilar problems.\nTesting Hypotheses About the\nEvolution of the Faculty of Language\nGiven the definitions of the faculty of lan-\nguage, together with the comparative frame-\nwork, we can distinguish several plausible\nhypotheses about the evolution of its various\ncomponents. Here, we suggest two hypothe-\nses that span the diversity of opinion among\ncurrent scholars, plus a third of our own.\nHypothesis 1: FLB is strictly homologous\nto animal communication. This hypothesis\nholds that homologs of FLB, including FLN,\nexist ( perhaps in less developed or otherwise\nmodified form) in nonhuman animals (3, 10,\n26). This has historically been a popular hy-\npothesis outside of linguistics and closely\nallied fields, and has been defended by some\nin the speech sciences. According to this\nhypothesis, human FLB is composed of the\nsame functional components that underlie\ncommunication in other species.\nHypothesis 2: FLB is a derived, uniquely\nhuman adaptation for language. According\nto this hypothesis, FLB is a highly complex\nadaptation for language, on a par with the\nvertebrate eye, and many of its core compo-\nnents can be viewed as individual traits that\nhave been subjected to selection and perfect-\ned in recent human evolutionary history. This\nappears to represent the null hypothesis for\nmany scholars who take the complexity of\nlanguage seriously (27, 28). The argument\nstarts with the assumption that FLB, as a\nwhole, is highly complex, serves the function\nof communication with admirable effective-\nness, and has an ineliminable genetic compo-\nnent. Because natural selection is the only\nknown biological mechanism capable of gen-\nerating such functional complexes [the argu-\nment from design (29)], proponents of this\nview conclude that natural selection has\nplayed a powerful role in shaping many as-\npects of FLB, including FLN, and, further,\nthat many of these are without parallel in\nnonhuman animals. Although homologous\nmechanisms may exist in other animals, the\nhuman versions have been modified by nat-\nural selection to the extent that they can be\nreasonably seen as constituting novel traits,\nperhaps exapted from other contexts [e.g.,\nsocial intelligence, tool-making (7, 30 –32)].\nS C I E N C E ’ S C O M P A S S\n22 NOVEMBER 2002 VOL 298SCIENCE www.sciencemag.org1572\nHypothesis 3: Only FLN is uniquely human.\nOn the basis of data reviewed below, we hy-\npothesize that most, if not all, of FLB is based\non mechanisms shared with nonhuman animals\n(as held by hypothesis 1). In contrast, we sug-\ngest that FLN—the computational mechanism\nof recursion—is recently evolved and unique to\nour species (33, 34). According to this hypoth-\nesis, much of the complexity manifested in\nlanguage derives from complexity in the pe-\nripheral components of FLB, especially those\nunderlying the sensory-motor (speech or sign)\nand conceptual-intentional interfaces, com-\nbined with sociocultural and communicative\ncontingencies. FLB as a whole thus has an\nancient evolutionary history, long predating the\nemergence of language, and a comparative\nanalysis is necessary to understand this com-\nplex system. By contrast, according to recent\nlinguistic theory, the computations underlying\nFLN may be quite limited. In fact, we propose\nin this hypothesis that FLN comprises only the\ncore computational mechanisms of recursion as\nthey appear in narrow syntax and the mappings\nto the interfaces. If FLN is indeed this restrict-\ned, this hypothesis has the interesting effect of\nnullifying the argument from design, and thus\nrendering the status of FLN as an adaptation\nopen to question. Proponents of the idea that\nFLN is an adaptation would thus need to supply\nadditional data or arguments to support this\nviewpoint.\nThe available comparative data on animal\ncommunication systems suggest that the faculty\nof language as a whole relies on some uniquely\nhuman capacities that have evolved recently in\nthe approximately 6 million years since our\ndivergence from a chimpanzee-like common\nancestor (35). Hypothesis 3, in its strongest\nform, suggests that only FLN falls into this\ncategory (34). By this hypothesis, FLB contains\na wide variety of cognitive and perceptual\nmechanisms shared with other species, but only\nthose mechanisms underlying FLN—particu-\nlarly its capacity for discrete infinity—are\nuniquely human. This hypothesis suggests that\nall peripheral components of FLB are shared\nwith other animals, in more or less the same\nform as they exist in humans, with differences\nof quantity rather than kind (9, 34). What is\nunique to our species is quite specific to FLN,\nand includes its internal operations as well as its\ninterface with the other organism-internal sys-\ntems of FLB.\nEach of these hypotheses is plausible to\nsome degree. Ultimately, they can be distin-\nguished only by empirical data, much of which\nis currently unavailable. Before reviewing some\nof the relevant data, we briefly consider some\nkey distinctions between them. From a compar-\native evolutionary viewpoint, an important\nquestion is whether linguistic precursors were\ninvolved in communication or in something\nelse. Proponents of both hypotheses 1 and 2\nposit a direct correspondence, by descent with\nmodification, between some trait involved in\nFLB in humans and a similar trait in another\nspecies; these hypotheses differ in whether\nthe precursors functioned in communication.\nTable 1. A sampler of empirical approaches to understanding the evolution of the faculty of language, including both broad (FLB) and narrow (FLN)\ncomponents.\nEmpirical problem Examples References\nFLB—sensory-motor system\nVocal imitation and invention Tutoring studies of songbirds, analyses of vocal dialects in whales, spontaneous imitation\nof artificially created sounds in dolphins\n(11, 12, 24, 65)\nNeurophysiology of\naction-perception systems\nStudies assessing whether mirror neurons, which provide a core substrate for the\naction-perception system, may subserve gestural and ( possibly) vocal imitation\n(67, 68, 71)\nDiscriminating the sound patterns\nof language\nOperant conditioning studies of the prototype magnet effect in macaques and starlings (52, 120)\nConstraints imposed by vocal tract\nanatomy\nStudies of vocal tract length and formant dispersion in birds and primates (54–61)\nBiomechanics of sound production Studies of primate vocal production, including the role of mandibular oscillations (121, 122)\nModalities of language production\nand perception\nCross-modal perception and sign language in humans versus unimodal communication in\nanimals\n(3, 25, 123)\nFLB—conceptual-intentional system\nTheory of mind, attribution of\nmental states\nStudies of the seeing/knowing distinction in chimpanzees (84, 86–89)\nCapacity to acquire nonlinguistic\nconceptual representations\nStudies of rhesus monkeys and the object/kind concept (10, 76, 77, 124)\nReferential vocal signals Studies of primate vocalizations used to designate predators, food, and social\nrelationships\n(3, 78, 90, 91, 93,\n94, 97)\nImitation as a rational, intentional\nsystem\nComparative studies of chimpanzees and human infants suggesting that only the latter\nread intentionality into action, and thus extract unobserved rational intent\n(125–127)\nVoluntary control over signal\nproduction as evidence of\nintentional communication\nComparative studies that explore the relationship between signal production and the\ncomposition of a social audience\n(3, 10, 92, 128)\nFLN—recursion\nSpontaneous and training methods\ndesigned to uncover constraints\non rule learning\nStudies of serial order learning and finite-state grammars in tamarins and macaques (114, 116, 117,\n129)\nSign or artificial language in\ntrained apes and dolphins\nStudies exploring symbol sequencing and open-ended combinatorial manipulation (130, 131)\nModels of the faculty of language\nthat attempt to uncover the\nnecessary and sufficient\nmechanisms\nGame theory models of language acquisition, reference, and universal grammar (72–74)\nExperiments with animals that\nexplore the nature and content\nof number representation\nOperant conditioning studies to determine whether nonhuman primates can represent\nnumber, including properties such as ordinality and cardinality, using such\nrepresentations in conjunction with mathematical operands (e.g., add, divide)\n(102–106, 132)\nShared mechanisms across\ndifferent cognitive domains\nEvolution of musical processing and structure, including analyses of brain function and\ncomparative studies of music perception\n(133–135)\nS C I E N C E ’ S C O M P A S S\nwww.sciencemag.org SCIENCE VOL 29822 NOVEMBER 2002 1573\nAlthough many aspects of FLB very likely\narose in this manner, the important issue for\nthese hypotheses is whether a series of gradual\nmodifications could lead eventually to the ca-\npacity of language for infinite generativity. De-\nspite the inarguable existence of a broadly\nshared base of homologous mechanisms in-\nvolved in FLB, minor modifications to this\nfoundational system alone seem inadequate to\ngenerate the fundamental difference—discrete\ninfinity—between language and all known\nforms of animal communication. This claim is\none of several reasons why we suspect that\nhypothesis 3 may be a productive way to char-\nacterize the problem of language evolution.\nA primary issue separating hypotheses 2\nand 3 is whether the uniquely human capac-\nities of FLN constitute an adaptation. The\nviewpoint stated in hypothesis 2, especially\nthe notion that FLN in particular is a highly\nevolved adaptation, has generated much en-\nthusiasm recently [e.g., (36)], especially\namong evolutionary psychologists (37, 38).\nAt present, however, we see little reason to\nbelieve either that FLN can be anatomized\ninto many independent but interacting traits,\neach with its own independent evolutionary\nhistory, or that each of these traits could have\nbeen strongly shaped by natural selection,\ngiven their tenuous connection to communi-\ncative efficacy (the surface or phenotypic\nfunction upon which selection presumably\nacted).\nWe consider the possibility that certain spe-\ncific aspects of the faculty of language are\n“spandrels”—by-products of preexisting con-\nstraints rather than end products of a history of\nnatural selection (39). This possibility, which\nopens the door to other empirical lines of inqui-\nry, is perfectly compatible with our firm support\nof the adaptationist program. Indeed, it follows\ndirectly from the foundational notion that adap-\ntation is an “onerous concept” to be invoked\nonly when alternative explanations fail (40).\nThe question is not whether FLN in toto is\nadaptive. By allowing us to communicate an\nendless variety of thoughts, recursion is clearly\nan adaptive computation. The question is\nwhether particular components of the function-\ning of FLN are adaptations for language, spe-\ncifically acted upon by natural selection—or,\neven more broadly, whether FLN evolved for\nreasons other than communication.\nAn analogy may make this distinction\nclear. The trunk and branches of trees are\nnear-optimal solutions for providing an indi-\nvidual tree’s leaves with access to sunlight.\nFor shrubs and small trees, a wide variety of\nforms (spreading, spherical, multistalked,\netc.) provide good solutions to this problem.\nFor a towering rainforest canopy tree, how-\never, most of these forms are rendered im-\npossible by the various constraints imposed\nby the properties of cellulose and the prob-\nlems of sucking water and nutrients up to the\nleaves high in the air. Some aspects of such\ntrees are clearly adaptations channeled by\nthese constraints; others (e.g., the popping of\nxylem tubes on hot days, the propensity to be\ntoppled in hurricanes) are presumably un-\navoidable by-products of such constraints.\nRecent work on FLN (4, 41–43) suggests\nthe possibility that at least the narrow-syntactic\ncomponent satisfies conditions of highly effi-\ncient computation to an extent previously unsus-\npected. Thus, FLN may approximate a kind of\n“optimal solution” to the problem of linking\nthe sensory-motor and conceptual-intentional\nsystems. In other words, the generative process-\nes of the language system may provide a\nnear-optimal solution that satisfies the interface\nconditions to FLB. Many of the details of lan-\nguage that are the traditional focus of linguistic\nstudy [e.g., subjacency, Wh- movement, the\nexistence of garden-path sentences (4, 44)] may\nrepresent by-products of this solution, gener-\nated automatically by neural/computational\nconstraints and the structure of FLB—\ncomponents that lie outside of FLN. Even\nnovel capacities such as recursion are imple-\nmented in the same type of neural tissue as the\nrest of the brain and are thus constrained by\nbiophysical, developmental, and computation-\nal factors shared with other vertebrates. Hy-\npothesis 3 raises the possibility that structural\ndetails of FLN may result from such preexisting\nconstraints, rather than from direct shaping by\nnatural selection targeted specifically at com-\nmunication. Insofar as this proves to be true,\nsuch structural details are not, strictly speaking,\nadaptations at all. This hypothesis and the\nalternative selectionist account are both viable\nand can eventually be tested with comparative\ndata.\nComparative Evidence for the Faculty\nof Language\nStudy of the evolution of language has accel-\nerated in the past decade (45, 46). Here, we\noffer a highly selective review of some of\nthese studies, emphasizing animal work that\nseems particularly relevant to the hypotheses\nadvanced above; many omissions were nec-\nessary for reasons of space, and we firmly\nbelieve that a broad diversity of methods and\nperspectives will ultimately provide the rich-\nest answers to the problem of language evo-\nlution. For this reason, we present a broader\nsampler of the field’s offerings in Table 1.\nHow “special” is speech? Comparative\nstudy of the sensory-motor system. Starting with\nearly work on speech perception, there has been\na tradition of considering speech “special,” and\nthus based on uniquely human mechanisms\nadapted for speech perception and/or produc-\ntion [e.g., (7, 8, 47, 48)]. This perspective has\nstimulated a vigorous research program study-\ning animal speech perception and, more recent-\nly, speech production. Surprisingly, this re-\nsearch has turned up little evidence for uniquely\nhuman mechanisms special to speech, despite a\npersistent tendency to assume uniqueness even\nin the absence of relevant animal data.\nOn the side of perception, for example,\nmany species show an impressive ability to\nboth discriminate between and generalize\nover human speech sounds, using formants as\nthe critical discriminative cue (17–19, 49 –\n51). These data provide evidence not only of\ncategorical perception, but also of the ability\nto discriminate among prototypical exem-\nplars of different phonemes (52). Further, in\nthe absence of training, nonhuman primates\ncan discriminate sentences from two different\nlanguages on the basis of rhythmic differenc-\nes between them (53).\nOn the side of production, birds and non-\nhuman primates naturally produce and per-\nceive formants in their own species-typical\nvocalizations (54 –59). The results also shed\nlight on discussions of the uniquely human\nstructure of the vocal tract and the unusual\ndescended larynx of our species (7, 48, 60),\nbecause new evidence shows that several oth-\ner mammalian species also have a descended\nlarynx (61). Because these nonhuman species\nlack speech, a descended larynx clearly has\nnonphonetic functions; one possibility is ex-\naggerating apparent size. Although this par-\nticular anatomical modification undoubtedly\nplays an important role in speech production\nin modern humans, it need not have first\nevolved for this function. The descended lar-\nynx may thus be an example of classic Dar-\nwinian preadaptation.\nMany phenomena in human speech percep-\ntion have not yet been investigated in animals\n[e.g., the McGurk effect, an illusion in which\nthe syllable perceived from a talking head rep-\nresents the interaction between an articulatory\ngesture seen and a different syllable heard; see\n(62)]. However, the available data suggest a\nmuch stronger continuity between animals and\nhumans with respect to speech than previously\nbelieved. We argue that the continuity hypoth-\nesis thus deserves the status of a null hypothe-\nsis, which must be rejected by comparative\nwork before any claims of uniqueness can be\nvalidated. For now, this null hypothesis of no\ntruly novel traits in the speech domain appears\nto stand.\nThere is, however, a striking ability tied to\nspeech that has received insufficient atten-\ntion: the human capacity for vocal imitation\n(63, 64). Imitation is obviously a necessary\ncomponent of the human capacity to acquire\na shared and arbitrary lexicon, which is itself\ncentral to the language capacity. Thus, the\ncapacity to imitate was a crucial prerequisite\nof FLB as a communicative system. Vocal\nimitation and learning are not uniquely hu-\nman. Rich multimodal imitative capacities\nare seen in other mammals (dolphins) and\nsome birds ( parrots), with most songbirds\nexhibiting a well-developed vocal imitative\nS C I E N C E ’ S C O M P A S S\n22 NOVEMBER 2002 VOL 298SCIENCE www.sciencemag.org1574\ncapacity (65). What is surprising is that mon-\nkeys show almost no evidence of visually\nmediated imitation, with chimpanzees show-\ning only slightly better capacities (66). Even\nmore striking is the virtual absence of evi-\ndence for vocal imitation in either monkeys\nor apes (3). For example, intensively trained\nchimpanzees are incapable of acquiring any-\nthing but a few poorly articulated spoken\nwords, whereas parrots can readily acquire a\nlarge vocal repertoire. With respect to their\nown vocalizations, there are few convincing\nstudies of vocal dialects in primates, thereby\nsuggesting that they lack a vocal imitative\ncapacity (3, 65). Evidence for spontaneous\nvisuomanual imitation in chimpanzees is not\nmuch stronger, although with persistent train-\ning they can learn several hundred hand\nsigns. Further, even in cases where\nnonhuman animals are capable of im-\nitating in one modality (e.g., song\ncopying in songbirds), only dolphins\nand humans appear capable of imita-\ntion in multiple modalities. The de-\ntachment from modality-specific in-\nputs may represent a substantial\nchange in neural organization, one\nthat affects not only imitation but\nalso communication; only humans\ncan lose one modality (e.g., hear-\ning) and make up for this deficit by\ncommunicating with complete com-\npetence in a different modality (i.e.,\nsigning).\nOur discussion of limitations is\nnot meant to diminish the impressive\nachievements of monkeys and apes,\nbut to highlight how different the\nmechanisms underlying the produc-\ntion of human and nonhuman primate\ngestures, either vocally expressed or\nsigned, must be. After all, the aver-\nage high school graduate knows up to\n60,000 words, a vocabulary achieved\nwith little effort, especially when\ncontrasted with the herculean efforts\ndevoted to training animals. In sum,\nthe impressive ability of any normal\nhuman child for vocal imitation may\nrepresent a novel capacity that\nevolved in our recent evolutionary\nhistory, some time after the diver-\ngence from our chimpanzee-like an-\ncestors. The existence of analogs in\ndistantly related species, such as\nbirds and cetaceans, suggests consid-\nerable potential for the detailed com-\nparative study of vocal imitation. There are,\nhowever, potential traps that must be avoid-\ned, especially with respect to explorations of\nthe neurobiological substrates of imitation.\nFor example, although macaque monkeys\nand humans are equipped with so-called\n“mirror neurons” in the premotor cortex that\nrespond both when an individual acts in a\nparticular way and when the same individual\nsees someone else act in this same way (67,\n68), these neurons are not sufficient for imi-\ntation in macaques, as many have presumed:\nAs mentioned, there is no convincing evi-\ndence of vocal or visual imitation in mon-\nkeys. Consequently, as neuroimaging studies\ncontinue to explore the neural basis of imita-\ntion in humans (69 –71), it will be important\nto distinguish between the necessary and suf-\nficient neural correlates of imitation. This is\nespecially important, given that some recent\nattempts to model the evolution of language\nbegin with a hypothetical organism that is\nequipped with the capacity for imitation and\nintentionality, as opposed to working out how\nthese mechanisms evolved in the first place\n[see below; (72–74)]. If a deeper evolution-\nary exploration is desired, one dating back to\na chimpanzee-like ancestor, then we need to\nexplain how and why such capacities\nemerged from an ancestral node that lacked\nsuch abilities (75) (Fig. 4).\nThe conceptual-intentional systems of non-\nlinguistic animals. A wide variety of studies\nindicate that nonhuman mammals and birds\nhave rich conceptual representations (76, 77).\nSurprisingly, however, there is a mismatch be-\ntween the conceptual capacities of animals and\nthe communicative content of their vocal and\nvisual signals (78, 79). For example, although a\nwide variety of nonhuman primates have access\nto rich knowledge of who is related to whom, as\nwell as who is dominant and who is subordi-\nnate, their vocalizations only coarsely express\nsuch complexities.\nStudies using classical training approach-\nes as well as methods that tap spontaneous\nabilities reveal that animals acquire and use a\nwide range of abstract concepts, including\ntool, color, geometric relationships, food, and\nnumber (66, 76 – 82). More controversially,\nbut of considerable relevance to intentional\naspects of language and conditions of felici-\ntous use, some studies claim that animals\nhave a theory of mind (83– 85), including a\nsense of self and the ability to represent the\nbeliefs and desires of other group members.\nOn the side of positive support, recent studies\nof chimpanzees suggest that they recognize\nthe perceptual act of seeing as a proxy for the\nmental state of knowing (84, 86, 87). These\nFig. 4. The distribution of imitation in the animal kingdom is patchy. Some animals such as songbirds,\ndolphins, and humans have evolved exceptional abilities to imitate; other animals, such as apes and\nmonkeys, either lack such abilities or have them in a relatively impoverished form. [Illustration: John Yanson]\nS C I E N C E ’ S C O M P A S S\nwww.sciencemag.org SCIENCE VOL 29822 NOVEMBER 2002 1575\nstudies suggest that at least chimpanzees, but\nperhaps no other nonhuman animals, have a\nrudimentary theory of mind. On the side of\nnegative support, other studies suggest that\neven chimpanzees lack a theory of mind,\nfailing, for example, to differentiate between\nignorant and knowledgeable individuals with\nrespect to intentional communication (88,\n89). Because these experiments make use of\ndifferent methods and are based on small\nsample sizes, it is not possible at present to\nderive any firm conclusions about the pres-\nence or absence of mental state attribution in\nanimals. Independently of how this contro-\nversy is resolved, however, the best evidence\nof referential communication in animals\ncomes not from chimpanzees but from a va-\nriety of monkeys and birds, species for which\nthere is no convincing evidence for a theory\nof mind.\nThe classic studies of vervet monkey\nalarm calls (90) have now been joined by\nseveral others, each using comparable meth-\nods, with extensions to different species (ma-\ncaques, Diana monkeys, meerkats, prairie\ndogs, chickens) and different communicative\ncontexts (social relationships, food, inter-\ngroup aggression) (91–97). From these stud-\nies we can derive five key points relevant to\nour analysis of the faculty of language. First,\nindividuals produce acoustically distinctive\ncalls in response to functionally important\ncontexts, including the detection of predators\nand the discovery of food. Second, the acous-\ntic morphology of the signal, although arbi-\ntrary in terms of its association with a partic-\nular context, is sufficient to enable listeners to\nrespond appropriately without requiring any\nother contextual information. Third, the num-\nber of such signals in the repertoire is small,\nrestricted to objects and events experienced\nin the present, with no evidence of creative\nproduction of new sounds for new situations.\nFourth, the acoustic morphology of the calls\nis fixed, appearing early in development, with\nexperience only playing a role in refining the\nrange of objects or events that elicit such\ncalls. Fifth, there is no evidence that calling is\nintentional in the sense of taking into account\nwhat other individuals believe or want.\nEarly interpretations of this work suggest-\ned that when animals vocalize, they are func-\ntionally referring to the objects and events\nthat they have encountered. As such, vervet\nalarm calls and rhesus monkey food calls, to\ntake two examples, were interpreted as word-\nlike, with callers referring to different kinds\nof predators or different kinds of food. More\nrecent discussions have considerably weak-\nened this interpretation, suggesting that if the\nsignal is referential at all, it is in the mind of\nthe listener who can extract information\nabout the signaler’s current context from the\nacoustic structure of the call alone (78, 95).\nDespite this evidence that animals can extract\ninformation from the signal, there are several\nreasons why additional evidence is required\nbefore such signals can be considered as pre-\ncursors for, or homologs of, human words.\nRoughly speaking, we can think of a partic-\nular human language as consisting of words and\ncomputational procedures (“rules”) for con-\nstructing expressions from them. The computa-\ntional system has the recursive property briefly\noutlined earlier, which may be a distinct human\nproperty. However, key aspects of words may\nalso be distinctively human. There are, first of\nall, qualitative differences in scale and mode of\nacquisition, which suggest that quite different\nmechanisms are involved; as pointed out above,\nthere is no evidence for vocal imitation in non-\nhuman primates, and although human children\nmay use domain-general mechanisms to ac-\nquire and recall words (98, 99), the rate at\nwhich children build the lexicon is so massively\ndifferent from nonhuman primates that one\nmust entertain the possibility of an indepen-\ndently evolved mechanism. Further-\nmore, unlike the best animal exam-\nples of putatively referential signals,\nmost of the words of human lan-\nguage are not associated with specif-\nic functions (e.g., warning cries, food\nannouncements) but can be linked to\nvirtually any concept that humans\ncan entertain. Such usages are often\nhighly intricate and detached from\nthe here and now. Even for the sim-\nplest words, there is typically no\nstraightforward word-thing relation-\nship, if “thing” is to be understood in\nmind-independent terms. Without\npursuing the matter here, it appears\nthat many of the elementary proper-\nties of words—including those that\nenter into referentiality—have only\nweak analogs or homologs in natural\nanimal communication systems, with\nonly slightly better evidence from the\ntraining studies with apes and dol-\nphins. Future research must therefore\nprovide stronger support for the pre-\ncursor position, or it must instead\nabandon this hypothesis, arguing that\nthis component of FLB (conceptual-\nintentional) is also uniquely human.\nDiscrete infinity and constraints\non learning. The data summarized thus far,\nalthough far from complete, provide overall\nsupport for the position of continuity between\nhumans and other animals in terms of FLB.\nHowever, we have not yet addressed one\nissue that many regard as lying at the heart of\nlanguage: its capacity for limitless expressive\npower, captured by the notion of discrete\ninfinity. It seems relatively clear, after nearly\na century of intensive research on animal\ncommunication, that no species other than\nhumans has a comparable capacity to recom-\nbine meaningful units into an unlimited vari-\nety of larger structures, each differing sys-\ntematically in meaning. However, little\nprogress has been made in identifying the\nspecific capabilities that are lacking in other\nanimals.\nFig. 5. Human and nonhuman animals exhibit the capacity to compute numerosities, including small precise\nnumber quantification and large approximate number estimation. Humans may be unique, however, in the ability\nto show open-ended, precise quantificational skills with large numbers, including the integer count list. In parallel\nwith the faculty of language, our capacity for number relies on a recursive computation. [Illustration: John Yanson]\nS C I E N C E ’ S C O M P A S S\n22 NOVEMBER 2002 VOL 298SCIENCE www.sciencemag.org1576\nThe astronomical variety of sentences any\nnatural language user can produce and under-\nstand has an important implication for lan-\nguage acquisition, long a core issue in devel-\nopmental psychology. A child is exposed to\nonly a small proportion of the possible sen-\ntences in its language, thus limiting its data-\nbase for constructing a more general version\nof that language in its own mind/brain. This\npoint has logical implications for any system\nthat attempts to acquire a natural language on\nthe basis of limited data. It is immediately\nobvious that given a finite array of data, there\nare infinitely many theories consistent with it\nbut inconsistent with one another. In the\npresent case, there are in principle infinitely\nmany target systems ( potential I-languages)\nconsistent with the data of experience, and\nunless the search space and acquisition mech-\nanisms are constrained, selection among\nthem is impossible. A version of the problem\nhas been formalized by Gold (100) and more\nrecently and rigorously explored by Nowak\nand colleagues (72–75). No known “general\nlearning mechanism” can acquire a natural\nlanguage solely on the basis of positive or\nnegative evidence, and the prospects for find-\ning any such domain-independent device\nseem rather dim. The difficulty of this prob-\nlem leads to the hypothesis that whatever\nsystem is responsible must be biased or con-\nstrained in certain ways. Such constraints\nhave historically been termed “innate dispo-\nsitions,” with those underlying language re-\nferred to as “universal grammar.” Although\nthese particular terms have been forcibly re-\njected by many researchers, and the nature of\nthe particular constraints on human (or ani-\nmal) learning mechanisms is currently unre-\nsolved, the existence of some such con-\nstraints cannot be seriously doubted. On the\nother hand, other constraints in animals must\nhave been overcome at some point in human\nevolution to account for our ability to acquire\nthe unlimited class of generative systems that\nincludes all natural languages. The nature of\nthese latter constraints has recently become\nthe target of empirical work. We focus here\non the nature of number representation and\nrule learning in nonhuman animals and hu-\nman infants, both of which can be investigat-\ned independently of communication and pro-\nvide hints as to the nature of the constraints\non FLN.\nMore than 50 years of research using clas-\nsical training studies demonstrates that ani-\nmals can represent number, with careful con-\ntrols for various important confounds (80). In\nthe typical experiment, a rat or pigeon is\ntrained to press a lever x number of times to\nobtain a food reward. Results show that ani-\nmals can hit the target number to within a\nclosely matched mean, with a standard devi-\nation that increases with magnitude: As the\ntarget number increases, so does variation\naround the mean. These results have led to\nthe idea that animals, including human in-\nfants and adults, can represent number ap-\nproximately as a magnitude with scalar vari-\nability (101, 102). Number discrimination is\nlimited in this system by Weber’s law, with\ngreater discriminability among small num-\nbers than among large numbers (keeping dis-\ntances between pairs constant) and between\nnumbers that are farther apart (e.g., 7 versus\n8 is harder than 7 versus 12). The approxi-\nmate number sense is accompanied by a sec-\nond precise mechanism that is limited to val-\nues less than 4 but accurately distinguishes 1\nfrom 2, 2 from 3, and 3 from 4; this second\nsystem appears to be recruited in the context\nof object tracking and is limited by working\nmemory constraints (103). Of direct rele-\nvance to the current discussion, animals can\nbe trained to understand the meaning of num-\nber words or Arabic numeral symbols. How-\never, these studies reveal striking differences\nin how animals and human children acquire\nthe integer list, and provide further evidence\nthat animals lack the capacity to create open-\nended generative systems.\nBoysen and Matsuzawa have trained\nchimpanzees to map the number of objects\nonto a single Arabic numeral, to correctly\norder such numerals in either an ascending or\ndescending list, and to indicate the sums of\ntwo numerals (104 –106). For example, Boy-\nsen shows that a chimpanzee seeing two or-\nanges placed in one box, and another two\noranges placed in a second box, will pick the\ncorrect sum of four out of a lineup of three\ncards, each with a different Arabic numeral.\nThe chimpanzees’ performance might sug-\ngest that their representation of number is like\nours. Closer inspection of how these chim-\npanzees acquired such competences, howev-\ner, indicates that the format and content of\ntheir number representations differ funda-\nmentally from those of human children. In\nparticular, these chimpanzees required thou-\nsands of training trials, and often years, to\nacquire the integer list up to nine, with no\nevidence of the kind of “aha” experience that\nall human children of approximately 3.5\nyears acquire (107). A human child who has\nacquired the numbers 1, 2, and 3 (and some-\ntimes 4) goes on to acquire all the others; he\nor she grasps the idea that the integer list is\nconstructed on the basis of the successor\nfunction. For the chimpanzees, in contrast,\neach number on the integer list required the\nsame amount of time to learn. In essence,\nalthough the chimpanzees’ understanding of\nArabic numerals is impressive, it parallels\ntheir understanding of other symbols and\ntheir referential properties: The system appar-\nently never takes on the open-ended genera-\ntive property of human language. This limi-\ntation may, however, reveal an interesting\nquirk of the child’s learning environment and\na difference from the training regime of ani-\nmals: Children typically first learn an arbi-\ntrary ordered list of symbols (“1, 2, 3, 4 . . . ”)\nand later learn the precise meaning of such\nwords; apes and parrots, in contrast, were\ntaught the meanings one by one without\nlearning the list. As Carey (103) has argued,\nthis may represent a fundamental difference\nin experience, a hypothesis that could be\ntested by first training animals with an arbi-\ntrary ordered list.\nA second possible limitation on the class\nof learnable structures concerns the kinds of\nstatistical inferences that animals can com-\npute. Early work in computational linguistics\n(108 –110) suggested that we can profitably\nthink about language as a system of rules\nplaced within a hierarchy of increasing com-\nplexity. At the lowest level of the hierarchy\nare rule systems that are limited to local\ndependencies, a subcategory of so-called\n“finite-state grammars.” Despite their attrac-\ntive simplicity, such rule systems are inade-\nquate to capture any human language. Natu-\nral languages go beyond purely local struc-\nture by including a capacity for recursive\nembedding of phrases within phrases, which\ncan lead to statistical regularities that are\nseparated by an arbitrary number of words or\nphrases. Such long-distance, hierarchical re-\nlationships are found in all natural languages\nfor which, at a minimum, a “phrase-structure\ngrammar” is necessary. It is a foundational\nobservation of modern generative linguistics\nthat, to capture a natural language, a grammar\nmust include such capabilities (Fig. 5).\nRecent studies suggest that the capacity to\ncompute transitional probabilities—an exam-\nple of a rule at the lowest level of the hierar-\nchy—might be available to human infants\nand provide a mechanism for segmenting\nwords from a continuous acoustic stream\n(111–113). Specifically, after familiarization\nto a continuous sequence of consonant-vowel\n(CV) syllables, where particular trigrams\n(three CVs in sequence, considered to be\n“words” in this context) have a high proba-\nbility of appearing within the corpus, infants\nare readily able to discriminate these tri-\ngrams from others that are uncommon.\nAlthough this ability may provide a mech-\nanism for word segmentation, it is appar-\nently not a mechanism that evolved unique-\nly in humans or for language: The same\ncomputation is spontaneously available to\nhuman infants for visual sequences and\ntonal melodies (113), as well as to nonhu-\nman primates (cotton-top tamarins) tested\nwith the same methods and stimuli (114 ).\nSimilarly, in the same way that human\ninfants appear capable of computing alge-\nbraic rules that operate over particular CV\nsequences (115), so too can cotton-top\ntamarins (116 ), again demonstrating that\nthe capacity to discover abstract rules at a\nS C I E N C E ’ S C O M P A S S\nwww.sciencemag.org SCIENCE VOL 29822 NOVEMBER 2002 1577\nlocal level is not unique to humans, and\nalmost certainly did not evolve specifically\nfor language.\nFitch and Hauser (117) recently complet-\ned a study comparing finite-state and phrase-\nstructure grammar acquisition in human\nadults and tamarins, using the same subjects\nand methods as the studies above. The\nphrase-structure rule tested was AnBn, where\nA and B were each represented by one of a\nset of eight different CVs. The rule therefore\nspecified both a set of consistent strings (n\nA’s must precede n B’s) and a set of incon-\nsistent strings; the latter consisted of viola-\ntions of order (B tokens precede A tokens) or\nof patterning (alternations of A’s and B’s\nsuch as ABAB). Results showed that human\nadults rapidly learned this rule implicitly,\ndistinguishing consistent and inconsistent\nstrings. Tamarins, in contrast, failed in three\nseparate experiments testing their ability to\nacquire this grammar, but they readily\nmastered a finite-state variant (ABn) imple-\nmented with the same stimuli and testing\nconditions. This suggests that tamarins have a\nlimited capacity to learn the type of long-\ndistance hierarchical dependencies necessary\nto achieve the class of phrase-structure gram-\nmars. If true, this limitation would place se-\nvere restrictions on their capacity to learn any\nnatural human language. It is currently un-\nclear whether this limitation generalizes to\nother animals, and whether it is similarly\nimposed on humans at different stages of\ndevelopment. Nonetheless, such experiments\nprovide an empirical approach to exploring\nkey differences between humans and animals\nrelevant to FLN.\nOur review has stressed the usefulness\nof animal data for theories about humans,\nbut this exchange need not be one-way. As\nthe research program we have sketched\nprogresses, more general principles about\ncognitive evolution may emerge. For exam-\nple, suppose we adopt the conception of\nhypothesis 3, oversimplifying radically,\nthat the interface systems—sensory-motor\nand conceptual-intentional—are given, and\nthe innovation that yielded the faculty of\nlanguage was the evolution of the compu-\ntational system that links them. The com-\nputational system must (i) construct an in-\nfinite array of internal expressions from the\nfinite resources of the conceptual-intention-\nal system, and (ii) provide the means to\nexternalize and interpret them at the senso-\nry-motor end. We may now ask to what\nextent the computational system is optimal,\nmeeting natural conditions of efficient\ncomputation such as minimal search and no\nbacktracking. To the extent that this can be\nestablished, we will be able to go beyond\nthe (extremely difficult, and still distant)\naccomplishment of finding the principles of\nthe faculty of language, to an understanding\nof why the faculty follows these particular\nprinciples and not others. We would then\nunderstand why languages of a certain class\nare attainable, whereas other imaginable\nlanguages are impossible to learn and sus-\ntain. Such progress would not only open the\ndoor to a greatly simplified and empirically\nmore tractable evolutionary approach to the\nfaculty of language, but might also be more\ngenerally applicable to domains beyond\nlanguage in a wide range of species—per-\nhaps especially in the domain of spatial\nnavigation and foraging, where problems of\noptimal search are relevant. For example,\nelegant studies of insects, birds, and pri-\nmates reveal that individuals often search\nfor food by an optimal strategy, one involv-\ning minimal distances, recall of locations\nsearched, and kinds of objects retrieved\n(77, 118, 119). Only after a concerted, mul-\ntidisciplinary attack on the problems of\nlanguage evolution, paralleling 40 years of\noptimal foraging research, will we learn\nwhether such similarities are more than\nsuperficial.\nConclusions\nWe conclude by making three points. First, a\npractical matter: Linguists and biologists,\nalong with researchers in the relevant branch-\nes of psychology and anthropology, can\nmove beyond unproductive theoretical debate\nto a more collaborative, empirically focused\nand comparative research program aimed at\nuncovering both shared (homologous or anal-\nogous) and unique components of the faculty\nof language. Second, although we have ar-\ngued that most if not all of FLB is shared with\nother species, whereas FLN may be unique to\nhumans, this represents a tentative, testable\nhypothesis in need of further empirical inves-\ntigation. Finally, we believe that a compara-\ntive approach is most likely to lead to new\ninsights about both shared and derived fea-\ntures, thereby generating new hypotheses\nconcerning the evolutionary forces that led to\nthe design of the faculty of language. Specif-\nically, although we have said relatively little\nabout the role of natural selection in shaping\nthe design features of FLN, we suggest that\nby considering the possibility that FLN\nevolved for reasons other than language, the\ncomparative door has been opened in a new\nand (we think) exciting way.\nComparative work has generally focused\non animal communication or the capacity to\nacquire a human-created language. If, how-\never, one entertains the hypothesis that recur-\nsion evolved to solve other computational\nproblems such as navigation, number quanti-\nfication, or social relationships, then it is\npossible that other animals have such abili-\nties, but our research efforts have been tar-\ngeted at an overly narrow search space (Fig.\n3). If we find evidence for recursion in ani-\nmals, but in a noncommunicative domain,\nthen we are more likely to pinpoint the mech-\nanisms underlying this ability and the selec-\ntive pressures that led to it. This discovery, in\nturn, would open the door to another suite of\npuzzles: Why did humans, but no other ani-\nmal, take the power of recursion to create an\nopen-ended and limitless system of commu-\nnication? Why does our system of recursion\noperate over a broader range of elements or\ninputs (e.g., numbers, words) than other ani-\nmals? One possibility, consistent with current\nthinking in the cognitive sciences, is that\nrecursion in animals represents a modular\nsystem designed for a particular function\n(e.g., navigation) and impenetrable with re-\nspect to other systems. During evolution, the\nmodular and highly domain-specific system\nof recursion may have become penetrable and\ndomain-general. This opened the way for hu-\nmans, perhaps uniquely, to apply the power\nof recursion to other problems. This change\nfrom domain-specific to domain-general may\nhave been guided by particular selective pres-\nsures, unique to our evolutionary past, or as a\nconsequence (by-product) of other kinds of\nneural reorganization. Either way, these are\ntestable hypotheses, a refrain that highlights\nthe importance of comparative approaches to\nthe faculty of language.\nReferences and Notes\n1. N. Chomsky, Aspects of the Theory of Syntax (MIT\nPress, Cambridge, MA, 1965).\n2. \u0002\u0002\u0002\u0002, Reflections on Language (Pantheon, New\nYork, 1975).\n3. M. D. Hauser, The Evolution of Communication (MIT\nPress, Cambridge, MA, 1996).\n4. R. Jackendoff, Foundations of Language (Oxford\nUniv. Press, New York, 2002).\n5. L. Jenkins, Biolinguistics (Cambridge Univ. Press,\nCambridge, 2000).\n6. E. H. Lenneberg, Biological Foundations of Language\n(Wiley, New York, 1967).\n7. P. Lieberman, The Biology and Evolution of Language\n(Harvard Univ. Press, Cambridge, MA, 1984).\n8. A. Liberman, Speech: A Special Code (MIT Press,\nCambridge, MA, 1996).\n9. W. T. Fitch, Trends Cognit. Sci. 4, 258(2000).\n10. D. L. Cheney, R. M. Seyfarth, How Monkeys See the\nWorld: Inside the Mind of Another Species (Univ. of\nChicago Press, Chicago, 1990).\n11. A. Doupe, P. Kuhl, Annu. Rev. Neurosci. 22, 567\n(1999).\n12. P. Marler, Am. Sci. 58, 669 (1970).\n13. C. Darwin, On the Origin of Species ( John Murray,\nLondon, 1859).\n14. \u0002\u0002\u0002\u0002, The Descent of Man and Selection in Rela-\ntion to Sex ( John Murray, London, 1871).\n15. A. M. Liberman, K. S. Harris, H. S. Hoffman, B. C.\nGriffith, J. Exp. Psychol. 54, 358(1957).\n16. A. M. Liberman, F. S. Cooper, D. P. Shankweiler, M.\nStuddert-Kennedy, Psychol. Rev. 74, 431 (1967).\n17. P. K. Kuhl, J. D. Miller, Science 190, 69 (1975).\n18. P. K. Kuhl, D. M. Padden, Percept. Psychophys. 32,\n542 (1982).\n19. K. R. Kluender, R. Diehl, P. R. Killeen, Science 237,\n1195 (1987).\n20. S. J. Gould, in Evolution, Brain and Behavior: Persis-\ntent Problems, R. B. Masterton, W. Hodos, H. Jerison,\nEds. (Wiley, New York, 1976), pp. 175–179.\n21. W. J. Gehring, Master Control Genes in Development\nand Evolution: The Homeobox Story (Yale Univ.\nPress, New Haven, CT, 1998).\n22. R. M. Seyfarth, D. L. Cheney, in The Design of Animal\nS C I E N C E ’ S C O M P A S S\n22 NOVEMBER 2002 VOL 298SCIENCE www.sciencemag.org1578\nCommunication, M. D. Hauser, M. Konishi, Eds. (MIT\nPress, Cambridge, MA, 1999), pp. 391– 418.\n23. P. Marler, J. Neurobiol. 33, 1 (1997).\n24. F. Nottebohm, in The Design of Animal Communi-\ncation, M. D. Hauser, M. Konishi, Eds. (MIT Press,\nCambridge, MA, 1999), pp. 63–110.\n25. L. A. Petitto, P. Marentette, Science 251, 1483\n(1991).\n26. K. R. Kluender, A. J. Lotto, L. L. Holt, in Listening to\nSpeech: An Auditory Perspective, S. Greenberg, W.\nAinsworth, Eds. (Erlbaum, Mahwah, NJ, in press).\n27. R. Jackendoff, Trends Cognit. Sci. 3, 272 (1999).\n28. S. Pinker, P. Bloom, Behav. Brain Sci. 13, 707 (1990).\n29. R. Dawkins, The Blind Watchmaker (Norton, New\nYork, 1986).\n30. D. Bickerton, Species and Language (Univ. of Chica-\ngo Press, Chicago, 1990).\n31. R. Dunbar, Grooming, Gossip and the Evolution of\nLanguage (Harvard Univ. Press, Cambridge, MA,\n1996).\n32. D. Kimura, Neuromotor Mechanisms in Human Com-\nmunication (Oxford Univ. Press, Oxford, 1993).\n33. N. Chomsky, Rules and Representations (Columbia\nUniv. Press, New York, 1980).\n34. M. D. Hauser, in Language, Brain, and Cognitive\nDevelopment: Essays in Honor of Jacques Mehler, E.\nDupoux, Ed. (MIT Press, Cambridge, MA, 2001), pp.\n417– 434.\n35. W. Enard et al., Nature 418, 869 (2002).\n36. J. Maynard Smith, E. Szathmary, The Major Transi-\ntions of Evolution (Freeman, Oxford, 1995).\n37. L. Barrett, R. Dunbar, J. Lycett, Human Evolutionary\nPsychology (Princeton Univ. Press, Princeton, NJ,\n2002).\n38. D. Buss, Evolutionary Psychology (Allyn & Bacon,\nLondon, 1999).\n39. S. J. Gould, R. C. Lewontin, Proc. R. Soc. London 205,\n281 (1979).\n40. G. C. Williams, Adaptation and Natural Selection\n(Princeton Univ. Press, Princeton, NJ, 1966).\n41. N. Chomsky, The Minimalist Program (MIT Press,\nCambridge, MA, 1995).\n42. C. Collins, Local Economy (MIT Press, Cambridge,\nMA, 1997).\n43. S. D. Epstein, N. Hornstein, Working Minimalism\n(MIT Press, Cambridge, MA, 1999).\n44. L. Haegeman, Introduction to Government & Binding\nTheory (Blackwell, Oxford, 1991).\n45. J. R. Hurford, M. Studdert-Kennedy, C. Knight, Eds.,\nApproaches to the Evolution of Language: Social and\nCognitive Bases (Cambridge Univ. Press, Cambridge,\n1998).\n46. A. Wray, Ed., The Transition to Language (Oxford\nUniv. Press, Oxford, 2002).\n47. A. Liberman, D. H. Whalen, Trends Cognit. Sci. 4,\n187 (2000).\n48. P. Lieberman, Uniquely Human (Harvard Univ. Press,\nCambridge, MA, 1991).\n49. R. J. Dooling, C. T. Best, S. D. Brown, J. Acoust. Soc.\nAm. 97, 1839 (1995).\n50. J. M. Sinnott, C. H. Brown, J. Acoust. Soc. Am. 102,\n588 (1997).\n51. M. S. Sommers, D. B. Moody, C. A. Prosen, W. C.\nStebbins, J. Acoust. Soc. Am. 91, 3499 (1992).\n52. K. R. Kluender, A. J. Lotto, L. L. Holt, S. L. Bloedel, J.\nAcoust. Soc. Am. 104, 3568(1998).\n53. F. Ramus, M. D. Hauser, C. T. Miller, D. Morris, J.\nMehler, Science 288, 349 (2000).\n54. W. T. Fitch, J. Acoust. Soc. Am. 102, 1213 (1997).\n55. \u0002\u0002\u0002\u0002, J. P. Kelley, Ethology 106, 559 (2000).\n56. M. D. Hauser, C. S. Evans, P. Marler, Anim. Behav. 45,\n423 (1993).\n57. M. J. Owren, R. Bernacki, J. Acoust. Soc. Am. 83,\n1927 (1988).\n58. M. J. Owren, J. Comp. Psychol. 104, 20 (1990).\n59. D. Rendall, M. J. Owren, P. S. Rodman, J. Acoust. Soc.\nAm. 103, 602 (1998).\n60. V. E. Negus, The Comparative Anatomy and Physi-\nology of the Larynx (Hafner, New York, 1949).\n61. W. T. Fitch, D. Reby, Proc. R. Soc. London Ser. B 268,\n1669 (2001).\n62. J. D. Trout, Psychol. Rev. 108, 523 (2000).\n63. M. Donald, in Approaches to the Evolution of Lan-\nguage: Social and Cognitive Bases, J. R. Hurford, M.\nStuddert-Kennedy, C. Knight, Eds. (Cambridge Univ.\nPress, Cambridge, 1998), pp. 44 – 67.\n64. M. Studdert-Kennedy, Hum. Neurobiol. 2, 191\n(1983).\n65. V. M. Janik, P. J. B. Slater, Anim. Behav. 60, 1 (2000).\n66. M. Tomasello, J. Call, Primate Cognition (Oxford\nUniv. Press, Oxford, 1997).\n67. G. Rizzolatti, M. A. Arbib, Trends Cognit. Sci. 2, 18 8\n(1998).\n68. G. Rizzolatti, L. Fadiga, L. Fogassi, V. Gallese, Arch.\nItal. Biol. 137, 169 (1999).\n69. T. Chaminade, A. N. Meltzoff, J. Decety, Neuroimage\n15, 318(2002).\n70. J. Decety, T. Chaminade, J. Grezes, A. N. Meltzoff,\nNeuroimage 15, 265 (2002).\n71. M. Iacoboni et al., Science 286, 2526 (1999).\n72. M. A. Nowak, N. L. Komarova, P. Niyogi, Science\n291, 114 (2001).\n73. M. A. Nowak, N. L. Komarova, Trends Cognit. Sci. 5,\n288 (2001).\n74. M. A. Nowak, J. B. Plotkin, V. A. Jansen, Nature 404,\n495 (2000).\n75. M. A. Nowak, N. L. Komarova, P. Niyogi, Nature 417,\n611 (2002).\n76. C. M. Heyes, F. Huber, The Evolution of Cognition\n(MIT Press, Cambridge, MA, 2000).\n77. S. Shettleworth, Cognition, Evolution and Behavior\n(Oxford Univ. Press, New York, 1998).\n78. D. L. Cheney, R. M. Seyfarth, in The Tanner Lectures\non Human Values, G. Peterson, Ed. (Univ. of Utah\nPress, Salt Lake City, UT, 1998), pp. 173–210.\n79. M. D. Hauser, Wild Minds: What Animals Really Think\n(Holt, New York, 2000).\n80. C. R. Gallistel, The Organization of Learning (MIT\nPress, Cambridge, MA, 1990).\n81. I. M. Pepperberg, The Alex Studies (Harvard Univ.\nPress, Cambridge, MA, 2000).\n82. D. Premack, Gavagai! or the Future History of the\nAnimal Language Controversy (MIT Press, Cam-\nbridge, MA, 1986).\n83. \u0002\u0002\u0002\u0002, G. Woodruff, Behav. Brain Sci. 4, 515\n(1978).\n84. D. Premack, A. Premack, Original Intelligence\n(McGraw-Hill, New York, 2002).\n85. D. C. Dennett, Behav. Brain Sci. 6, 343 (1983).\n86. B. Hare, J. Call, B. Agnetta, M. Tomasello, Anim.\nBehav. 59, 771 (2000).\n87. B. Hare, J. Call, M. Tomasello, Anim. Behav. 61, 139\n(2001).\n88. C. M. Heyes, Behav. Brain Sci. 21, 101 (1998).\n89. D. J. Povinelli, T. J. Eddy, Monogr. Soc. Res. Child\nDev. 247 (1996).\n90. R. M. Seyfarth, D. L. Cheney, P. Marler, Science 210,\n801 (1980).\n91. W. P. G. Dittus, Anim. Behav. 32, 470 (1984).\n92. C. S. Evans, P. Marler, in Comparative Approaches to\nCognitive Science, H. Roitblatt, Ed. (MIT Press, Cam-\nbridge, MA, 1995), pp. 241–282.\n93. J. Fischer, Anim. Behav. 55, 799 (1998).\n94. S. Gouzoules, H. Gouzoules, P. Marler, Anim. Behav.\n32, 182 (1984).\n95. M. D. Hauser, Anim. Behav. 55, 1647 (1998).\n96. C. N. Slobodchikoff, J. Kiriazis, C. Fischer, E. Creef,\nAnim. Behav. 42, 713 (1991).\n97. K. Zuberbuhler, D. L. Cheney, R. M. Seyfarth,\nJ. Comp. Psychol. 113, 33 (1999).\n98. P. Bloom, L. Markson, Trends Cognit. Sci. 2, 67\n(1998).\n99. P. Bloom, How Children Learn the Meanings of\nWords (MIT Press, Cambridge, MA, 2000).\n100. E. M. Gold, Inform. Control 10, 447 (1967).\n101. S. Dehaene, The Number Sense (Oxford Univ. Press,\nOxford, 1997).\n102. C. R. Gallistel, R. Gelman, Trends Cognit. Sci. 4, 59\n(2000).\n103. S. Carey, Mind Lang. 16, 37 (2001).\n104. S. T. Boysen, G. G. Bernston, J. Comp. Psychol. 103,\n23 (1989).\n105. N. Kawai, T. Matsuzawa, Nature 403, 39 (2000).\n106. T. Matsuzawa, Nature 315, 57 (1985).\n107. K. Wynn, Cognit. Psychol. 24, 220 (1992).\n108. N. Chomsky, Logical Structure of Linguistic Theory/\nExcerpted Manuscript (Plenum, New York, 1975).\n109. \u0002\u0002\u0002\u0002, IRE Trans. Inform. Theory 2 (no. 2), 113\n(1956).\n110. \u0002\u0002\u0002\u0002, G. Miller, Inform. Control 1, 91 (1958).\n111. Z. S. Harris, Language 31, 190 (1955).\n112. J. R. Saffran, R. N. Aslin, E. L. Newport, Science 274,\n1926 (1996).\n113. J. Saffran, E. Johnson, R. N. Aslin, E. Newport, Cog-\nnition 70, 27 (1999).\n114. M. D. Hauser, E. L. Newport, R. N. Aslin, Cognition\n78, B53 (2001).\n115. G. Marcus, S. Vijayan, S. Bandi Rao, P. M. Vishton,\nScience 283, 77 (1999).\n116. M. D. Hauser, D. Weiss, G. Marcus, Cognition 86,\nB15 (2002).\n117. W. T. Fitch, M. D. Hauser, in preparation.\n118. N. S. Clayton, A. Dickinson, Nature 395, 272 (1998).\n119. C. R. Gallistel, A. E. Cramer, J. Exp. Biol. 199, 211\n(1996).\n120. P. Kuhl, Percept. Psychophys. 50, 93 (1991).\n121. P. F. MacNeilage, Behav. Brain Sci. 21, 499(1998).\n122. M. Studdert-Kennedy, in Approaches to the Evolu-\ntion of Language: Social and Cognitive Bases, J. R.\nHurford, M. Studdert-Kennedy, C. Knight, Eds.\n(Cambridge Univ. Press, Cambridge, 1998), pp. 202–\n221.\n123. H. McGurk, J. MacDonald, Nature 264, 746 (1976).\n124. L. R. Santos, G. M. Sulkowski, G. M. Spaepen, M. D.\nHauser, Cognition 83, 241 (2002).\n125. G. Gergerly, H. Bekkering, I. Kiraly, Nature 415, 755\n(2002).\n126. A. N. Meltzoff, M. K. Moore, Infant Behav. Dev. 17,\n83 (1994).\n127. A. Whiten, D. Custance, in Social Learning in Ani-\nmals: The Roots of Culture, C. M. Heyes, J. B. G.\nGalef, Eds. (Academic Press, San Diego, CA, 1996),\npp. 291–318.\n128. P. Marler, S. Karakashian, M. Gyger, in Cognitive\nEthology: The Minds of Other Animals, C. Ristau, Ed.\n(Erlbaum, Hillsdale, NJ, 1991), pp. 135–186.\n129. H. S. Terrace, L. K. Son, E. M. Brannon, Psychol. Sci.,\nin press.\n130. L. M. Herman, D. G. Richards, J. P. Wolz, Cognition\n16, 129 (1984).\n131. E. S. Savage-Rumbaugh et al., Monogr. Soc. Res.\nChild Dev. 58 (1993).\n132. E. M. Brannon, H. S. Terrace, Science 282, 746\n(1998).\n133. F. Lerdahl, R. Jackendoff, A Generative Theory of\nTonal Music (MIT Press, Cambridge, MA, 1983).\n134. N. Wallin, B. Merker, S. D. Brown, The Origins of\nMusic (MIT Press, Cambridge, MA, 2000).\n135. R. Zatorre, I. Peretz, The Biological Foundations of\nMusic (National Academy Press, New York,\n2000).\n136. For comments on an earlier draft of the manuscript,\nwe thank D. Cheney, R. Jackendoff, L. Jenkins, M.\nNowak, M. Piatelli-Palmerini, S. Pinker, and R.\nSeyfarth.\nS C I E N C E ’ S C O M P A S S\nwww.sciencemag.org SCIENCE VOL 29822 NOVEMBER 2002 1579", "affiliations": [{"university": "Harvard University", "country": "United States", "discipline": "Psychology"}, {"university": "Massachusetts Institute of Technology", "country": "United States", "discipline": "Linguistics"}], "species_categories": ["Primate", "Bird", "Marine Mammal"], "specialized_species": ["chimpanzees", "songbirds", "dolphins"], "computational_stages": ["Data Collection", "Pre-processing", "Meaning Identification"], "linguistic_features": ["Vocal Auditory Channel and Turn-taking", "Broadcast and Direct Reception", "Semanticity", "Arbitrariness and Duality of Patterns", "Recursion"], "status": "saved", "created_at": "2026-01-13T12:49:59.887501", "updated_at": "2026-01-13T14:07:10.756603", "committed_at": "2026-01-13T14:07:13.691442"}
{"id": "fd2be71a-9ed9-4fc9-b6f2-1cc40a3db9fa", "title": "Fast and accurate annotation of acoustic signals with deep neural networks", "authors": ["Steinfath, Elsa", "Palacios-Munoz, Adrian", "Rottsch{\\\"a}fer, Julian R", "Yuezak, Deniz", "Clemens, Jan"], "year": "2021", "journal": "Elife", "abstract": "", "doi": "", "analysis_notes": "*For correspondence:\nclemensjan@gmail.com\nCompeting interests: The\nauthors declare that no\ncompeting interests exist.\nFunding: See page 20\nReceived: 26 March 2021\nPreprinted: 29 March 2021\nAccepted: 04 October 2021\nPublished: 01 November 2021\nReviewing editor: Ronald L\nCalabrese, Emory University,\nUnited States\nCopyright Steinfath et al. This\narticle is distributed under the\nterms of the Creative Commons\nAttribution License, which\npermits unrestricted use and\nredistribution provided that the\noriginal author and source are\ncredited.\nFast and accurate annotation of acoustic\nsignals with deep neural networks\nElsa Steinfath1,2, Adrian Palacios-Mun˜ oz1,2, Julian R Rottscha¨fer1,2\n,\nDeniz Yuezak1,2, Jan Clemens1,3*\n1European Neuroscience Institute - A Joint Initiative of the University Medical\nCenter Go¨ttingen and the Max-Planck-Society, Go¨ttingen, Germany; 2\nInternational\nMax Planck Research School and Go¨ttingen Graduate School for Neurosciences,\nBiophysics, and Molecular Biosciences (GGNB) at the University of Go¨ttingen,\nGo¨ttingen, Germany; 3Bernstein Center for Computational Neuroscience,\nGo¨ttingen, Germany\nAbstract Acoustic signals serve communication within and across species throughout the animal\nkingdom. Studying the genetics, evolution, and neurobiology of acoustic communication requires\nannotating acoustic signals: segmenting and identifying individual acoustic elements like syllables\nor sound pulses. To be useful, annotations need to be accurate, robust to noise, and fast.\nWe here introduce DeepAudioSegmenter (DAS), a method that annotates acoustic signals across\nspecies based on a deep-learning derived hierarchical presentation of sound. We demonstrate the\naccuracy, robustness, and speed of DAS using acoustic signals with diverse characteristics from\ninsects, birds, and mammals. DAS comes with a graphical user interface for annotating song,\ntraining the network, and for generating and proofreading annotations. The method can be trained\nto annotate signals from new species with little manual annotation and can be combined with\nunsupervised methods to discover novel signal types. DAS annotates song with high throughput\nand low latency for experimental interventions in realtime. Overall, DAS is a universal, versatile, and\naccessible tool for annotating acoustic communication signals.\nIntroduction\nAnimals produce sounds to foster group cohesion (Haack et al., 1983; Janik and Slater, 1998;\nChaverri et al., 2013), to signal the presence of food, friend, or foe (Ca¨sar et al., 2013; Clay et al.,\n2012), and to find and evaluate mating partners (Baker et al., 2019; Behr and von Helversen,\n2004; Holy and Guo, 2005; Sangiamo et al., 2020). Studying acoustic communication not only provides insight into social interactions within and across species; it can also reveal the mechanisms driving complex behaviors: The genetics and evolution of signal production and recognition (Ding et al.,\n2016), the genes and circuits driving song learning (Kollmorgen et al., 2020), or the fast and precise sensorimotor transformations involved in vocal interactions (Coen et al., 2016; Cator et al.,\n2009; Fortune et al., 2011; Okobi et al., 2019). The first step in many studies of acoustic communication is song annotation: the segmentation and labeling of individual elements in a recording.\nAcoustic signals are diverse and range from the repetitive long-distance calling songs of crickets,\ngrasshoppers, and anurans (Gerhardt and Huber, 2002), the dynamic and context-specific courtship\nsongs of vinegar flies or rodents (Coen et al., 2014; Clemens et al., 2018; Neunuebel et al., 2015;\nSangiamo et al., 2020), to the complex vocalizations produced by some birds and primates\n(Lipkind et al., 2013; Weiss et al., 2014; Landman et al., 2020).\nThis diversity in signal structure has spawned a zoo of annotation tools (Arthur et al., 2013;\nCoffey et al., 2019; Tachibana et al., 2020; Goffinet et al., 2021; Koumura and Okanoya, 2016;\nCohen et al., 2020), but existing methods still face challenges: First, assessing vocal repertoires and\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 1 of 25\nTOOLS AND RESOURCES\ntheir relation to behavioral and neural dynamics (Clemens et al., 2018; Coffey et al., 2019;\nNeunuebel et al., 2015; Fortune et al., 2011; Okobi et al., 2019) requires annotations to be complete and temporally precise even at low signal levels, but annotation can fail when signals are weak\n(Coffey et al., 2019; Stern et al., 2017). Second, analyses of large datasets and experimental interventions during behavior (Fortune et al., 2011; Okobi et al., 2019; Bath et al., 2014; Tschida and\nMooney, 2012; Stowers et al., 2017) need annotations to be fast, but existing methods are often\nslow. Last, annotation methods should be flexible and adaptable (Ding et al., 2016; Ding et al.,\n2019; Clemens et al., 2018; Clemens and Hennig, 2013), but existing methods often work only for\nrestricted types of signals or adapting them to new signals requires tedious manual tuning\n(Clemens et al., 2018).\nIn brief, an accurate, fast, and flexible framework for annotating song across species is missing. A\ngeneral framework would not only improve upon existing methods but would also facilitate the\nstudy of species for which automated methods do not yet exist. Deep neural networks have\nemerged as powerful and flexible tools for solving data annotation tasks relevant for neuroscience\nsuch as object recognition, pose tracking, or speech recognition (Krizhevsky et al., 2012;\nGraves and Jaitly, 2014; Mathis et al., 2018; Pereira et al., 2019; Graving et al., 2019). These\nmethods are not only fast and accurate but also easily adapted to novel signals by non-experts since\nthey only require annotated examples for learning. Recently, deep neural networks have also been\nused for annotating animal vocalizations (Oikarinen et al., 2019; Coffey et al., 2019; Cohen et al.,\n2020; Sainburg et al., 2020; Arthur et al., 2021; Goffinet et al., 2021).\nWe here present a new deep-learning-based framework for annotating acoustic signals, called\nDeep Audio Segmenter (DAS). We test the framework on a diverse set of recordings from insects,\nbirds, and mammals, and show that DAS annotates song in single- and multi-channel recordings\nwith high accuracy. The framework produces annotations with low latency on standard PCs and is\ntherefore ideally suited for closed-loop applications. Small-to-moderate amounts of manual annotations suffice for adapting the method to a new species and annotation work can be simplified by\ncombining DAS with unsupervised methods. We provide DAS as an open-source software package\nwith a graphical user interface for manually annotating audio, training the network, and inferring and\nproofreading annotations. Integration into existing frameworks for signal analysis or experimental\ncontrol is possible using a programmatic interface. The code and documentation for DAS are available at https://janclemenslab.org/das/.\nResults\nArchitecture and working principle of DAS\nAcoustic signals are defined by features on multiple timescales—the fast harmonic oscillations of the\nsound carrier (<10 ms), modulations of amplitude (AM) and frequency (FM) (10–100 ms), and the\nsequencing of different AM and FM patterns into bouts, syllables, or phrases (10–1000 ms). These\npatterns are typically made explicit using a hand-tuned pre-processing step based on time-resolved\nFourier or wavelet transforms (Arthur et al., 2013; Van Segbroeck et al., 2017; Coffey et al.,\n2019; Oikarinen et al., 2019; Cohen et al., 2020). Most deep-learning-based methods then treat\nthis pre-defined spectrogram as an image and use methods derived from computer vision to extract\nthe AM and FM features relevant for annotation (Oikarinen et al., 2019; Coffey et al., 2019;\nCohen et al., 2020). Recurrent units are sometimes used to track the sound features over time\n(Cohen et al., 2020). This approach can produce accurate annotations but has drawbacks: First, the\nspectrogram constitutes a strong and proven pre-processing step, but it is unsuitable for some signal types, like short pulsatile signals. Second, the pre-processing transform is typically tuned by hand\nand may therefore require expert knowledge for it to produce optimal results. Lastly, the recurrent\nunits used in some methods (Cohen et al., 2020) excel at combining information over time to provide the context information necessary to annotate spectrally complex signals, but they can be hard\nto train and slow to run (Bai et al., 2018).\nDAS solves these limitations in three ways: First, the pre-processing step is optional. This makes\nDAS more flexible, since signals for which a time-resolved Fourier transform is not appropriate—for\ninstance, short pulsatile signals—can now also be processed. Second, the optional preprocessing\nstep is integrated and optimized with the rest of the network. This removes the need to hand-tune\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 2 of 25\nTools and resources Neuroscience\nthis step and allows the network to learn a preprocessing that deviates from a time-resolved Fourier\nor wavelet transform if beneficial (Choi et al., 2017). Integrating the preprocessing into the network\nalso increases inference speed due to the efficient implementation and hardware acceleration of\ndeep-learning frameworks. Third and last, DAS learns a task-specific representation of sound features using temporal convolutional networks (TCNs) (Bai et al., 2018; van den Oord et al., 2016;\nGuirguis et al., 2021; Figure 1—figure supplement 1A–E). At the core of TCNs are so-called\ndilated convolutions (Yu and Koltun, 2016). In standard convolutions, short templates slide over the\nsignal and return the similarity with the signal at every time point. In dilated convolutions, these templates have gaps, allowing to analyze features on longer timescales without requiring more parameters to specify the template. Stacking dilated convolutions with growing gap sizes results in a\nhierarchical, multi-scale representation of sound features, which is ideally suited for the hierarchical\nand harmonic structure of animal vocalizations.\nThe output of the deep neural network in DAS is a set of confidence scores for each audio sample, corresponding to the probability of each song type (Figure 1C). Annotation labels for the different song types are mutually exclusive and are produced by comparing the confidence score to a\nthreshold or by choosing the most probable song type. Brief gaps in the annotations are closed and\nshort spurious detections are removed to smoothen the annotation. For song types that are\ndescribed as events, like the pulses in fly song (Figure 1A), the event times are extracted as local\nmaxima that exceed a confidence threshold.\nDAS accurately annotates song from a diverse range of species\nFly courtship song\nWe first tested DAS on the courtship song of Drosophila melanogaster, which consists of two major\nmodes (Figure 1A): The sine song, which corresponds to sustained oscillations with a species-specific carrier frequency (150 Hz), and two types of pulse song, which consists of trains of short (5–10\nms) pulses with carrier frequencies between 180 and 500 Hz, produced with a species-specific interval (35–45 ms in D. melanogaster). Males dynamically choose the song modes based on sensory\nfeedback from the female (Coen et al., 2014; Clemens et al., 2018; Calhoun et al., 2019). Despite\nthe relative simplicity of the individual song elements, an accurate annotation of fly song is challenging because of low signal-to-noise ratio (SNR): The song attenuates rapidly with distance (BennetClark, 1998) and is highly directional (Morley et al., 2018), which can lead to weak signals if the\nmale is far from the microphone (Figure 1A). Moreover, the interactions between the flies introduce\npulsatile noise and complicate the accurate and complete annotation of the pulse song.\nWe first trained DAS to detect the pulse and the sine song recorded using a single microphone\n(data from Stern, 2014) and compared the performance of DAS to that of the current state-of-theart in fly song segmentation, FlySongSegmenter (FSS) (Arthur et al., 2013; Coen et al., 2014;\nClemens et al., 2018). Annotation performance was quantified using precision, the fraction of correct detections, and recall, the fraction of true song that is detected (Figure 1E,J, Figure 1—figure\nsupplement 1F,G). We counted detected pulses within 10 ms of a true pulse as correct detections.\nTen ms corresponds to 1/4th of the typical interval between pulses in a train and results are robust\nto the choice of this value (Figure 1—figure supplement 2A). DAS detects pulses with a high precision of 97% - only 3% of all detected pulses are false detections - and a high recall of 96% - it misses\nonly 4% of all pulses. This is a substantial improvement in recall over FSS, which has slightly higher\nprecision (99%) but misses 13% of all pulses (87% recall) (Figure 1D,E). In DAS, the balance between\nprecision and recall can be controlled via the confidence threshold, which corresponds to the minimal confidence required for labeling a pulse (Figure 1C): Lowering this threshold from 0.7 to 0.5\nyields a recall of 99% for pulse song with a modest reduction in precision to 95%. The performance\ngain of DAS over FSS for pulse stems from better recall at high frequencies (>400 Hz) and low SNR\n(Figure 1G,H). To assess DAS performance for sine song, we evaluated the sample-wise precision\nand recall. DAS has similar precision to FSS (92% vs 91%) but higher recall (98% vs. 91%) (Figure 1I,\nJ). Recall is higher in particular for short sine songs (<100 ms) and at low SNR (<1.0) (Figure 1L,M).\nThe performance boost for pulse and sine arises because DAS exploits context information, similar\nto how humans annotate song: For instance, DAS discriminates soft song pulses from pulsatile noise\nbased on the pulse shape but also because song pulses occur in regular trains while noise pulses do\nnot (Figure 1—figure supplement 2C). A comparison of DAS’ performance to that of human\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 3 of 25\nTools and resources Neuroscience\n0.1 s\nFrequency [Hz]\n0\n1\nConfidence\nDAS\nManual\nD\nI\nE\nPrecision [%]\nRecall [%]\nJ\n0 2 4\n0.0\n1.6\nPDF\n0.8\nF\n0 10 20 30 40\nTemporal error [ms]\n0.00\n0.03\n0.06 K\n200 400\nCarrier frequency [Hz]\n0\n50\n100\nRecall [%]\nG\n4 16\nPulse SNR\nH\nDAS\nFSS\nPulse\n500 1000 1500\nDuration [ms]\n0\n50\n100\nRecall [%]\nL\n1/2 2\nSine SNR\nM\nDAS\nFSS\nSine\nA\nTemporal error [ms]\nC\nRecall [%]\nPDF\nNo pulse Pulse\nNo pulse\nPulse\n0\n100\nNo sine Sine\nManual\nNo sine\nSine DAS\n0\n100\n%\n%\n70 80 90 100\n70\n80\n90\nDAS\nFSS\n70 80 90 100\n70\n80\n90\n100\nDAS\nFSS\n5066\n17\n21\n535\n94.8\n1.3\n0.4\n15.9\nConfidence\nscores\nB Raw audio\n100\nManual Precision [%]\nDAS\n0\n200\n400\n600\n-50\n0\ndB\n1\nHuman\nHuman\nFigure 1. DAS performance for fly song. (A) Fly song (black, top) with manual annotations of sine (blue) and pulse (red) song. The spectrogram (bottom)\nshows the signal’s frequency content over time (see color bar). (B) DAS builds a hierarchical presentation of song features relevant for annotation using\na deep neural network. The network consists of three TCN blocks, which extract song features at multiple timescales. The output of the network is a\nconfidence score for each sample and song type. (C) Confidence scores (top) for sine (blue) and pulse (red) for the signal in A. The confidence is\ntransformed into annotation labels (bottom) based on a confidence threshold (0.5 for sine, 0.7 for pulse). Ground truth (bottom) from manual\nannotations shown for comparison. (D) Confusion matrix for pulse from the test data set. Color indicates the percentage (see color bar) and text labels\nindicate the number of pulses for each quadrant. All confusion matrices are normalized such that columns sum to 100%. The concentration of values\nalong the diagonal indicates high annotation performance. (E) Precision-recall curve for pulse depicts the performance characteristics of DAS for\ndifferent confidence thresholds (from 0 to 1, black arrow points in the direction of increasing threshold). Recall decreases and precision increases with\nthe threshold. The closer the curve to the upper and right border, the better. The red circle corresponds to the performance of DAS for a threshold of\n0.7. The black circle depicts the performance of FlySongSegmenter (FSS) and gray circles the performance of two human annotators. (F) Probability\ndensity function of temporal errors for all detected pulses (red shaded area), computed as the distance between each pulse annotated by DAS and the\nnearest manually annotated pulse. Lines depict the median temporal error for DAS (red line, 0.3 ms) and FSS (gray line, 0.1 ms). (G, H) Recall of DAS\n(red line) and FSS (gray line) as a function of the pulse carrier frequency (G) and signal-to-noise ratio (SNR) (H). Red shaded areas show the distributions\nof carrier frequencies (G) and SNRs (H) for all pulses. DAS outperforms FSS for all carrier frequencies and SNRs. (I) Same as in D but for sine. Color\nindicates the percentage (see color bar) and text labels indicate seconds of sine for each quadrant. (J) Same as in E but for sine. The blue circle depicts\nthe performance for the confidence threshold of 0.5. (K) Distribution of temporal errors for all detected sine on- and offsets. Median temporal error is\n12 ms for DAS (blue line) and 22 ms for FSS (gray line). (L, M) Recall for DAS (blue line) and FSS (gray line) as a function of sine duration (L) and SNR (M).\nBlue-shaded areas show the distributions of durations and SNRs for all sine songs. DAS outperforms FSS for all durations and SNRs.\nThe online version of this article includes the following figure supplement(s) for figure 1:\nFigure supplement 1. DAS architecture and evaluation.\nFigure supplement 2. Performance and the role of context for annotating fly pulse song.\nFigure supplement 3. Performance for multi-channel recordings of fly courtship song.\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 4 of 25\nTools and resources Neuroscience\nannotators reveals that our methods exceeds human-level performance for pulse and sine\n(Figure 1E,J, Table 1).\nTemporally precise annotations are crucial, for instance when mapping sensorimotor transformations based on the timing of behavioral or neuronal responses relative to individual song elements\n(Coen et al., 2014; Srivastava et al., 2017; Long and Fee, 2008; Benichov and Vallentin, 2020).\nWe therefore quantified the temporal error of the annotations produced by DAS. For pulse song,\nthe temporal error was taken as the distance of each pulse annotated by DAS to the nearest true\npulse. The median temporal error for pulse is 0.3 ms which is negligible compared to the average\nduration of a pulse (5–10 ms) or of a pulse interval (35–45 ms) (Deutsch et al., 2019). For sine song,\nthe median temporal error for on- and offsets was 12 ms, which is almost half of that of FSS (22 ms).\nSine song can have low SNR (Figure 1M) and fades in and out, making the precise identification of\nsine song boundaries difficult even for experienced manual annotators (see Figure 1A,C).\nRecording song during naturalistic interactions in large behavioral chambers often requires multiple microphones (Coen et al., 2014; Neunuebel et al., 2015). To demonstrate that DAS can process\nmulti-channel audio, we trained DAS to annotate recordings from a chamber tiled with nine microphones (Coen et al., 2014; Figure 1—figure supplement 3, Figure 1—figure supplement 1B).\nDAS processes multi-channel audio by using filters that take into account information from all channels simultaneously. As is the case for existing methods (Arthur et al., 2013), we achieved maximal\nperformance by training separate networks for the pulse and for the sine song (Table 2). In multichannel recordings, DAS annotates pulse song with 98% precision and 94% recall, and sine song\nwith 97% precision and 93% recall, and matches the performance of FSS (FSS pulse precision/recall\n99/92%, sine 95/93%) (Figure 1—figure supplement 3D–L). Annotations of multi-channel audio\nhave high temporal precision for pulse (DAS 0.3 ms, FSS 0.1 ms) and sine (DAS 8 ms, FSS 15 ms)\n(Figure 1—figure supplement 3E,J). Overall, DAS performs better or as well as the current state-ofthe-art method for annotating single and multi-channel recordings of fly song.\nMouse ultrasonic vocalizations\nMice produce ultrasonic vocalizations (USVs) in diverse social contexts ranging from courtship to\naggression (Sangiamo et al., 2020; Warren et al., 2020; Neunuebel et al., 2015). We tested DAS\nusing audio from an intruder assay, in which an anesthetized female was put into the home cage and\nthe USVs produced by a resident female or male were recorded (Ivanenko et al., 2020). The female\nUSVs from this assay typically consist of pure tones with weak harmonics and smooth frequency\nmodulations that are often interrupted by frequency steps (Figure 2A,B). The male USVs are similar\nbut also contain complex frequency modulations not produced by the females in this assay\n(Figure 2C,D). Recording noise from animal movement and interaction as well as the frequency\nsteps often challenge spectral threshold-based annotation methods and tend to produce false positive syllables (Tachibana et al., 2020; Coffey et al., 2019). Moreover, weak signals often lead to\nmissed syllables or imprecisely delimited syllables. We first trained and tested DAS on recordings of\na female mouse interacting with an anesthetized female intruder (Figure 2A). DAS annotates the\nfemale USVs with excellent precision (98%) and recall (99%) (Figure 2E) and low median temporal\nerror (0.3 ms) (Figure 2F). DAS is robust to noise: Even for weak signals (SNR 1/16) the recall is 90%\n(Figure 2G). These performance values are on par with that of methods specialized to annotate\nUSVs (Tachibana et al., 2020; Coffey et al., 2019; Van Segbroeck et al., 2017) (see Table 3). USVs\nof female and male residents have similar characteristics (Figure 2A,B) and the female-trained DAS\nnetwork also accurately annotated the male vocalizations (Figure 2H). Notably, even the male\nTable 1. Comparison to human annotators for fly song.\nSee also Figure 1E,J.\nAnnotator Sine recall [%] Sine precision [%] Pulse recall [%] Pulse precision [%]\nHuman A 89 98 99 93\nHuman B 93 91 98 88\nFSS 91 91 87 99\nDAS 98 92 96 97\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 5 of 25\nTools and resources Neuroscience\nsyllables with characteristics not seen in females in the paradigm were detected (Figure 2D). Overall,\nDAS accurately and robustly annotates mouse USVs and generalizes across sexes.\nTable 2. Precision, recall, and temporal error of DAS.\nPrecision and recall values are sample-wise for all except fly pulse song, for which it is event-wise. The number of classes includes the\n‘no song’ class. (p) Pulse, (s) Sine.\nSpecies Trained Classes Threshold Precision [%] Recall [%] Temporal error [ms]\nFly single channel Pulse (p) and sine (s) 3 0.7 97/92 (p/s) 96/98 (p/s) 0.3/12 (p/s)\nFly multi channel Pulse (p) 2 0.5 98 94 0.3\nFly multi channel Sine (s) 2 0.5 97 93 8\nMouse Female 2 0.5 98 99 0.3\nMarmoset five male-female pairs 5 0.5 85 91 4.4\nBengalese finch four males 49 (38 in test set) 0.5 97 97 0.3\nZebra finch one males 7 0.5 98 97 1.2\n40\n60\n80\n100\n120\n140\nFrequency [kHz]\n0\n1\nConfidence\nManual\nDAS\nNo song Song\nManual\nNo song\nSong\nDAS\n57.9\n0.3\n0.2\n13.0\n0 1 2 3 4\nTemporal error [ms]\n0.0\n0.5\n1.0\n1.5\nPDF\n1/16 1/4 1 4 16 64\nSignal-to-noise ratio\n0\n50\n100\nRecall [%]\nDAS\nSong\nNo song Song\nManual\n154.6\n0.4\n0.3\n24.1\n0\n100\n%\nƂ Ƃ ƃ\nC\nD\nE F H\nA\nB\nG\nNo song\nSong\nDAS\n0\n100\n%\n0.05 s 0.05 s\n-50\n0\ndB\nTrain , test Train , test Ƃ\nFigure 2. DAS performance for mouse ultrasonic vocalizations. (A) Waveform (top) and spectrogram (bottom) of USVs produced by a female mouse in\nresponse to an anesthetized female intruder. Shaded areas (top) show manual annotations. (B) Confidence scores (top) and DAS and manual\nannotations (bottom) for the female USVs in A. Brief gaps in confidence are filled smooth annotations. (C) Example of male USVs with sex-specific\ncharacteristics produced in the same assay. (D) Confidence scores (top) and DAS and manual annotations (bottom) for the male USVs in C from a DAS\nnetwork trained to detect female USVs. (E) Confusion matrix from a female-trained network for a test set of female USVs. Color indicates the\npercentage (see color bar) and text labels the seconds of song in each quadrant. (F) Distribution of temporal errors for syllable on- and offsets in female\nUSVs. The median temporal error is 0.3 ms for DAS (brown line) and 0.4 ms for USVSEG Tachibana et al., 2020, a method developed to annotate\nmouse USVs (gray line). (G) Recall of the female-trained network (brown line) as a function of SNR. The brown shaded area represents the distribution of\nSNRs for all samples containing USVs. Recall is high even at low SNR. (H) Confusion matrix of the female-trained DAS network for a test set of male\nUSVs (see C, D for examples). Color indicates the percentage (see color bar) and text labels the seconds of song in each quadrant.\nThe online version of this article includes the following figure supplement(s) for figure 2:\nFigure supplement 1. Performance for marmoset vocalizations.\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 6 of 25\nTools and resources Neuroscience\nMarmoset vocalizations\nWe next examined the robustness of annotations produced by DAS to noisy recordings and variable\nvocalization types, by training a network to annotate vocalization from pairs of marmosets\n(Landman et al., 2020). The recordings contain lots of background noises like faint calls from nearby\nanimals, overdrive from very loud calls of the recorded animals, and large variability within syllable\ntypes (Figure 2—figure supplement 1A–D). Recently, a deep-learning-based method was shown to\nproduce good performance (recall 77%, precision 85%, 12.5 ms temporal error) when trained on\n16,000 syllables to recognize seven vocalization types (Oikarinen et al., 2019). We trained DAS on\n1/9th of the data (1800 syllables) containing four of the seven vocalization types. Despite the noisy\nand variable vocalizations, DAS achieves high syllable-wise precision and recall (96%, 92%, (Figure 2—figure supplement 1E,F)). Note that DAS obtains this higher performance at millisecond resolution (temporal error 4.4 ms, Figure 2—figure supplement 1G), while the method by\nOikarinen et al., 2019 only produces annotations with a resolution of 50 ms (Table 2).\nBird song\nBird song is highly diverse and can consist of large, individual-specific repertoires. The spectral complexity and large diversity of the song complicates the annotation of syllable types. Traditionally, syllable types are annotated based on statistics derived from the segmented syllable spectrogram.\nRecently, good annotation performance has been achieved with unsupervised methods\n(Sainburg et al., 2020; Goffinet et al., 2021) and deep neural networks (Koumura and Okanoya,\n2016; Cohen et al., 2020). We first trained DAS to annotate the song from four male Bengalese\nfinches (data and annotations from Nicholson et al., 2017). The network was then tested on a random subset of the recordings from all four individuals which contained 37 of the 48 syllable types\nfrom the training set (Figure 3A,B, Figure 3—figure supplement 1A,B). DAS annotates the bird\nsong with high accuracy: Sample-wise precision and recall are 97% and syllable on- and offsets are\ndetected with sub-millisecond precision (median temporal error 0.3 ms, Figure 3C). The types of\n98.5% the syllables are correctly annotated, with only 0.3% false positives (noise annotated as a syllable), 0.2% false negatives (syllables annotated as noise), and 1% type confusions (Figure 3D, Figure 3—figure supplement 1C–D). This results in a low sequence error (corresponding to the\nminimal number of substitutions, deletions, or insertions required to transform the true sequence of\nsyllables into the inferred one) of 0.012. Overall, DAS performs as well as specialized deep-learningbased methods for annotating bird song (Koumura and Okanoya, 2016; Cohen et al.,\n2020, Table 2).\nTable 3. Comparison to alternative methods.\nMethods used for comparisons: (1) Arthur et al., 2013, (2) Tachibana et al., 2020, (3) Oikarinen et al., 2019, (4) Cohen et al., 2020.\n(A,B) DAS was trained on 1825/15970 syllables which contained 4/7 of the call types from Oikarinen et al., 2019. (B) The method by\nOikarinen et al., 2019 produces an annotation every 50 ms of the recording - since the on/offset can occur anywhere within the\n50 ms, the expected error of the method by Oikarinen et al., 2019 is at least 12.5 ms. (C) The method by Oikarinen et al., 2019\nannotates 60 minutes of recordings in 8 minutes. (D) Throughput assessed on the CPU, since the methods by Arthur et al., 2013 and\nTachibana et al., 2020 do not run on a GPU. (E) Throughput assessed on the GPU. The methods by Cohen et al., 2020 and\nOikarinen et al., 2019 use a GPU.\nPrecision [%] Recall [%] Jitter [ms] Throughput [s/s]\nSpecies DAS Other DAS Other DAS Other DAS Other\nFly single (1) 97/92 (p/s) 99/91 96/98 (p/s) 87/91 0.3/12 (p/s) 0.1/22 15 4 (D)\nFly multi (1) 98 99 94 92 0.3 0.1 8 (p) 0.4 (p+s) (D)\nFly multi (1) 97 95 93 93 8.0 15.0 8 (s) 0.4 (p+s) (D)\nMouse (2) 98 98 99 99 0.3 0.4 12 4 (D)\nMarmoset (3) 96 85 (A) 92 77 (A) 4.4 12.5 (B) 82 7.5 (C, E)\nBengalese finch (4) 99 99 99 99 0.3 1.1 15 5 (E)\nZebra finch (4) 100 100 100 100 1.3 2.0 18 5 (E)\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 7 of 25\nTools and resources Neuroscience\nTo further demonstrate the robustness of DAS, we trained a network to annotate song from\nZebra finches. In Zebra finch males, individual renditions of a given syllable type tend to be more\nvariable (Fitch et al., 2002). Moreover, the particular recordings used here (Goffinet et al., 2021)\ncontain background noise from the bird’s movement. Despite the variability and noise, DAS annotates the six syllables from a male’s main motif with excellent precision and recall, and low temporal\nerror, demonstrating that DAS is robust to song variability and recording noise (Figure 3—figure\nsupplement 2).\nIn summary, DAS accurately and robustly annotates a wide range of signals—from the pulsatile\nsong pulses of flies to the spectrally complex syllables of mammals and birds. DAS therefore constitutes a universal method for annotating acoustic signals that is as good as or better than methods\nspecialized for particular types of signals (Table 2).\nDAS is fast\nTo efficiently process large corpora of recordings and to be suitable for closed-loop applications,\nDAS needs to infer annotations quickly. We therefore assessed the throughput and latency of DAS.\nThroughput measures the rate at which DAS annotates song and high throughput means that large\ndatasets are processed quickly. Across the five species tested here, DAS has a throughput of 8-82x\nrealtime on a CPU and of 24-267x on a GPU (Figure 4A). This means that a 60 min recording is\nannotated in less than five minutes on a standard desktop PC and in less than 1.5 minutes using a\nGPU, making the annotation of large datasets feasible (Figure 4—figure supplement 1A–H). The\ndifferences in throughput arise from the different sample rates and network architectures: The marmoset network is fastest because of a relatively shallow architecture with only 2 TCN blocks and a\nlow sampling rate (44.1 kHz). By contrast, the multi-channel Drosophila networks have the lowest\nthroughput because of multi-channel inputs (9 channels at 10.0 kHz) and a comparatively deep architecture with four TCN blocks (Table 4).\nA\n0.1 s\n0.5\n1.0\n2.0\n4.0\n8.0\nFrequency [kHz]\nB\nC\nDAS\nManual\n0 20\nDAS\n0\n10\n20\n30\nManual\n0 1 2 3 4\nTemporal error [ms]\n0.0\n1.0\n2.0\nPDF\nD\nSong\nNo song\n%\n-70\n0\ndB\n0.1\n1\n10\n100\nFigure 3. DAS performance for the song of Bengalese finches. (A) Waveform (top) and spectrogram (bottom) of the song of a male Bengalese finch.\nShaded areas (top) show manual annotations colored by syllable type. (B) DAS and manual annotation labels for the different syllable types in the\nrecording in A (see color bar). DAS accurately annotates the syllable boundaries and types. (C) Confusion matrix for the different syllables in the test\nset. Color was log-scaled to make the rare annotation errors more apparent (see color bar). Rows depict the probability with which DAS annotated each\nsyllable as any of the 37 types in the test dataset. The type of 98.5% of the syllables were correctly annotated, resulting in the concentration of\nprobability mass along the main diagonal. (D) Distribution of temporal errors for the on- and offsets of all detected syllables (green-shaded area). The\nmedian temporal error is 0.3 ms for DAS (green line) and 1.1 ms for TweetyNet Cohen et al., 2020, a method developed to annotate bird song (gray\nline).\nThe online version of this article includes the following figure supplement(s) for figure 3:\nFigure supplement 1. Performance for the song of Bengalese finches.\nFigure supplement 2. Performance for the song of a Zebra finch.\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 8 of 25\nTools and resources Neuroscience\nA\nB\n1 channel 9 channels\nPulse Sine Pulse Sine Mouse B. finch\n1 channel\nAnnotate\nby hand\nFast-train\nAnnotate\nPredict on\nnew data\nCorrect\npredictions\nFull-train\nC EValidation loss\n0.3\n0.2\n0.1\n0.06\n0 5 10 15\nMinutes of training\n1.0\n10.0\n100.0\nThroughput\n1x realtime\n10x realtime\n40x realtime\n0\n10\n20\nLatency [ms]\nCPU\nGPU\nMarmoset Z. finch\n100x realtime\n1\n10\n100\n1000\nNumber of\nannotations\n1 channel 9 channels\nPulse Sine Pulse Sine Mouse B. finch\n1 channel\nD 1611\n632\n48\n22\n264 272\n44\n16\nMarmoset Z. finch\nFigure 4. DAS annotates song with high throughput and low latency and requires little data. (A, B) Throughput (A) and latency (B) of DAS (see also\nFigure 4—figure supplement 1). Throughput (A) was quantified as the amount of audio data in seconds annotated in one second of computation\ntime. Horizontal lines in A indicate throughputs of 1, 10, 40, and 100. Throughput is >8x realtime on a CPU (dark shades) and >24x or more on a GPU\n(light shades). Latency (B) corresponds to the time it takes to annotate a single chunk of audio and is similar on a CPU (dark shades) and a GPU (light\nshades). Multi-channel audio from flies was processed using separate networks for pulse and sine. For estimating latency of fly song annotations, we\nused networks with 25 ms chunks, not the 410 ms chunks used in the original network (see Figure 1—figure supplement 2). (C) Flow diagram of the\niterative protocol for fast training DAS. (D) Number of manual annotations required to reach 90% of the performance of DAS trained on the full data set\nshown in Figure 1, Figure 1—figure supplement 3, Figure 2, Figure 2—figure supplement 1, Figure 3, and Figure 3—figure supplement 2 (see\nalso Figure 4—figure supplement 3). Performance was calculated as the F1 score, the geometric mean of precision and recall. For most tasks, DAS\nrequires small to modest amounts of manual annotations. (E) Current best validation loss during training for fly pulse song recorded on a single channel\nfor 10 different training runs (red lines, 18 min of training data). The network robustly converges to solutions with low loss after fewer than 15 min of\ntraining (40 epochs).\nThe online version of this article includes the following figure supplement(s) for figure 4:\nFigure supplement 1. Throughput and latency of inference.\nFigure supplement 2. Reducing the chunk duration reduces latency and comes with minimal performance penalties.\nFigure supplement 3. DAS requires small to moderate amounts of data for training.\nFigure supplement 4. Example of fast training mouse USVs.\nFigure supplement 5. DAS performance is robust to changes in the structural parameters of the network.\nTable 4. Structural parameters of the tested networks.\nSpecies Rate [kHz] Chunk [samples] Channels STFT downsample Separable conv. TCN stacks Kernel size [samples] Kernel\nFly single channel 10.0 4096 1 - - 3 32 32\nFly multi channel (pulse) 10.0 2048 9 - TCN blocks 1+2 4 32 32\nFly multi channel (sine) 10.0 2048 9 - TCN blocks 1+2 4 32 32\nMouse 300.0 8192 1 16x - 2 16 32\nMarmoset 44.1 8192 1 16x - 2 16 32\nBengales finch 32.0 1024 1 16x - 4 32 64\nZebra finch 32.0 2048 1 16x - 4 32 64\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 9 of 25\nTools and resources Neuroscience\nA measure of speed crucial for closed-loop experiments is latency, which quantifies the time it\ntakes to annotate a chunk of song and determines the delay for experimental feedback. Latencies\nare short, between 7 and 15 ms (Figure 4B, Figure 4—figure supplement 1I–P) on CPUs and\nGPUs. One network parameter impacting latency is the chunk size—the duration of audio processed\nat once—and we find that for fly song, latency can be optimized by reducing chunk size with a minimal impact on accuracy (Figure 4—figure supplement 2). The low latency of annotation makes DAS\nwell suited for triggering realtime optogenetic or acoustic feedback upon the detection of specific\nvocalizations (Bath et al., 2014; Stowers et al., 2017).\nWe also compared the speed of DAS to that of other methods that were specifically developed\nto annotate the types of signals tested here. Since most existing methods are not suitable for estimating latency due to constraints in their design and interface, we only compared throughput. We\nfind that DAS achieves 3x to 10x higher throughput than existing methods (Table 2). This has three\nmain reasons: First, the relatively simple, purely convolutional architecture exploits the parallel processing capabilities of modern CPUs and GPUs. Second, Fourier or wavelet-like preprocessing steps\nare integrated into the network and profit from a fast implementation and hardware acceleration.\nThird, for multi-channel data, DAS combines information from all audio channels early, which\nincreases throughput by reducing the data bandwidth.\nOverall, DAS annotates audio with high throughput (>8x realtime) and low latency (<15 ms) and\nis faster than the alternative methods tested here. The high speed renders DAS suitable for annotating large corpora and for realtime applications without requiring specialized hardware.\nDAS requires little manual annotation\nTo be practical, DAS should achieve high performance with little manual annotation effort. We find\nthat DAS can be efficiently trained using an iterative protocol (Pereira et al., 2019, Figure 4C, Figure 4—figure supplement 4): Annotate a small set of recordings and train the network for a few\nepochs; then generate annotations on a larger set of recordings and correct these annotations.\nRepeat the predict-correct-train cycle on ever larger datasets until performance is satisfactory. To\nestimate the amount of manual annotations required to achieve high performance, we evaluated\nDAS trained on subsets of the full training data sets used above (Figure 4—figure supplement 3).\nWe then took the number of manual annotations needed to reach 90% of the performance of DAS\ntrained on the full data sets as an upper bound on the data requirements (Figure 4D). With a performance threshold of 90%, the resulting networks will produce sufficiently accurate annotations for\ncreating a larger body of training data with few corrections. Performance was taken as the F1 score,\nthe geometric mean of precision and recall. For single-channel recordings of fly song, fewer than 50\npulses and 20 sine songs are needed to reach 90% of the performance achieved with the full data\nset. For mouse vocalizations, DAS achieves 90% of its peak performance with fewer than 25 manually\nannotated syllables. Even for the six syllables from a zebra finch, DAS reaches the 90% threshold\nwith only 48 manually annotated syllables (eight per type). Manually annotating such small amounts\nof song for flies, mice, or zebra finches takes less than 5 min. Likewise, for the song of Bengalese\nfinches, 1–51 (median 8, mean 17) manual annotations are required per syllable type, with one outlier requiring 200 syllables (Figure 4—figure supplement 3C). Closer inspection reveals that the outlier results from an annotation error and consists of a mixture of three distinct syllable types\n(Figure 4—figure supplement 3D–F). Even with this outlier, only 626 manually annotated syllabes\n(424 without) are required in total to reach 90% of the test performance of a network trained on\n>3000 annotated syllables. Data requirements are higher for the multi-channel recordings of fly song\n(270 pulses and sine songs), and for the noisy and variable marmoset data (1610 annotations, 400\nper type), but even in these cases, the iterative training protocol can reduce the manual annotation\nwork.\nOverall, DAS requires small to moderate amounts of data for reaching high performance. High\nthroughput (Figure 4A) and small training data sets (Figure 4D) translate to short training times\n(Figure 4E). The single-channel data sets typically achieve 90% of the performance after less than 10\nmin of training on a GPU. Training on the full data sets typically finishes in fewer than five hours.\nThus, DAS can be adapted to novel species in short time and with little manual annotation work.\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 10 of 25\nTools and resources Neuroscience\nDAS can be combined with unsupervised methods\nDAS is a supervised annotation method: It discriminates syllable types that have been manually\nassigned different labels during training. By contrast, unsupervised methods can determine in unlabelled data whether syllables fall into distinct types and if so, classify the syllables (Tabler et al.,\n2017; Coffey et al., 2019; Clemens et al., 2018; Goffinet et al., 2021; Sainburg et al., 2020;\nSangiamo et al., 2020; Arthur et al., 2021). While DAS does not require large amounts of manual\nannotations (Figure 4D), manual labeling of syllable types can be tedious when differences between\nsyllable types are subtle (Clemens et al., 2018) or when repertoires are large (Sangiamo et al.,\n2020). In these cases, combining DAS with unsupervised methods facilitates annotation work. To\ndemonstrate the power of this approach, we use common procedures for unsupervised classification,\nwhich consist of an initial preprocessing (e.g. into spectograms) and normalization (e.g. of amplitude)\nof the syllables, followed by dimensionality reduction and clustering (see Materials and methods)\n(Clemens et al., 2018; Sainburg et al., 2020; Sangiamo et al., 2020).\nFor fly song, DAS was trained to discriminate two major song modes, pulse and sine. However,\nDrosophila melanogaster males produce two distinct pulse types, termed Pslow and\nPfast (Clemens et al., 2018), and unsupervised classification robustly discriminates the two pulse\ntypes as well as the sine song in the DAS annotations (Figure 5A–C). Mouse USVs do not fall into\ndistinct types (Tabler et al., 2017; Goffinet et al., 2021; Sainburg et al., 2020). In this case, unsupervised clustering produces a low-dimensional representation that groups the syllables by the similarity of their spectrograms (Tabler et al., 2017; Coffey et al., 2019; Sangiamo et al., 2020,\nFigure 5D,E). For marmosets, unsupervised classification recovers the four manually defined call\ntypes (Figure 5F,G). However, most call types are split into multiple clusters, and the clusters for\ntrills and twitters tend to separate poorly (Figure 4—figure supplement 3G), which reflects the\nlarge variability of the marmoset vocalizations. This contrasts with the song of Zebra finches, for\nwhich the unsupervised method produces a one-to-one mapping between manually defined and\nunsupervised syllable types (Goffinet et al., 2021; Sainburg et al., 2020; Figure 5H,I). For the song\nof Bengalese finches, the unsupervised classification recovers the manual labeling (Goffinet et al.,\n2021; Sainburg et al., 2020; Figure 5J,K) and reveals manual annotation errors: For instance, the\nsong syllable that required >200 manual annotations to be annotated correctly by DAS is a mixture\nof three distinct syllable types (Figure 4—figure supplement 3C–F).\nOverall, unsupervised methods simplify annotation work: DAS can be trained using annotations\nthat do not discriminate between syllable types and the types can be determined post hoc. If distinct\ntypes have been established, DAS can be retrained to directly annotate these types using the labels\nproduced by the unsupervised method as training data.\nDiscussion\nWe here present Deep Audio Segmenter (DAS), a method for annotating acoustic signals. DAS\nannotates song in single- and multi-channel recordings from flies (Figure 1, Figure 1—figure supplement 3), mammals ( Figure 2, Figure 2—figure supplement 1), and birds (Figs Figure 3, Figure 3—figure supplement 2) accurately, robustly, and quickly (Figure 4A,B). DAS performs as well\nas or better than existing methods that were designed for specific types of vocalizations\n(Koumura and Okanoya, 2016; Cohen et al., 2020; Tachibana et al., 2020; Coffey et al., 2019,\nTable 2). DAS performs excellently for signals recorded on single and multiple channels (Figure 1,\nFigure 1—figure supplement 3), with different noise levels, and with diverse characteristics. This\nsuggests that DAS is a general method for accurately annotating signals from a wide range of\nrecording setups and species.\nUsing a user-friendly graphical interface, our method can be optimized for new species without\nrequiring expert knowledge and with little manual annotation work (Figure 4C–E). Network performance is robust to changes in the structural parameters of the network, like filter number and duration, or the network depth (Figure 4—figure supplement 5). Thus, the structural parameters do not\nneed to be finely tuned to obtain a performant network for a new species. We have trained networks\nusing a wide range of signal types (Table 4) and these networks constitute good starting points for\nadapting DAS to novel species. We provide additional advice for the design of novel networks in\nMethods. This makes the automatic annotation and analysis of large corpora of recordings from\ndiverse species widely accessible.\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 11 of 25\nTools and resources Neuroscience\nWe show that the annotation burden can be further reduced using unsupervised classification of\nsyllable types, in particular for species with large or individual-specific repertoires (Figure 5,\nClemens et al., 2018; Tabler et al., 2017; Coffey et al., 2019; Goffinet et al., 2021;\nSainburg et al., 2020; Arthur et al., 2021). In the future, incorporating recent advances in the selfsupervised or semi-supervised training of neural networks will likely further reduce data requirements\n(Mathis et al., 2021; Raghu et al., 2019; Devlin et al., 2019; Chen and He, 2020). These\napproaches use unlabeled data to produce networks with a general and rich representation of sound\nfeatures that can then be fine-tuned for particular species or individuals using few annotated samples. DAS currently does not work well with recordings in which the signals produced by multiple\nanimals overlap. In the future, DAS will be extended with methods for multi-speaker speech recognition to robustly annotate vocalizations from animal groups.\nLastly, the high inference speed (Figure 4A,B) allows integration of DAS in closed-loop systems\nin which song is detected and stimulus playback or optogenetic manipulation is triggered with low\nlatency (Bath et al., 2014; Stowers et al., 2017). In combination with realtime pose tracking\n(Mathis et al., 2018; Pereira et al., 2019; Graving et al., 2019), DAS provides unique opportunities\nto tailor optogenetic manipulations to specific behavioral contexts, for instance to dissect the neural\ncircuits underlying acoustic communication in interacting animals (Coen et al., 2014; Fortune et al.,\n2011; Okobi et al., 2019).\nA\nSine\nPslow\nPfast\nB\nC\n4 ms\nJ\nK\nD\nE\nH\nI\n0.5\n8.0kHz\n0.1 s\n-70\n-10\ndB\nek\ntwitter tsik\ntrill\nF\nG\nek\ntwitter\ntsik\ntrill\nFigure 5. DAS can be combined with unsupervised methods for song classification. (A) Low-dimensional representation obtained using the UMAP\n(McInnes and Healy, 2018) method of all pulse and sine song waveforms from Drosophila melanogaster annotated by DAS in a test data set. Data\npoints correspond to individual waveforms and were clustered into three distinct types (colors) using the density-based method HDBSCAN\n(McInnes et al., 2017). (B, C) All waveforms (B) and cluster centroids (C) from A colored by the cluster assignment. Waveforms cluster into one sine\n(blue) and two pulse types with symmetrical (red, Pslow) and asymmetrical (orange, Pfast) shapes. (D, E) Low-dimensional representation of the\nspectrograms of mouse USVs (D) and mean spectrogram for each cluster in D (E). Individual syllables (points) form a continuous space without distinct\nclusters. Song parameters vary continuously within this space, and syllables can be grouped by the similarity of their spectral contours using k-means\nclustering. (F, G) Low-dimensional representation of the spectrograms of the calls from marmosets (F) and mean spectrogram for each cluster in F (G).\nCalls separate into distinct types and density-based clustering (colors) produces a classification of syllables that recovers the manual annotations\n(Figure 4—figure supplement 3G, homogeneity score 0.88, completeness score 0.57, v-score 0.69). Most types split into multiple clusters, reflecting\nthe variability of the call types in marmosets. Colored bars on top of each spectrogram in G correspond to the colors for the individual clusters in F.\nThe dashed line shows the boundary separating trills and twitters. (H, I) Low-dimensional representation of the spectrograms of the syllables from one\nZebra finch male, mean spectrogram for each cluster in H (I, top), and example of each clustered syllable within the motif (I, bottom). Density-based\nclustering (colors) recovers the six syllable types forming the male’s motif. Colored bars on top of each spectrogram in I correspond to the colors for\nthe individual clusters in H. (J, K) Low-dimensional representation of the spectrograms of the syllables from four Bengalese finch males (J) and mean\nspectrogram for each cluster in J (K). Syllables separate into distinct types and density-based clustering (colors) produces a classification of syllables\nthat closely matches the manual annotations (homogeneity score 0.96, completeness score 0.89, v-score 0.92). X-axes of the average spectrograms for\neach cluster do not correspond to linear time, since the spectrograms of individual syllables were temporally log-rescaled and padded prior to\nclustering. This was done to reduce the impact of differences in duration between syllables.\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 12 of 25\nTools and resources Neuroscience\nMaterials and methods\nInstructions for installing and using DAS can be found at https://janclemenslab.org/das. Code for\nthe das python module is available at https://github.com/janclemenslab/das, code for the unsupervised methods is at https://github.com/janclemenslab/das_unsupervised. All fitted models (with\nexample data and code) can be found at https://github.com/janclemenslab/das-menagerie.\nWe also provide instructions for training DAS using google colab, which provides a GPU-accelerated python environment. Colab removes the need to install GPU libraries: Annotations can be\nmade locally in the GUI without a GPU and training and prediction are done on GPU-accelerated\nnodes in the cloud. See this notebook for details: http://janclemenslab.org/das/tutorials/colab.html.\nData sources\nAll data used for testing DAS were published previously. Sources for the original data sets, for the\ndata and annotations used for training and testing, and for the fitted models are listed in Table 5.\nSingle-channel recordings of Drosophila melanogaster (strain OregonR) males courting females were\ntaken from Stern, 2014. The multi-channel data from Drosophila melanogaster (strain NM91) males\ncourting females were recorded in a chamber tiled with nine microphones (Coen et al., 2014) and\nwas previously published in Clemens et al., 2018. Annotations for fly song were seeded with FlySongSegmenter (Arthur et al., 2013; Coen et al., 2014) and then manually corrected. Recordings\nof mouse USVs were previously published in Ivanenko et al., 2020. The USVs were manually labeled\nusing the DAS graphical interface. Marmoset recordings were taken from the data published with\nLandman et al., 2020. Since we required more precise delineation of the syllable boundaries than\nwas provided in the published annotations, we manually fixed annotations for a subset of the data\nthat then was used for training and testing. The network was trained and tested on a subset of four\nvocalization types (eks/trills/tsiks/twitters, N=603/828/115/868). The remaining vocalization types\nwere excluded since they had 60 or fewer instances in our subset. To test DAS on bird songs, we\nused a publicly available, hand-labeled collection of song from four male Bengalese finches\nTable 5. Sources of all data used for testing DAS.\n‘Data’ refers to the data used for DAS and to annotations that were either created from scratch or modified from the original annotations (deposited under https://data.goettingen-research-online.de/dataverse/das). ‘Original data’ refers to the recordings and annotations deposited by the authors of the original publication. ‘Model’ points to a directory with the model files as well as a small test data\nset and demo code for running the model (deposited under https://github.com/janclemenslab/das-menagerie).\nSpecies Reference Data and model repositories\nFly single channel Stern, 2014 data: https://doi.org/10.25625/TP4ODR\noriginal data: https://www.janelia.org/lab/stern-lab/tools-reagents-data\nmodel: https://github.com/janclemenslab/das-menagerie/dmel_single\nFly multi channel Clemens et al., 2018 data: https://doi.org/10.25625/8KAKHJ\nmodel: https://github.com/janclemenslab/das-menagerie/dmel_multi\nMouse Ivanenko et al., 2020 data: https://doi.org/10.25625/VVSKCH\noriginal data: https://data.donders.ru.nl/collections/di/dcn/DSC_620840_0003_891\nmodel: https://github.com/janclemenslab/das-menagerie/mouse\nMarmoset Landman et al., 2020 data: https://doi.org/10.25625/DYG3KV\noriginal data: https://osf.io/q4bm3/\nmodel: https://github.com/janclemenslab/das-menagerie/marmoset\nBengalese finch Nicholson et al., 2017 data: https://doi.org/10.25625/ENKMJS\noriginal data: https://doi.org/10.6084/m9.figshare.4805749.v6\nmodel: https://github.com/janclemenslab/das-menagerie/bengalese_finch\nZebra finch Goffinet et al., 2021 data: https://doi.org/10.25625/ZXJJJY\noriginal data: https://research.repository.duke.edu/concern/datasets/9k41zf38g\nmodel: https://github.com/janclemenslab/das-menagerie/zebra_finch\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 13 of 25\nTools and resources Neuroscience\n(Nicholson et al., 2017) and recordings of female-directed song from a male Zebra finch from\nGoffinet et al., 2021. For training and testing the Zebra finch network, we manually labeled 473 syllables of six types (320 s of recordings).\nDAS network\nDAS is implemented in Keras (Chollet, 2015) and Tensorflow (Abadi et al., 2016). At its core, DAS\nconsists of a stack of temporal convolutional blocks, which transform an input sequence of audio\ndata into an output sequence of labels.\nInputs\nDAS takes as input raw, single or multi-channel audio. Pre-processing of the audio using a wavelet\nor short-time Fourier transform (STFT) is optional and integrated into the network. DAS processes\naudio in overlapping chunks (Figure 1—figure supplement 1A–D). The chunking accelerates annotations since multiple samples are annotated in a single computational step. Edge effects are\navoided by processing overlapping chunks and by discarding a number of samples at the chunk\nboundaries. The overlap depends on the number of layers and the duration of filters in the network.\nSTFT frontend\nThe trainable STFT frontend is an optional step and was implemented using kapre (Choi et al.,\n2017). Each frequency channel in the output of the frontend is the result of two, one-dimensional\nstrided convolutions which are initialized with the real and the imaginary part of discrete Fourier\ntransform kernels:\nxði;fÞ ¼XT1\nt ¼0\nxði  sþ t Þ½cosð2pf t =TÞ isinð2pf t =TÞ\nyði;fÞ ¼ log10ð\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n<xði;fÞ\n2 þ =xði;fÞ\n2\nq\nÞ\nwhere f is the frequency, s is the stride, and T is the filter duration. The stride results in downsampling of the input by a factor s.\nThe STFT kernels are optimized with all other parameters of the network during training. The\nSTFT frontend was used for mammal and bird signals, but not for fly song. In the mammal and bird\nnetworks, we used 33 STFT filter pairs with a duration T ¼ 64 samples and a stride s ¼ 16 samples.\nFor mouse and bird song, the STFT frontend sped up training and inference, and had a small positive impact on performance. For fly song, the STFT frontend tended to reduce performance and was\nomitted.\nTemporal convolutional blocks\nTemporal convolutional network (TCN) blocks are central to DAS and produce a task-optimized hierarchical representation of sound features at high temporal resolution (Bai et al., 2018). Each TCN\nblock consists of a stack of so-called residual blocks (Figure 1—figure supplement 1E, He et al.,\n2016):\nA dilated convolutional layer filters the input with a number of kernels of a given duration:\nyiðtÞ ¼ P\nt ;g\nkiðt ; gÞ  xðt  at ; gÞ, where kiðt ; gÞ is the i th kernel, xðt; gÞ is the input on channel g at\ntime t, yiðtÞ the output, and a the gap or skip size (Yu and Koltun, 2016). In old-fashioned convolution a ¼ 1. Increasing a allows the kernel to span a larger range of inputs with the same number of\nparameters and without a loss of output resolution. The number of parameters is further reduced for\nnetworks processing multi-channel audio, by using separable dilated convolutions in the first two\nTCN blocks (Mamalet and Garcia, 2012). In separable convolutions, the full two-dimensional kðt ; gÞ\nconvolution over times and channels is decomposed into two one-dimensional convolutions. First, a\ntemporal convolution, k\nt\nðt ; 1Þ, is applied to each channel and then N channel convolutions, k\ng\nð1; gÞ,\ncombine information across channels. Instead of t  g parameters, the separable convolution only\nrequires t þ N  g parameters. Note that each temporal convolution is applied to each channel,\nleading to a sharing of filter parameters across channels. This makes explicit the intuition that some\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 14 of 25\nTools and resources Neuroscience\noperations should be applied to all channels equally. We also tested an alternative implementation,\nin which individual channels were first processed separately by a single-channel TCN, the outputs of\nthe TCN blocks for each channel were concatenated, and then fed into a stack of standard TCNs\nwith full two-dimensional convolutions. While this architecture slightly increased performance it was\nalso much slower and we therefore chose the architecture with separable convolutions. Architecture\nchoice ultimately depends on speed and performance requirements of the annotation task.\nA rectifying linear unit transmits only the positive inputs from the dilated convolutional layer by\nsetting all negative inputs to 0: yi ¼ maxð0; yiÞ.\nA normalization layer rescales the inputs to have a maximum absolute value close to 1:\nyi=ðmaxðjyi\njÞ þ 105\nÞ.\nThe output of the residual block, zðtÞ, is then routed to two targets: First, it is added to the input:\noðtÞ ¼ xðtÞ þ zðtÞ and fed into subsequent layers. Second, via so-called skip connections, the outputs\nof all residual blocks are linearly combined to produce the network’s final output (van den Oord\net al., 2016).\nA single TCN block is composed of a stack of five residual blocks. Within a stack, the skip size a\ndoubles - from one in the first to 2\n5 ¼ 16 in the final residual block of a stack. This exponential\nincrease in the span of the filter t  a allows the TCN block to produce a hierarchical representation\nof its inputs, from relatively low-level features on short timescales in early stacks to more derived features on longer timescales in late stacks. Finally, a full network consists of a stack of 2 to 4 TCN\nblocks, which extract ever more derived features (Figure 1—figure supplement 1A–D).\nOutput\nThe network returns a set of confidence scores—one for each song type (and for ’no song’)—for\neach sample from a linear combination of the output of each residual block in the network, by using\na single dense layer and a softmax activation function. Re-using information from all blocks via socalled skip connections ensures that downstream layers can discard information from upstream\nlayers and facilitates the generation of specialized higher order presentations. If the input recording\ngot downsampled by a STFT frontend, a final upsampling layer restores the confidence scores to the\noriginal audio rate by repeating values. The parameters of all used networks are listed in Table 4.\nChoice of structural network parameters\nDAS performance is relatively robust to the choice of structural network parameters like filter duration and number, or network depth (Figure 4—figure supplement 5). The networks tested here are\ngood starting points for adapting DAS to your own data (Table 4). In our experience, a network with\n32 filters, filter duration 32 samples, 3 TCN blocks, and a chunk duration of 2048 samples will produce good results for most signals. A STFT downsampling layer with 32 frequency bands and 16x\ndownsampling should be included for most signals except when the signals have a pulsatile character. These parameters have been set as defaults when creating a new DAS network. Given that DAS\ntrains quickly (Figure 4E), network structure can be optimized by training DAS networks over a grid\nof structural parameters, for instance to find the simplest network (in terms of the number of filters\nand TCN blocks) that saturates performance but has the shortest latency. We here provide additional guidelines for choosing a network’s key structural parameters:\nThe chunk duration corresponds to the length of audio the network processes in one step and\nconstitutes an upper bound for the context available to the network. Choose chunks sufficiently long\nso that the network has access to key features of your signal. For instance, for fly song, we ensured\nthat a single chunk encompasses several pulses in a train, so the network can learn to detect song\npulses based on their regular occurrence in trains. Longer chunks relative to this timescale can\nreduce short false positive detections, for instance for fly sine song and for bird song. Given that\nincreasing chunk duration does not increase the number of parameters for training, we recommend\nusing long chunks unless low latency is of essence (see below).\nDownsampling/STFT weakly affects performance but strongly accelerates convergence during\ntraining. This is because (A) the initialization with STFT filters is a good prior that reduces the number\nof epochs it takes to learn the optimal filters, and (B) the downsampling reduces the data bandwidth\nand thereby the time it takes to finish one training epoch. The overall increase in performance from\nadding the STFT layer is low because convolutional layers in the rest of the network can easily\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 15 of 25\nTools and resources Neuroscience\nreplicate the computations of the STFT layer. For short pulsatile signals or signals with low sampling\nrates, STFT and downsampling should be avoided since they can decrease performance due to the\nloss of temporal resolution.\nThe number of TCN blocks controls the network’s depth. A deeper network can extract more\nhigh-level features, though we found that even for the spectro-temporally complex song of Bengalese finches, deeper networks only weakly improved performance (Figure 4—figure supplement 5).\nMulti-channel audio can be processed with multi-channel filters via full convolutions or with\nshared channel-wise filters via time-channel separable convolutions. This can be set on a per-TCNblock basis. We recommend to use separable convolutions in the first 1–2 layers, since basic feature\nextraction is typically the same for each channel. Later layers can then have full multi-channel filters\nto allow more complex combination of information across channels.\nReal-time performance can be optimized by reducing networks complexity and chunk duration\n(Figure 4—figure supplement 2). We recommend starting with the default parameters suggested\nabove and then benchmarking latency. If required, latency can then be further reduced by reducing\nchunk duration, the number and duration of filters, and the number of TCN blocks.\nTraining\nNetworks were trained using the categorical cross-entropy loss and the Adam optimizer\n(Kingma and Ba, 2015) with a batch size of 32. Prediction targets were generated from fully annotated recordings and one-hot-encoded: Segments were coded as binary vectors, with yiðtÞ ¼ 1 if a\nsegment of type i occurred at time t, and yiðtÞ ¼ 0 otherwise. To encode uncertainty in the timing of\nfly song pulses, the pulses were represented as Gaussian bumps with a standard deviation of\n1.6 ms. A ’no song’ type was set to ynosongðtÞ ¼ 1\nP\ni\nyiðtÞ. That way, y corresponds to the probability of finding any of the annotated song types or no song. For bird song, short gaps (6.25 ms, 200\nsamples at 32 kHz) were introduced between adjacent syllables to aid the detection of syllable onand offsets after inference. That way, syllable on- and offsets could be unequivocally detected as\nchanges from ‘no song’ to any of the syllables. This reduced the amount of false positive on- and offsets from switches in the inferred label within a syllable.\nTypically, multiple fully annotated recordings were combined in a data set. Each recording was\nsplit 80:10:10 into a training, validation, and test set. The validation and test data were randomly\ntaken from the first, the middle or the last 10% of each recording. Given the uneven temporal distribution of call types in the marmoset recordings, we split the data 60:20:20 to ensure that each call\ntype was well represented in each split. For all networks, training was set to stop after 400 epochs\nor earlier if the validation loss was not reduced for at least 20 epochs. Training typically stopped\nwithin 40–80 epochs depending on the dataset. The test set was only used after training, for evaluating the model performance.\nGeneration of annotations from the network output\nThe confidence scores produced by the model correspond to the sample-wise probability for each\nsong type. To produce an annotation label for each sample, the confidence scores were further\nprocessed to extract event times and syllable segments. In the resulting annotations, song types are\nmutually exclusive, that is, each sample is labeled as containing a single song type even if song types\noverlap.\nEvent times for event-like song types like fly pulse song were determined based on local maxima\nin the confidence score, by setting a threshold value between 0 and 1 and a minimal distance\nbetween subsequent peaks (using peakutils, Negri and Vestri, 2017). For the pulse song of flies,\nwe set a minimal distance of 10 ms and a threshold of 0.7 for single channel data (Figure 1) and 0.5\nfor multi-channel data (Figure 1—figure supplement 3).\nFor segment-like song types like fly sine song or the syllables of mouse, marmoset, and bird\nsong, we first transformed the sample-wise probability into a sequence of labels using argmaxi\nyði; tÞ.\nThe resulting annotation of segments was then smoothed by filling short gaps (flies 20 ms, mice 10\nms, marmosets and birds 5 ms) and removing very short detections (flies 20 ms, mice 5 ms, marmosets and birds 30 ms). These values were chosen based on the statistics of song found in the training\ndata. Syllable on- and offsets were detected as changes from no-song to song and song to no-song,\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 16 of 25\nTools and resources Neuroscience\nrespectively. For bird and marmoset vocalizations, syllable labels were determined based on a\nmajority vote, by calculating the mode of the sample-wise labels for each detected syllable.\nEvaluation\nDAS was evaluated on segments of recordings that were not used during training.\nEvents\nFor events—fly pulse song, or the on- and offsets of segments—we matched each true event with its\nnearest neighbor in the list of true events and counted as true positives only events within a specified distance from a true event. For the pulse song of flies as well as for the onsets and offsets of\nmouse, marmoset, and bird syllables, this distance was set to 10 ms. Results were robust to the specific choice of the distance threshold (Figure 1—figure supplement 2A). For the onsets and offsets\nof fly sine song and of the marmoset vocalizations, we set this distance to 40 ms, since these signals\ntended to fade in and out, making the delineation of exact boundaries difficult. False positive events\nwere counted if the distance from a detected event to the nearest true event exceeded the distance\nthreshold or if another detected event was closer to each true event within the distance threshold. If\nseveral detected pulses shared the same nearest true pulses, only the nearest of those was taken as\na true positive, while the remaining detections were matched with other true pulses within the distance threshold or counted as false positives.\nFalse negatives were counted as all true events without nearby detected events. For pulse,\npseudo true negative events were estimated as the number of tolerance distances (2x tolerance distance) fitting into the recording, minus the number of pulses. These true negatives for pulse do not\ninfluence precision, recall, and F1-scores and are only used to fill the confusion matrices in\nFigure 1D,I and Figure 1—figure supplement 3C,H. Pulse and sine song were evaluated only up to\nthe time of copulation.\nMatching segment labels\nFor songs with only one syllable type, we compared the predicted and true labels for each sample\nto compute the confusion matrix (Figure 1—figure supplement 1F,G). In the case of multiple syllable types, the mode of the true and predicted labels for the samples of each detected syllable were\ncompared. A true positive was counted if the mode of the true labels was the same for the samples\ncovered by the detected syllable. Using the true syllables as reference produces similar results (Figure 2—figure supplement 1E,F and Figure 3—figure supplement 1D,E).\nPerformance scores\nFrom the false negative (FN), false positive (FP), and true positive (TP) counts we extracted several\nscores: Precision (P)—the fraction of true positive out of all detections TP/(FP+TP)—and recall (R)—\nthe fraction of true positives out of all positives TP/(TP+FN). The F1 score combines precision and\nrecall via their geometric mean: 2  P  R=ðP þ RÞ. For datasets with many different syllable types,\nwe also used as a summary measure of performance the accuracy—the fraction of correctly labelled\nsyllables: (TP+TN)/(TP+TN+FP+FN). For comparison with other studies, we additionally provide the\nerror rate for the song of Bengalese finches, which is based on the Levenshtein edit distance and\ncorresponds to the minimal number of inserts, deletions, and substitutions required to transform the\nsequence of true syllable labels into the sequence of predicted syllable labels normalized by the\nlength of the true sequence (Koumura and Okanoya, 2016; Cohen et al., 2020).\nTemporal precision\nThe temporal precision for events (pulses, syllable onsets and offsets) was calculated as the median\nabsolute distance between all matched events.\nAnnotation errors for Bengalese finches\nThe network for annotating bird song was trained on all syllable types. We removed from the test\ndata one syllable type with only a single instance in the test set (which was correctly classified),\nbecause the performance could not be assessed reliably based on a single instance. We also\nexcluded as annotation error a syllable type that contained syllables of more than six distinct types.\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 17 of 25\nTools and resources Neuroscience\nEstimation of signal-to-noise ratio from audio recordings\nTo assess the robustness of annotation performance to noise, we assessed the recall of DAS for\nepochs with different signal-to-noise ratios (SNRs) for the fly and the mouse networks. Because of\nfundamental differences in the nature of the signals, SNR values were computed with different methods and are therefore not directly comparable across species.\nPulse\nPulse waveforms were 20 ms long and centered on the peak of the pulse energy. The root-mean\nsquare (RMS) amplitudes of the waveform margins (first and last 5 ms) and center (7.5–12.5 ms) were\ntaken as noise and signal, respectively. RMS is defined as\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi P\ni\nxðiÞ\n2\nq\n. For multi-channel recordings,\nthe pulse waveform from the channel with the highest center RMS was chosen to calculate the SNR.\nSine\nSignal was given by the RMS amplitudes of the recording during sine song. Noise is the RMS amplitude in the 200 ms before and after each sine song, with a 10 ms buffer. For instance, if a sine song\nended at 1000 ms, the recording between 1010 and 1210 ms was taken as noise. From the 200 ms\nof noise, we excluded samples that were labeled as sine or pulse and included intervals between\npulses. For multi-channel recordings, the SNR was calculated for the channel with the largest signal\namplitude.\nMouse\nWe assumed an additive noise model: s\n2\ntotal ¼ s\n2\nsignal þ s\n2\nnoise is the squared signal averaged over a\nwindow of 1 ms. Since noise variance changed little relative to the signal variance in our recordings,\nwe can assume constant noise over time to calculate the signal strength: s\n2\nsignal ¼ s\n2\ntotal\nP\nt s\n2\nnoise.\nThe sample-wise SNR is then given by SNRðtÞ ¼ ssignalðtÞ\n2\n=\nP\nt s\n2\nnoise.\nSpeed benchmarks\nInference speed was assessed using throughput and latency. Throughput is the number of samples\nannotated per second and latency is the time it takes to annotate a single chunk. Throughput and\nlatency depend on the chunk duration—the duration of a recording snippet processed by the network at once—and on the batch size—the number of chunks processed during one call. Larger\nbatches maximize throughput by more effectively exploiting parallel computation in modern CPUs\nand GPUs and reducing overheads from data transfer to the GPU. This comes at the cost of higher\nlatency, since results are available only after all chunks in a batch have been processed. Using small\nbatch sizes and short chunks therefore reduces latency, since results are available earlier, but this\ncomes at the cost of reduced throughput because of overhead from data transfer or under-utilized\nparallel compute resources. To assess throughput and latency, run times of model.predict were\nassessed for batch sizes ranging from 1 to 1024 (log spaced) with 10 repetitions for each batch size\nafter an initial warmup run (Figure 4—figure supplement 1A–L). Results shown in the main text are\nfrom a batch size corresponding to 1 s of recording for throughput (Figure 4A) and a batch size of 1\nfor latency (Figure 4B, see also Figure 4—figure supplement 1). For fly song, latency was optimized\nby reducing the chunk size to 25.6 ms (Figure 4—figure supplement 2). Benchmarks were run on\nWindows 10, Tensorflow 2.1, with the network either running on a CPU (Intel i7-7770, 3.6 GHz) or on\na GPU (GTX1050 Ti 4 GB RAM).\nWe also benchmarked the throughput of existing methods for comparison with DAS (Table 2).\nSince neither of the methods considered are designed to be used in ways in which latency can be\nfairly compared to that of DAS, we did not assess latency. The throughput values include all preprocessing steps (like calculation of a spectrogram) and comparisons to DAS were done using the\nsame hardware (CPU for FSS and USVSEG, GPU for TweetyNet and Oikarinen et al., 2019). The\nthroughput of FSS (Arthur et al., 2013; Coen et al., 2014) was tested using 400 s of single-channel\nand 9-channel recordings in Matlab2019a. USVSEG Tachibana et al., 2020 was tested on a 72 s\nrecording in Matlab2019a. TweetyNet (Cohen et al., 2020) was tested using a set of 4 recordings\n(total duration 35 s). Throughput for TweetyNet was given by the combined runtimes of the preprocessing steps (calculating of spectrograms from raw audio and saving them as temporary files)\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 18 of 25\nTools and resources Neuroscience\nand the inference steps (running the network on a GPU). For the previously published network for\nannotating marmoset calls (Oikarinen et al., 2019), we relied on published values for estimating\nthroughput: A processing time of 8 min for a 60 min recording corresponds to a throughput of 7.5\ns/s.\nData economy\nFor estimating the number of manual annotations required to obtain accurate annotations, we\ntrained the networks using different fractions of the full training and validation sets (for instance,\n0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0). Performance of all networks trained on the different subsets\nwas evaluated on the full test sets. The number of manual annotations in each subset was determined after training from the training and validation sets. The number of annotations required to\nexceed 90% of the F1 score of a model trained on the full data sets was calculated based on a lowess fit (Cleveland, 1979) to the data points (Figure 4A,B).\nUnsupervised classification\nSegmented signals were clustered using unsupervised methods described previously in\nClemens et al., 2018, Sainburg et al., 2020, and Sangiamo et al., 2020. First, signals were preprocessed: For fly song, pulse and sine waveforms of duration 15 ms were extracted from the\nrecording, aligned to their peak energy, normalized to unit norm, and adjusted for sign (see\nClemens et al., 2018 for details). For mouse, marmoset, and bird vocalizations, we adapted the procedures described in Sainburg et al., 2020: Noise was reduced in the bird song recordings using\nthe noisereduce package (https://github.com/timsainb/noisereduce). For mouse and marmoset\nvocalizations, noise reduction tended to blur the spectral contours and was omitted. Then, syllable\nspectrograms were extracted from mel spectrograms of the recordings. The noise floor of the spectrogram at each frequency was estimated as the median spectrogram over time and each spectral\nband was then divided by the frequency-specific noise floor value. Finally, the spectrogram values\nwere log-transformed and thresholded at zero for mice and two for marmosets and birds after visual\ninspection of the spectrograms to further remove background noise. To reduce differences in the\nduration of different syllables, all syllables were first log resized in time (scaling factor 8) and then\npadded with zeros to the duration of the longest syllable in the data set. Lastly, the frequency axis\nof the spectrograms for mouse syllables were aligned to the peak frequency, to make clustering\nrobust to jitter in the frequency of the thin spectral contours (Sangiamo et al., 2020). The peak frequency of each mouse syllable was calculated from its time-averaged spectrogram, and only the 40\nspectrogram frequencies around the peak frequency were retained.\nThe dimensionality of the pre-processed waveforms (fly) or spectrograms (mouse, marmoset,\nbirds) was then reduced to two using the UMAP method (McInnes and Healy, 2018) (mindistX= 0.5, 0.1 for marmosets to improve separation of clusters). Finally, signals were grouped using\nunsupervised clustering. For the fly, marmoset, and bird signals, the UMAP distribution revealed distinct groups of syllables and we used a density-based method to cluster the syllables\n(Campello et al., 2013, min_samplesX= 10, min_cluster_sizeX= 20). For mouse USVs, no clusters were visible in the UMAP distribution and density-based clustering failed to identify distinct\ngroups of syllables. Syllables were therefore split into 40 groups using k-means clustering.\nOpen source software used\n. avgn https://github.com/timsainb/avgn_paper Sainburg et al., 2020\n. hdbscan McInnes et al., 2017\n. ipython Perez and Granger, 2007\n. jupyter Kluyver et al., 2016\n. kapre Choi et al., 2017\n. keras Chollet, 2015\n. keras-tcn https://github.com/philipperemy/keras-tcn\n. librosa McFee et al., 2015\n. matplotlib Hunter, 2007\n. noisereduce https://github.com/timsainb/noisereduce\n. numpy Harris et al., 2020\n. pandas McKinney, 2010\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 19 of 25\nTools and resources Neuroscience\n. peakutils Negri and Vestri, 2017\n. scikit-learn Pedregosa et al., 2011\n. scipy Virtanen et al., 2020\n. seaborn Waskom et al., 2017\n. snakemake Ko¨ster and Rahmann, 2018\n. tensorflow Abadi et al., 2016\n. UMAP McInnes and Healy, 2018\n. zarr Miles et al., 2020\n. xarray Hoyer and Hamman, 2017\nAcknowledgements\nWe thank Kurt Hammerschmidt for providing mouse data prior to publication. We thank Mala Murthy, David Stern, and all members of the Clemens lab for feedback on the manuscript.\nThis work was supported by the DFG through grants 329518246 (Emmy Noether) and 430158535\n(SPP2205) and by the European Research Council (ERC) under the European Union’s Horizon 2020\nresearch and innovation programme (Starting Grant agreement No. 851210 NeuSoSen).\nAdditional information\nFunding\nFunder Grant reference number Author\nDeutsche Forschungsgemeinschaft\n329518246 Jan Clemens\nDeutsche Forschungsgemeinschaft\n430158535 Jan Clemens\nEuropean Research Council 851210 Jan Clemens\nThe funders had no role in study design, data collection and interpretation, or the\ndecision to submit the work for publication.\nAuthor contributions\nElsa Steinfath, Data curation, Validation, Writing - review and editing; Adrian Palacios-Mun˜ oz, Julian\nR Rottscha¨fer, Deniz Yuezak, Data curation, Writing - review and editing; Jan Clemens, Conceptualization, Software, Funding acquisition, Visualization, Methodology, Writing - original draft, Project\nadministration\nAuthor ORCIDs\nElsa Steinfath https://orcid.org/0000-0002-8455-9092\nAdrian Palacios-Mun˜ oz https://orcid.org/0000-0002-9335-7767\nJulian R Rottscha¨fer https://orcid.org/0000-0003-3741-8358\nJan Clemens https://orcid.org/0000-0003-4200-8097\nDecision letter and Author response\nDecision letter https://doi.org/10.7554/eLife.68837.sa1\nAuthor response https://doi.org/10.7554/eLife.68837.sa2\nAdditional files\nSupplementary files\n. Transparent reporting form\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 20 of 25\nTools and resources Neuroscience\nData availability\nAny code and data used during this study is deposited at https://data.goettingen-research-online.\nde/dataverse/das and https://github.com/janclemenslab/das (copy archived at https://archive.softwareheritage.org/swh:1:rev:0cab8a136523bcfd18e419a2e5f516fce9aa4abf). All fitted models are\ndeposited at https://github.com/janclemenslab/das-menagerie (copy archived at https://archive.softwareheritage.org/swh:1:rev:c41f87f8fd77ca122ca6f2dcc4676717526aaf24).\nThe following dataset was generated:\nAuthor(s) Year Dataset title Dataset URL\nDatabase and\nIdentifier\nSteinfath E,\nPalacios-Mun˜ oz A,\nRottscha¨fer JR,\nYuezak D, Clemens\nJ\n2021 Data and models for Steinfath et al.\n2021\nhttps://data.goettingenresearch-online.de/dataverse/das\nGoettingen, das\nThe following previously published datasets were used:\nAuthor(s) Year Dataset title Dataset URL\nDatabase and\nIdentifier\nNicholson D,\nQueen JE, Sober JS\n2017 Bengalese finch song repository http://dx.doi.org/10.\n6084/m9.figshare.\n4805749.v5\nfigshare, 10.6084/m9.\nfigshare.4805749.v5\nIvanenko A,\nWatkins P, Gerven\nMAJ,\nHammerschmidt K,\nEnglitz B\n2020 Data from: Classifying sex and\nstrain from mouse ultrasonic\nvocalizations using deep learning\nhttps://data.donders.ru.\nnl/collections/di/dcn/\nDSC_620840_0003_891?0\nDonders Repository,\ndi.dcn.DSC_620\n840_0003_891\nLandman R 2017 Data from: Close range vocal\ninteraction through trill calls in the\ncommon marmoset (Callithrix\njacchus)\nhttps://osf.io/q4bm3/ Open Science\nFramework, 10.17605/\nOSF.IO/PSWQD\nStern D 2014 Data from: Reported Drosophila\ncourtship song rhythms are artifacts\nof data analysis.\nhttp://research.janelia.\norg/sternlab/rawData.tar\nJanelia, sternlab/\nrawData.tar\nGoffinet J, Brudner\nS, Mooney R,\nPearson J\n2021 Data from: Low-dimensional\nlearned feature spaces quantify\nindividual and group differences in\nvocal repertoires\nhttps://doi.org/10.7924/\nr4gq6zn8w\nDuke Digital\nRepository, 10.7924/\nr4gq6zn8w\nClemens J, Coen P,\nRoemschied FA,\nPereira TD,\nMazumder D,\nAldarondo DE,\nPacheco DA,\nMurthy M\n2018 Data from: Discovery of a New\nSong Mode in Drosophila Reveals\nHidden Structure in the Sensory\nand Neural Drivers of Behavior.\nhttps://doi.org/10.25625/\n8KAKHJ\nGoettingen Research\nOnline, 10.25625/\n8KAKHJ\nReferences\nAbadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, Devin M, Ghemawat S, Irving G, Isard M, Kudlur M,\nLevenberg J, Monga R, Moore S, Murray DG, Steiner B, Tucker P, Vasudevan V, Warden P, Wicke M, et al.\n2016. Tensorflow: A System for Large-Scale Machine Learning OSDI’16 . https://www.usenix.org/system/files/\nconference/osdi16/osdi16-abadi.pdf\nArthur BJ, Sunayama-Morita T, Coen P, Murthy M, Stern DL. 2013. Multi-channel acoustic recording and\nautomated analysis of Drosophila courtship songs. BMC biology 11:11. DOI: https://doi.org/10.1186/1741-\n7007-11-11, PMID: 23369160\nArthur BJ, Ding Y, Sosale M, Khalif F, Kim E, Waddell P, Turaga SC, Stern DL. 2021. Songexplorer: a deep\nlearning workflow for discovery and segmentation of animal acoustic communication signals. bioRxiv.\nDOI: https://doi.org/10.1101/2021.03.26.437280\nBai S, Kolter JZ, Koltun V. 2018. An empirical evaluation of generic convolutional and recurrent networks for\nsequence modeling. arXiv . https://arxiv.org/abs/1803.01271.\nBaker CA, Clemens J, Murthy M. 2019. Acoustic Pattern Recognition and Courtship Songs: Insights from Insects.\nAnnual review of neuroscience 42:129–147. DOI: https://doi.org/10.1146/annurev-neuro-080317-061839,\nPMID: 30786225\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 21 of 25\nTools and resources Neuroscience\nBath DE, Stowers JR, Ho¨ rmann D, Poehlmann A, Dickson BJ, Straw AD. 2014. FlyMAD: rapid thermogenetic\ncontrol of neuronal activity in freely walking Drosophila. Nature Methods 11:756–762. DOI: https://doi.org/10.\n1038/nmeth.2973, PMID: 24859752\nBehr O, von Helversen O. 2004. Bat serenades—complex courtship songs of the sac-winged bat (Saccopteryx\nbilineata). Behavioral Ecology and Sociobiology 56:106–115. DOI: https://doi.org/10.1007/s00265-004-0768-7\nBenichov JI, Vallentin D. 2020. Inhibition within a premotor circuit controls the timing of vocal turn-taking in\nzebra finches. Nature Communications 11:1–10. DOI: https://doi.org/10.1038/s41467-019-13938-0, PMID: 31\n924758\nBennet-Clark HC. 1998. Size and scale effects as constraints in insect sound communication. Philosophical\nTransactions of the Royal Society of London. Series B: Biological Sciences 353:407–419. DOI: https://doi.org/\n10.1098/rstb.1998.0219\nCalhoun AJ, Pillow JW, Murthy M. 2019. Unsupervised identification of the internal states that shape natural\nbehavior. Nature neuroscience 22:1–10. DOI: https://doi.org/10.1038/s41593-019-0533-x, PMID: 31768056\nCampello R, Moulavi D, Sander J. 2013. Density-Based Clustering Based on Hierarchical Density Estimates. In:\nMoulavi D (Ed). Advances in Knowledge Discovery and Data Mining. Heidelberg, Germany: Springer. p. 160–\n172. DOI: https://doi.org/10.1007/978-3-030-75768-7\nCa¨ sar C, Zuberbu¨ hler K, Young RJ, Byrne RW. 2013. Titi monkey call sequences vary with predator location and\ntype. Biology letters 9:20130535. DOI: https://doi.org/10.1098/rsbl.2013.0535, PMID: 24004492\nCator LJ, Arthur BJ, Harrington LC, Hoy RR. 2009. Harmonic convergence in the love songs of the dengue vector\nmosquito. Science 323:1166541. DOI: https://doi.org/10.1126/science.1166541, PMID: 19131593\nChaverri G, Gillam EH, Kunz TH. 2013. A call-and-response system facilitates group cohesion among disc-winged\nbats. Behavioral Ecology 24:481–487. DOI: https://doi.org/10.1093/beheco/ars188\nChen X, He K. 2020. Exploring simple siamese representation learning. arXiv. https://arxiv.org/abs/2011.10566.\nChoi K, Joo D, Kim J. 2017. Kapre: on-gpu audio preprocessing layers for a quick implementation of deep neural\nnetwork models with keras. arXiv . https://arxiv.org/abs/1706.05781.\nChollet F. 2015. Keras. https://keras.io.\nClay Z, Smith CL, Blumstein DT. 2012. Food-associated vocalizations in mammals and birds: what do these calls\nreally mean? Animal Behaviour 83:323–330. DOI: https://doi.org/10.1016/j.anbehav.2011.12.008\nClemens J, Coen P, Roemschied FA, Pereira TD, Mazumder D, Aldarondo DE, Pacheco DA, Murthy M. 2018.\nDiscovery of a New Song Mode in Drosophila Reveals Hidden Structure in the Sensory and Neural Drivers of\nBehavior. Current biology : CB 28:2400–2412. DOI: https://doi.org/10.1016/j.cub.2018.06.011, PMID: 3005730\n9\nClemens J, Hennig RM. 2013. Computational principles underlying the recognition of acoustic signals in insects.\nJournal of computational neuroscience 35:75–85. DOI: https://doi.org/10.1007/s10827-013-0441-0,\nPMID: 23417450\nCleveland WS. 1979. Robust locally weighted regression and smoothing scatterplots. Journal of the American\nStatistical Association 74:829–836. DOI: https://doi.org/10.1080/01621459.1979.10481038\nCoen P, Clemens J, Weinstein AJ, Pacheco DA, Deng Y, Murthy M. 2014. Dynamic sensory cues shape song\nstructure in Drosophila. Nature 507:233–237. DOI: https://doi.org/10.1038/nature13131, PMID: 24598544\nCoen P, Xie M, Clemens J, Murthy M. 2016. Sensorimotor Transformations Underlying Variability in Song\nIntensity during Drosophila Courtship. Neuron 89:629–644. DOI: https://doi.org/10.1016/j.neuron.2015.12.035,\nPMID: 26844835\nCoffey KR, Marx RG, Neumaier JF. 2019. DeepSqueak: a deep learning-based system for detection and analysis\nof ultrasonic vocalizations. Neuropsychopharmacology : official publication of the American College of\nNeuropsychopharmacology 44:1–10. DOI: https://doi.org/10.1038/s41386-018-0303-6, PMID: 30610191\nCohen Y, Nicholson D, Sanchioni A, Mallaber EK, Skidanova V, Gardner TJ. 2020. TweetyNet: a neural network\nthat enables high-throughput, automated annotation of birdsong. bioRxiv. DOI: https://doi.org/10.1101/2020.\n08.28.272088\nDeutsch D, Clemens J, Thiberge SY, Guan G, Murthy M. 2019. Shared Song Detector Neurons in Drosophila\nMale and Female Brains Drive Sex-Specific Behaviors. Current biology : CB 29:3200–3215. DOI: https://doi.\norg/10.1016/j.cub.2019.08.008, PMID: 31564492\nDevlin J, Chang M-W, Lee K, Toutanova K. 2019. Bert: pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv. https://arxiv.org/abs/1810.04805.\nDing Y, Berrocal A, Morita T, Longden KD, Stern DL. 2016. Natural courtship song variation caused by an intronic\nretroelement in an ion channel gene. Nature 536:329–332. DOI: https://doi.org/10.1038/nature19093,\nPMID: 27509856\nDing Y, Lillvis JL, Cande J, Berman GJ, Arthur BJ, Long X, Xu M, Dickson BJ, Stern DL. 2019. Neural evolution of\nContext-Dependent fly song. Current Biology 29:1089–1099. DOI: https://doi.org/10.1016/j.cub.2019.02.019,\nPMID: 30880014\nFitch WT, Neubauer J, Herzel H. 2002. Calls out of chaos: the adaptive significance of nonlinear phenomena in\nmammalian vocal production. Animal Behaviour 63:407–418. DOI: https://doi.org/10.1006/anbe.2001.1912\nFortune ES, Rodrı´guez C, Li D, Ball GF, Coleman MJ. 2011. Neural mechanisms for the coordination of duet\nsinging in wrens. Science 334:666–670. DOI: https://doi.org/10.1126/science.1209867, PMID: 22053048\nGerhardt CH, Huber F. 2002. Acoustic Communication in Insects and Anurans. Illinois, United States: University\nof Chicago Press. DOI: https://doi.org/10.1093/icb/42.5.1080\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 22 of 25\nTools and resources Neuroscience\nGoffinet J, Brudner S, Mooney R, Pearson J. 2021. Low-dimensional learned feature spaces quantify individual\nand group differences in vocal repertoires. eLife 10:e67855. DOI: https://doi.org/10.7554/eLife.67855,\nPMID: 33988503\nGraves A, Jaitly N. 2014. Towards End-To-End speech recognition with recurrent neural networks. International\nConference on Machine Learning 1764–1772. http://proceedings.mlr.press/v32/graves14.pdf.\nGraving JM, Chae D, Naik H, Li L, Koger B, Costelloe BR, Couzin ID. 2019. DeepPoseKit, a software toolkit for\nfast and robust animal pose estimation using deep learning. eLife 8:18. DOI: https://doi.org/10.7554/eLife.\n47994, PMID: 31570119\nGuirguis K, Schorn C, Guntoro A, Abdulatif S, Yang B. 2021. Seld-Tcn: sound event localization & detection via\ntemporal convolutional networks. 2020 28th European Signal Processing Conference (EUSIPCO) 16–20.\nDOI: https://doi.org/10.23919/Eusipco47968.2020.9287716\nHaack B, Markl H, Ehret G. 1983. Sound communication between parents and offspring. In: Willott J. F (Ed). The\nAuditory Psychobiology of the Mouse. Springfield, Illinois: CC Thomas. p. 57–97. DOI: https://doi.org/10.\n18725/OPARU-1174\nHarris CR, Millman KJ, van der Walt SJ, Gommers R, Virtanen P, Cournapeau D, Wieser E, Taylor J, Berg S, Smith\nNJ, Kern R, Picus M, Hoyer S, van Kerkwijk MH, Brett M, Haldane A, del Rı´o JF, Wiebe M, Peterson P, Ge´ rardMarchant P, et al. 2020. Array programming with NumPy. Nature 585:357–362. DOI: https://doi.org/10.1038/\ns41586-020-2649-2\nHe K, Zhang X, Ren S, Sun J. 2016. Deep residual learning for image recognition. 2016 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR). DOI: https://doi.org/10.1109/CVPR.2016.90\nHoly TE, Guo Z. 2005. Ultrasonic songs of male mice. PLOS Biology 3:e386. DOI: https://doi.org/10.1371/\njournal.pbio.0030386, PMID: 16248680\nHoyer S, Hamman JJ. 2017. Xarray: n-d labeled arrays and datasets in python. Journal of Open Research\nSoftware 5:148. DOI: https://doi.org/10.5334/jors.148\nHunter JD. 2007. Matplotlib: a 2D graphics environment. Computing in Science & Engineering 9:90–95.\nDOI: https://doi.org/10.1109/MCSE.2007.55\nIvanenko A, Watkins P, van Gerven MAJ, Hammerschmidt K, Englitz B. 2020. Classifying sex and strain from\nmouse ultrasonic vocalizations using deep learning. PLOS Computational Biology 16:e1007918. DOI: https://\ndoi.org/10.1371/journal.pcbi.1007918, PMID: 32569292\nJanik VM, Slater PJB. 1998. Context-specific use suggests that bottlenose dolphin signature whistles are\ncohesion calls. Animal behaviour 56:829–838. DOI: https://doi.org/10.1006/anbe.1998.0881, PMID: 9790693\nKingma DP, Ba J. 2015. Adam: a method for stochastic optimization. Conference Paper at ICLR 2015. https://\narxiv.org/pdf/1412.6980.pdf.\nKluyver T, Ragan-Kelley B, Pe´ rez F, Granger B, Bussonnier M, Frederic J, Kelley K, Hamrick J, Grout J, Corlay S,\nIvanov P, Avila D, Abdalla S, Willing C, development team J. 2016. Jupyter notebooks - a publishing format for\nreproducible computational workflows. In: Loizides F, Scmidt B (Eds). Positioning and Power in Academic\nPublishing: Players, Agents and Agendas. Netherlands: IOS Press. p. 87–90.\nKollmorgen S, Hahnloser RHR, Mante V. 2020. Nearest neighbours reveal fast and slow components of motor\nlearning. Nature 577:526–530. DOI: https://doi.org/10.1038/s41586-019-1892-x\nKo¨ ster J, Rahmann S. 2018. Snakemake-a scalable bioinformatics workflow engine. Bioinformatics 34:3600.\nDOI: https://doi.org/10.1093/bioinformatics/bty350, PMID: 29788404\nKoumura T, Okanoya K. 2016. Automatic Recognition of Element Classes and Boundaries in the Birdsong with\nVariable Sequences. PLOS ONE 11:e0159188. DOI: https://doi.org/10.1371/journal.pone.0159188,\nPMID: 27442240\nKrizhevsky A, Sutskever I, Hinton GE. 2012. ImageNet classification with deep convolutional neural networks.\nAdvances in Neural Information Processing Systems 25 (NIPS 2012). https://papers.nips.cc/paper/2012/hash/\nc399862d3b9d6b76c8436e924a68c45b-Abstract.html.\nLandman R, Sharma J, Hyman JB, Fanucci-Kiss A, Meisner O, Parmar S, Feng G, Desimone R. 2020. Close-range\nvocal interaction in the common marmoset (Callithrix jacchus). PLOS ONE 15:e0227392. DOI: https://doi.org/\n10.1371/journal.pone.0227392, PMID: 32298305\nLipkind D, Marcus GF, Bemis DK, Sasahara K, Jacoby N, Takahasi M, Suzuki K, Feher O, Ravbar P, Okanoya K,\nTchernichovski O. 2013. Stepwise acquisition of vocal combinatorial capacity in songbirds and human infants.\nNature 498:104–108. DOI: https://doi.org/10.1038/nature12173, PMID: 23719373\nLong MA, Fee MS. 2008. Using temperature to analyse temporal dynamics in the songbird motor pathway.\nNature 456:189–194. DOI: https://doi.org/10.1038/nature07448, PMID: 19005546\nMamalet F, Garcia C. 2012. Simplifying ConvNets for Fast Learning. In: Garcia C (Ed). Artificial Neural Networks\nand Machine Learning – ICANN 2012. Heidelberg: Springer. p. 58–65. DOI: https://doi.org/10.1007/978-3-642-\n33266-1_8\nMathis A, Mamidanna P, Cury KM, Abe T, Murthy VN, Mathis MW, Bethge M. 2018. DeepLabCut: markerless\npose estimation of user-defined body parts with deep learning. Nature Neuroscience 21:1281–1289.\nDOI: https://doi.org/10.1038/s41593-018-0209-y, PMID: 30127430\nMathis A, Biasi T, Schneider S, Yu¨ ksekgo¨ nu¨ l M, Rogers B, Bethge M, Mathis M. 2021. Pretraining boosts Out-ofDomain robustness for pose estimation. 2021 IEEE Winter Conference on Applications of Computer Vision\n(WACV). DOI: https://doi.org/10.1109/WACV48630.2021.00190\nMcFee B, Raffel C, Liang D, Ellis DP, McVicar M, Battenberg E, Nieto O. 2015. Librosa: audio and music signal\nanalysis in python. Proceedings of the 14th Python in Science Conference. https://conference.scipy.org/\nproceedings/scipy2015/pdfs/brian_mcfee.pdf.\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 23 of 25\nTools and resources Neuroscience\nMcInnes L, Healy J, Astels S. 2017. Hdbscan: hierarchical density based clustering. The Journal of Open Source\nSoftware 2:205. DOI: https://doi.org/10.21105/joss.00205\nMcInnes L, Healy J. 2018. Umap: uniform manifold approximation and projection for dimension reduction. arXiv.\nhttps://arxiv.org/abs/1802.03426.\nMcKinney W. 2010. Data structures for statistical computing in python. Proc. of the 9th Python in Science Conf.\n(SCIPY 2010). https://conference.scipy.org/proceedings/scipy2010/pdfs/mckinney.pdf.\nMiles A, Kirkham J, Durant M, Bourbeau J, Onalan T, Hamman J, Patel Z, shikharsg R, Schut V, de Andrade ES,\nAbernathey R, Noyes C, Tran T, Saalfeld S, Swaney J, Moore J, Jevnik J, Kelleher J, Funke J, Sakkis G, et al.\n2020. Zarr-Developers/zarr-Python, Zenodo, v2.4.0 https://github.com/zarr-developers/zarr-python..\nMorley EL, Jonsson T, Robert D. 2018. Auditory sensitivity, spatial dynamics, and amplitude of courtship song in\nDrosophila melanogaster. The Journal of the Acoustical Society of America 144:734–739. DOI: https://doi.org/\n10.1121/1.5049791, PMID: 30180716\nNegri LH, Vestri C. 2017. Lucashn/peakutils, Zenodo, v1.1.0 https://github.com/lucashn/peakutils..\nNeunuebel JP, Taylor AL, Arthur BJ, Egnor SE. 2015. Female mice ultrasonically interact with males during\ncourtship displays. eLife 4:e06203. DOI: https://doi.org/10.7554/eLife.06203, PMID: 26020291\nNicholson D, Queen JE, Sober, S J. 2017. Bengalese finch song repository. figshare. DOI: https://doi.org/10.6084/\nm9.figshare.4805749.v5\nOikarinen T, Srinivasan K, Meisner O, Hyman JB, Parmar S, Fanucci-Kiss A, Desimone R, Landman R, Feng G.\n2019. Deep convolutional network for animal sound classification and source attribution using dual audio\nrecordings. The Journal of the Acoustical Society of America 145:654–662. DOI: https://doi.org/10.1121/1.\n5087827, PMID: 30823820\nOkobi DE, Banerjee A, Matheson AMM, Phelps SM, Long MA. 2019. Motor cortical control of vocal interaction in\nneotropical singing mice. Science 363:983–988. DOI: https://doi.org/10.1126/science.aau9480, PMID: 3081\n9963\nPedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R,\nDubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E´\n. 2011. Scikit-learn:\nmachine learning in python. Journal of Machine Learning Research 12:2825–2830.\nPereira TD, Aldarondo DE, Willmore L, Kislin M, Wang SS, Murthy M, Shaevitz JW. 2019. Fast animal pose\nestimation using deep neural networks. Nature methods 16:1–125. DOI: https://doi.org/10.1038/s41592-018-\n0234-5, PMID: 30573820\nPerez F, Granger BE. 2007. IPython: a system for interactive scientific computing. Computing in Science &\nEngineering 9:21–29. DOI: https://doi.org/10.1109/MCSE.2007.53\nRaghu M, Zhang C, Kleinberg J, Bengio S. 2019. Transfusion: understanding transfer learning for medical\nimaging. NeurIPS. https://paperswithcode.com/paper/transfusion-understanding-transfer-learning.\nSainburg T, Thielk M, Gentner TQ. 2020. Finding, visualizing, and quantifying latent structure across diverse\nanimal vocal repertoires. PLOS Computational Biology 16:e1008228. DOI: https://doi.org/10.1371/journal.pcbi.\n1008228, PMID: 33057332\nSangiamo DT, Warren MR, Neunuebel JP. 2020. Ultrasonic signals associated with different types of social\nbehavior of mice. Nature neuroscience 23:1–12. DOI: https://doi.org/10.1038/s41593-020-0584-z, PMID: 32066\n980\nSrivastava KH, Holmes CM, Vellema M, Pack AR, Elemans CP, Nemenman I, Sober SJ. 2017. Motor control by\nprecisely timed spike patterns. PNAS 114:1171–1176. DOI: https://doi.org/10.1073/pnas.1611734114, PMID: 2\n8100491\nStern DL. 2014. Reported Drosophila courtship song rhythms are artifacts of data analysis. BMC Biology 12:38.\nDOI: https://doi.org/10.1186/1741-7007-12-38, PMID: 24965095\nStern DL, Clemens J, Coen P, Calhoun AJ, Hogenesch JB, Arthur BJ, Murthy M. 2017. Experimental and\nstatistical reevaluation provides no evidence for Drosophila courtship song rhythms. PNAS 114:9978–9983.\nDOI: https://doi.org/10.1073/pnas.1707471114, PMID: 28851830\nStowers JR, Hofbauer M, Bastien R, Griessner J, Higgins P, Farooqui S, Fischer RM, Nowikovsky K, Haubensak\nW, Couzin ID, Tessmar-Raible K, Straw AD. 2017. Virtual reality for freely moving animals. Nature methods 14:\n995–1002. DOI: https://doi.org/10.1038/nmeth.4399, PMID: 28825703\nTabler JM, Rigney MM, Berman GJ, Gopalakrishnan S, Heude E, Al-Lami HA, Yannakoudakis BZ, Fitch RD, Carter\nC, Vokes S, Liu KJ, Tajbakhsh S, Egnor SR, Wallingford JB. 2017. Cilia-mediated hedgehog signaling controls\nform and function in the mammalian larynx. eLife 6:e19153. DOI: https://doi.org/10.7554/eLife.19153, PMID: 2\n8177282\nTachibana RO, Kanno K, Okabe S, Kobayasi KI, Okanoya K. 2020. USVSEG: A robust method for segmentation of\nultrasonic vocalizations in rodents. PLOS ONE 15:e0228907. DOI: https://doi.org/10.1371/journal.pone.\n0228907, PMID: 32040540\nTschida K, Mooney R. 2012. The role of auditory feedback in vocal learning and maintenance. Current opinion in\nneurobiology 22:320–327. DOI: https://doi.org/10.1016/j.conb.2011.11.006, PMID: 22137567\nvan den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, Kalchbrenner N, Senior A, Kavukcuoglu\nK. 2016. Wavenet: a generative model for raw audio. arXiv. https://arxiv.org/abs/1609.03499.\nVan Segbroeck M, Knoll AT, Levitt P, Narayanan S. 2017. MUPET-Mouse Ultrasonic Profile ExTraction: A Signal\nProcessing Tool for Rapid and Unsupervised Analysis of Ultrasonic Vocalizations. Neuron 94:465–485.\nDOI: https://doi.org/10.1016/j.neuron.2017.04.005, PMID: 28472651\nVirtanen P, Gommers R, Oliphant TE, Haberland M, Reddy T, Cournapeau D, Burovski E, Peterson P, Weckesser\nW, Bright J, van der Walt SJ, Brett M, Wilson J, Millman KJ, Mayorov N, Nelson ARJ, Jones E, Kern R, Larson E,\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 24 of 25\nTools and resources Neuroscience\nCarey CJ, et al. 2020. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature methods\n17:261–272. DOI: https://doi.org/10.1038/s41592-019-0686-2, PMID: 32015543\nWarren MR, Clein RS, Spurrier MS, Roth ED, Neunuebel JP. 2020. Ultrashort-range, high-frequency\ncommunication by female mice shapes social interactions. Scientific Reports 10:1–14. DOI: https://doi.org/10.\n1038/s41598-020-59418-0\nWaskom M, Botvinnik O, O’Kane D, Hobson P, Lukauskas S, Gemperline DC, Augspurger T, Halchenko Y, Cole\nJB, Warmenhoven J, de Ruiter J, Pye C, Hoyer S, Vanderplas J, Villalba S, Kunter G, Quintero E, Bachant P,\nMartin M, Meyer K, et al. 2017. Mwaskom/seaborn, Zenodo, v0.8.1. https://github.com/mwaskom/seaborn\nWeiss M, Hultsch H, Adam I, Scharff C, Kipper S. 2014. The use of network analysis to study complex animal\ncommunication systems: a study on nightingale song. Proceedings of the Royal Society B: Biological Sciences\n281:20140460. DOI: https://doi.org/10.1098/rspb.2014.0460\nYu F, Koltun V. 2016. Multi-scale context aggregation by dilated convolutions. arXiv. https://arxiv.org/abs/1511.\n07122.\nSteinfath et al. eLife 2021;10:e68837. DOI: https://doi.org/10.7554/eLife.68837 25 of 25\nTools and resources Neuroscience", "affiliations": [{"university": "European Neuroscience Institute", "country": "Germany", "discipline": "Neuroscience"}, {"university": "University of Göttingen", "country": "Germany", "discipline": "Neuroscience"}], "species_categories": ["Insect", "Terrestrial Mammal", "Other", "Bird"], "specialized_species": ["fruit fly", "mouse", "marmoset", "Bengalese finch", "Zebra finch"], "computational_stages": ["Data Collection", "Pre-processing", "Sequence Representation", "Meaning Identification", "Generation"], "linguistic_features": ["Vocal Auditory Channel and Turn-taking", "Broadcast and Direct Reception", "Reference and Displacement", "Semanticity", "Tradition and Cultural Transmission"], "status": "saved", "created_at": "2026-01-13T12:49:59.887465", "updated_at": "2026-01-13T14:10:48.973646", "committed_at": "2026-01-13T14:11:04.186346"}
{"id": "12ebc1f3-6a84-4d9c-87d5-28051444e4c3", "title": "ANIMAL-SPOT enables animal-independent signal detection and classification using deep learning", "authors": ["Bergler, Christian", "Smeele, Simeon Q", "Tyndel, Stephen A", "Barnhill, Alexander", "Ortiz, Sara T", "Kalan, Ammie K", "Cheng, Rachael Xi", "Brinkl{\\o}v, Signe", "Osiecka, Anna N", "Tougaard, Jakob", "others"], "year": "2022", "journal": "Scientific Reports", "abstract": "", "doi": "", "analysis_notes": "1\nVol.:(0123456789)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports\nANIMAL‑SPOT enables\nanimal‑independent signal\ndetection and classification using\ndeep learning\nChristian Bergler1*, Simeon Q. Smeele2,3,4, StephenA.Tyndel2,5, Alexander Barnhill1\n,\nSaraT. Ortiz6\n, Ammie K. Kalan7\n, Rachael Xi Cheng8\n, Signe Brinkløv9\n, Anna N. Osiecka10,\nJakobTougaard11, Freja Jakobsen12, MagnusWahlberg12, Elmar Nöth1\n, Andreas Maier1 &\nBarbara C. Klump2*\nBioacoustic research spans a wide range of biological questions and applications, relying on\nidentification of target species or smaller acoustic units, such as distinct call types. However, manually\nidentifying the signal of interest is time-intensive, error-prone, and becomes unfeasible with large\ndata volumes. Therefore, machine-driven algorithms are increasingly applied to various bioacoustic\nsignal identification challenges. Nevertheless, biologists still have major difficulties trying to transfer\nexisting animal- and/or scenario-related machine learning approaches to their specific animal datasets\nand scientific questions. This study presents an animal-independent, open-source deep learning\nframework, along with a detailed user guide. Three signal identification tasks, commonly encountered\nin bioacoustics research, were investigated: (1) target signal vs. background noise detection, (2)\nspecies classification, and (3) call type categorization. ANIMAL-SPOT successfully segmented humanannotated target signals in data volumes representing 10 distinct animal species and 1 additional\ngenus, resulting in a mean test accuracy of 97.9%, together with an average area under the ROC\ncurve (AUC) of 95.9%, when predicting on unseen recordings. Moreover, an average segmentation\naccuracy and F1-score of 95.4% was achieved on the publicly available BirdVox-Full-Night data corpus.\nIn addition, multi-class species and call type classification resulted in 96.6% and 92.7% accuracy on\nunseen test data, as well as 95.2% and 88.4% regarding previous animal-specific machine-based\ndetection excerpts. Furthermore, an Unweighted Average Recall (UAR) of 89.3% outperformed the\nmulti-species classification baseline system of the ComParE 2021 Primate Sub-Challenge. Besides\nanimal independence, ANIMAL-SPOT does not rely on expert knowledge or special computing\nresources, thereby making deep-learning-based bioacoustic signal identification accessible to a broad\naudience.\nIn order to gain deeper insights and a better understanding about animal communication, it is imperative to\nidentify vocalization prototypes, derive linguistic patterns, and correlate acoustic paradigms with corresponding\nbehavioral observations. Therefore, it is mandatory to perform in-depth data analysis of large-scale bioacoustic\nOPEN\n1\nPattern Recognition Lab, Department of Computer Science, Friedrich-Alexander-Universität Erlangen-Nürnberg,\n91058 Erlangen, Germany. 2\nCognitive and Cultural Ecology Lab, Max Planck Institute of Animal Behavior,\n78315 Radolfzell, Germany. 3\nDepartment of Human Behavior, Ecology and Culture, Max Planck Institute\nfor Evolutionary Anthropology, 04103 Leipzig, Germany. 4\nBiology Department, University of Konstanz,\n78464 Constance, Germany. 5\nDepartment of Natural Resources and Environmental Sciences, University of Illinois\nUrbana-Champaign, Champaign, IL, United States. 6\nMax Planck Institute for Biological Intelligence, in Foundation,\nSeewiesen Eberhard-Gwinner-Strasse, 82319 Starnberg, Germany. 7\nDepartment of Anthropology, University of\nVictoria, Victoria, BC V8P 5C2, Canada. 8\nLeibniz Institute for Zoo and Wildlife Research, Alfred‑Kowalke‑Straße\n17, 10315 Berlin, Germany. 9\nDepartment of Bioscience, Wildlife Ecology, Aarhus University, 8410 Rønde,\nDenmark. 10Department of Vertebrate Ecology and Zoology, Faculty of Biology, University of Gdańsk,\n80‑308 Gdańsk, Poland. 11Department of Bioscience, Marine Mammal Research, Aarhus University, 4000 Roskilde,\nDenmark. 12Department of Biology, University of Southern Denmark, 5230 Odense, Denmark. *email:\nchristian.bergler@fau.de; bklump@ab.mpg.de\n2\nVol:.(1234567890)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\ndata archives in order to draw statistically significant and representative hypotheses regarding the vocal repertoire of a particular species. Passive Acoustic Monitoring (PAM) concepts1–3\n are widely used to acquire massive\nbioacoustic data collections4–7\n, without affecting the natural animal habitats8\n and thus significantly increase the\nprobability to observe all natural communicative patterns, following the observer’s paradox principle9\n. Furthermore, PAM-based approaches strongly benefit from decreasing costs for recording equipment and data\nstorage10–13, combined with recent technological advances14–20. However, time- and human-resource restrictions prohibit a profound and comprehensive manual data analysis. Consequently, machine (deep) learning\napproaches are increasingly applied in bioacoustic research21,22 and have shown to be a productive avenue to\nidentify target animal species (e.g., marine mammals23–26, birds27,28, bats29, mosquitos30), smaller acoustic units\nsuch as call types (e.g., bird call types27,31) and group-level differences within target animal species (e.g., killer\nwhale pods32). Despite a growing deployment of various machine (deep) learning techniques in the field of bioacoustics, essential research tasks such as target species identification and call type classification still prove to\nbe extremely difficult and challenging.\nMachine (deep) learning approaches are often designed for a particular animal species and lack data-related\nmodel adaption and hyperparameter fine-tuning options. In addition, the software and/or source code is often\nnot publicly available, combined with missing or insufficient user guidelines which describe required data preparation, network training setup, and model evaluation. It thereby often precludes not just a general transfer to\nanimal- and user-specific research questions, but mainly prevents non-computer science operators to train their\nown use-case and animal-specific models, which in turn significantly hampers progress in research on animal\ncommunication. In this study, we introduce ANIMAL-SPOT, an open-source machine learning framework that\nenables biologists to independently train and evaluate animal-specific deep learning-based classification models\nin order to address fundamental biological research questions, including target/noise detection and/or species/\ncall type identification.\nThree typical scenarios present themselves when attempting to identify the vocalizations of a target species\nor individual: (1) The target signal appears without confounding factors such as other similar vocalizations\nand the task is to determine the target signal with respect to background noise, (2) the target signal appears in\nconjunction with other, dissimilar, species-specific vocalizations and the signal of interest must be distinguished\nbetween other bioacoustic signals and background noise, and (3) the target signal appears with other signals,\nsome of which share similar properties to the target vocalizations and the model must differentiate between\nsimilar signals, dissimilar signals, as well as background noise. The approach described here allows a researcher\nto address all of these tasks, with slight differences in data structure as well as usage of the trained models.\nA detailed user guide33, provided in conjunction with this work which describes the data setup as well as\nmodel configuration, allows users to create and apply models with no prior deep learning knowledge. The core\ndeep learning workflow took inspiration from ORCA-SPOT34, a ResNet-1835-based Convolutional Neural Network (CNN), originally designed for segmenting killer whale (Orcinus orca) vocalizations from environmental\nbackground noise. ANIMAL-SPOT has been adapted and extended to become an animal-independent deep\nlearning framework, evaluating bioacoustic target versus environmental noise detection for 10 species-specific\ndata volumes and 1 additional genus-based dataset, next to the publicly available BirdVox-Full-Night36 repository. In addition, multi-species classification has been performed in two different scenarios: (1) as a downstream\nprocess, using previously machine-detected and extracted genus-specific target signals, and (2) as a stand-alone\nprocedure, analyzing the Computational Paralinguistics Challenge Primate (ComParE-PRS)37,38 multi-species\ndata volume. Moreover, multi-class call type classification has been exemplarily conducted for a single species,\nusing the same downstream approach. The ANIMAL-SPOT workflow is generalizable, enabling unparalleled\nflexibility in processing task- and animal-specific bioacoustic data corpora. Figure 1 visualizes all animal speciesspecific spectrograms (10 different species, 1 additional genus), representing a single vocalization event.\nIn summary, ANIMAL-SPOT provides a publicly-available animal-independent bioacoustic machine learning\nenvironment, which allows scientists, regardless of their technical backgrounds, to train and evaluate speciesspecific deep neural networks, supported by detailed user guidelines, in order to answer fundamental biological\nresearch questions (target/sound detection, species classification, and call type recognition). To the best of the\nauthors’ knowledge, this is the first study presenting an animal-independent and publicly-available deep learning framework, evaluated across many bioacoustic signal identification scenarios. As input data, we use many\nannotated datasets from a broad range of animal species and provide evaluation results using these as well as\npublicly available species detection and classification challenge data corpora.\nMaterials and methods\nBioacoustic signal identification and classification scenarios. Signal identification can be performed at different levels: (1) taxonomic group (e.g., all birds), (2) species (e.g., monk parakeet), or (3) call type\n(e.g., contact call). The level highly influences data preparation and classification complexity. Raising taxonomic\nspecialization simultaneously leads to an increase of the labeled data granularity being required for an adequate\nnetwork training. Furthermore, classification intricacy grows with the level of taxonomic detail. The network\nhas to learn and derive features, being robust against all potential types of environmental noise as well as other\nanimal sounds, except the signals of interest, including within-species variation. The level of taxonomy also\naffects the amount of chosen network output classes—binary detection (e.g., delphinidae vs. noise) or multi-class\nclassification (killer whale vs. white-sided dolphin vs. bottlenose dolphin vs. noise)—which in turn impacts classification complexity. An adequate identification scenario is therefore determined by the biological use-case and\ntaxonomic depth, in combination with the available data material, recorded via active/passive acoustic monitoring. Regarding the animal corpora, two initial data material situations are possible: (1) dataset only contains\nbackground noise and target signals, or (2) dataset includes background noise, target signals, and other vocaliza-\n3\nVol.:(0123456789)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\ntions that often resemble the target signal. Consequently, the following classification procedures are conceivable:\n(1) binary target/noise detection—isolating environmental noise from the taxonomic-dependent animal signals\naccording to the above mentioned data scenarios, or (2) multi-class species/call type recognition—classifying\nbetween multiple target species or call types, combined with the illustrated potential data situations. To ensure\na robust, animal-independent identification procedure, a binary target/noise detection at the desired taxonomic\nlevel (e.g., animal genus) has to be conducted first, to remove noise and other irrelevant animal vocalizations\nin advance. Taxonomic depth leads to an increasing spectral closeness between signals being represented in the\nnoise class, which results in less distinctive network features separating both classes. Depending on the initial\nmodel performance for the chosen taxonomic rank, target/noise data distribution might be restructured with\nrespect to a higher, more generic taxonomic level (e.g. genus to order-level). Based on the respective detection\nresult, subsequent multi-class classification can be conducted with respect to more specific taxonomic ranks,\nsuch as animal species (e.g., Blue-winged vs. Golden-winged warbler) or different call types (e.g., monk parakeet\nalarm, contact, and other calls), ending up in a multi-stage classification procedure. ANIMAL-SPOT is also\ncapable of performing recognition with respect to different species-specific regional differences (dialects) as well\nas individual identification. Sufficient representative data for dialects of interest or individual-specific vocalizations, in the same way as for the other multi-class classification problems described here, is the only precluding\nfactor. Filtering away noise and other animal vocalizations via the two-step approach enables focus on analysis\nof regional differences (dialects) and acoustic identification of individuals. Instructions on model configuration\nand necessary data structure will be further detailed in the user guide33 with examples.\nAnimal species and recording setup. In order to show and prove animal independence, 10 different\nspecies and 1 additional genus within the chordate phylum were chosen. The overall goal was to test model\nrobustness for as many different habitat types (urban parks, marine reserve, arctic landice), frequency ranges\n(30 Hz for Atlantic cod to 100,000 Hz for Pygmy pipistrelle), vocalization durations (echolocation sweeps in ms\nto long roars of multiple seconds), signal-to-noise ratios (urban parks versus noise isolated laboratory), noise\ncharacteristics (underwater noise, human narrations, other species), as well as recording setups (passive acoustic\nmonitoring—e.g., Harbour seals—versus focal follows—e.g., Blue-/Golden-winged warblers). A detailed summary regarding all animal-specific recording and data collection setups, utilized within this study, is given in\nSupplementary Table S1.\nBioacoustic data material. No animals were approached for this study specifically but rather all data used\nwere collected by distinct research teams under their own ethics guidelines. In case of binary detection, each\nspecies/genus target and noise was manually annotated. The target class contained only vocalizations produced\nby the target species/genus (see Table 1). In cases where further sub-classification was envisioned (species level\nFigure 1.  Animal-specific data (10 different species, 1 additional genus) utilized to investigate the ANIMALSPOT framework (created via Inkscape39, Version 0.92.3).\n4\nVol:.(1234567890)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\nfor the two warblers—Table 2, call type level regarding monk parakeets—Table 3), these were labeled as well,\nbut all assigned to the target class. The noise class included all other sound segments, such as environmental/\nbackground noise, human narrations, and other animal sounds. While both the number of annotated segments\nand the class distribution differed for each species, the ratio between vocalization and noise ranges from ≈20%\nup to ≈57% for all listed data archives. To perform embedded noise augmentation, additional noise segments\nwere provided for some of the species (see Table 1). ANIMAL-SPOT was trained and evaluated in three different experiments: (1) detection between target and noise to separate noise from valuable animal signals, and (2)\nmulti-class species classification, and (3) multi-class call type identification. Besides the annotated detection data\nTable 1.  Animal-specific data corpora and distribution. *Additional noise augmentation training samples:\n[1.1] cockatiel—180 (2.87 min.), [1.2] monk parakeet—105 (1.08 min.), [1.3] Blue-/Golden-winged\nwarbler—500 (10.03 min.), [1.4] Harbour seal—2,531 (32.74 min.), [1.5] killer whale—6715 (258.27 min.),\n[1.6] Pygmy pipistrelle—543 (1.80 min.), [1.7] chimpanzee—1446 (40.34 min.). 1Samples (smp[#]), 2\nsample\nduration in minutes (smp[min.]), 3\nsample percentage (smp[%]), 4\nsummed target/noise duration of the three\nunseen test recordings (\u001fr[min.]). Significant values are in bold.\nDataset\nLabel type\nTarget label Noise label \u001f labels\nsmp[#]1 smp[min.]2 smp[%]3 \u001fr[min.]4 smp[#]1 smp[min.]2 smp[%]3 \u001fr[min.]4 smp[#]1 smp[min.]2 smp[%]3 \u001fr[min.]4\nCockatiel* 1271 12.41 40.9 2.46 1840 41.68 59.1 179.10 3111 54.09 100.0 181.56\nSulphur-crested\ncockatoo 1495 15.26 41.1 3.37 2145 34.99 58.9 46.81 3640 50.25 100.0 50.18\nPeach-fronted\nconure 1174 12.35 55.2 0.22 952 8.88 44.8 4.53 2126 21.23 100.0 4.75\nMonk parakeet* 3133 17.20 46.4 0.63 3612 75.25 53.6 12.63 6745 92.45 100.0 13.26\nBlue-/goldenwinged warbler* 1616 48.99 32.0 5.43 3431 95.10 68.0 14.57 5047 144.09 100.0 20.00\nChinstrap\npenguin 906 4.86 20.8 0.82 3454 15.40 79.2 3.26 4360 20.26 100.0 4.08\nAtlantic cod 382 3.14 30.6 0.19 867 6.00 69.4 20.82 1249 9.14 100.0 21.01\nHarbour seal* 2900 55.18 56.4 7.79 2245 58.67 43.6 22.21 5145 113.85 100.0 30.00\nKiller whale34* 17,104 649.45 27.8 20.64 44,323 2076.36 72.2 121.49 61,427 2725.81 100.0 142.13\nPygmy pipistrelle* 1570 0.18 31.0 0.10 3490 4.94 69.0 1.11 5060 5.12 100.0 1.21\nChimpanzee* 7079 231.17 57.2 2.89 5305 174.44 42.8 87.11 12,384 405.61 100.0 90.00\nBirdVox-FullNight36 35,402 295.02 50.0 − 35,402 295.02 50.0 − 70,804 588 100.0 −\nTable 2.  Blue-/golden-winged warbler data distribution. Significant values are in bold.\nLabel type\nDistribution\nSamples Min. %-samples\nBlue-winged warbler 707 21.16 22.4\nGolden-winged warbler 909 27.83 28.8\nOther bird 542 13.39 17.2\nNoise 1000 26.10 31.6\n\u001f 3158 88.48 100.0\nTable 3.  Monk parakeet call type data and distribution. Significant values are in bold.\nLabel type\nDistribution\nSamples Min. %-samples\nAlarm call 798 5.61 24.5\nContact call 689 3.61 21.2\nOther call 764 3.06 23.5\nNoise 1000 25.65 30.8\n\u001f 3251 37.93 100.0\n5\nVol.:(0123456789)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\ncorpora, reported in Table 1, three additional unseen recordings were provided for the 10 different species and\n1 extra genus, with low, medium, and high appearance of target vocalizations. These were additionally used to\nvalidate model performance. In order to prove detection accuracy even further, an additional publicly-available\ndataset was utilized—the BirdVox-Full-Night data archive—presented by Lostanlen et al.36 for the evaluation\nof approaches designed to detect avian flight calls (see Table 1, last row). The original dataset consists of 9.8 h\nof audio, recorded by six sensors placed in the area around Ithaca, New York, which were manually annotated\nresulting in 35,402 500 ms-long flight calls of nocturnally migrating birds of about 25 species of passerines.\nTo balance the dataset an equal number of 500-ms-long noise samples were added to the dataset, resulting in\n70,804 files (see Table 1, last row). Regarding the BirdVox-Full-Night archive, there were no additional unseen\nrecordings, compared to the remaining data repositories listed in Table 1. Multi-class species classification was\nconducted between Blue-winged and Golden-winged warbler (see Table  2). In addition the Computational\nParalinguistics Challenge Primate (ComParE-PRS)37,38 dataset was used to distinguish between four different\nprimate species (see Table 4). The dataset includes over 10,000 annotated vocalizations from Chimpanzees (Pan\ntroglodytes), Mandrills (Mandrillus sphinx), Red-capped mangabeys (Cercocebus torquatus), and a mixed group\nof Guenons (Cercopithecus spp.). Additionally, exactly the same number of noise samples as vocalizations were\nextracted to make up the noise class38. Multi-class call type classification was computed for three main call type\nclasses of monk parakeet vocalizations including alarm, contact, and other calls, listed in Table 3. Across all\nmulti-class scenarios, the existing target class repertoire was extended by an additional noise category to simulate real-world scenarios, as well as cover and handle potential false alarms caused within the first detection stage\n(see Supplementary Figs. S5 and S6).\nDeep learning concepts and network architectures. Convolutional Neural Networks (CNNs) were\nutilized in order to identify animal vocalizations of interest. A CNN is an end-to-end deep learning architecture\nbased on the principles of pattern recognition including a feature learning and classification component being\nable to efficiently process the complexity of 2-dimensional input data (e.g., images, spectrograms)34,40,41. Convolutional layers are responsible for feature learning, while the classification part is done by the fully connected\nlayers40. Convolutional layers embed and represent the following important concepts34,40: (1) local receptive\nfields, (2) shared weights, and (3) subsampling (pooling). Due to the fact that convolutional and pooling layers only compute linear operations, CNNs integrate activation layers (e.g., Rectified Linear Unit34,42) as well as\nnormalization layers (e.g., batch normalization34,43) to handle the non-linearity within the data and to ensure a\nmore stabilized and regularized training procedure34. Several repetitive sequentially ordered sequences of convolutional, pooling, normalization, and activation layers lead to extracted and learned features which are used as\ninput for the fully-connected layer projecting the features on the respective output classes34. The core concept of\nthe presented deep learning framework is based on a so-called Residual Network (ResNet)35. A ResNet is a network architecture, which is built up from different concatenated residual layers35. A residual layer is constructed\nfrom an arrangement of building blocks which in turn consist of weight (e.g., convolutional, fully-connected),\nnormalization (e.g. batch-norm43), and activation layers (e.g., ReLU42), as well as residual-/skip-connections.\nDue to this residual-/skip-connection technique it is possible to learn a residual mapping F(x) = H(x) − x\ninstead of a direct underlying mapping H(x) for a given input x35, enabling to counteract the accuracy degradation problem (accuracy decrease after saturation region, by further increasing network depth, compared to shallower versions of the network35) and training deeper nets. Different numbers and structures of building blocks\nresult in various ResNet architectures. Well known and established ResNet models are ResNet18, ResNet34,\nResNet50, ResNet101, and ResNet15235. For more detailed insights about residual learning/networks see He\net al.35.\nANIMAL–SPOT. The deep learning framework consists of a ResNet18-based CNN, derived from ORCASPOT34, our previous killer whale deep detection model, which has been adapted and extended to handle all\nkinds of vocalizing animals. The initial max-pooling layer within the traditional ResNet18 architecture has been\nremoved to avoid losing too much resolution at the early stage of the training process34. Depending on the size\nof the temporal domain T of the input spectrogram, defined by the chosen training sequence length and corresponding FFT-settings, a 512-large global-averaged pooled feature vector, derived from the 512 × F × T feature\nmaps of the last residual layer (see Fig. 2), is generated and mapped to a subsequent fully-connected layer34. In\nTable 4.  The INTERSPEECH 2021 Computational Paralinguistics Challenge Primate (ComParE-PRS)37,38\ndata archive and distribution. Significant values are in bold.\nLabel type\nDistribution\nSamples Min. %-samples\nChimpanzee 6652 59.76 32.0\nMandrills 2623 13.14 12.7\nRed-capped mangabeys 627 5.31 3.0\nGuenons 476 1.69 2.3\nNoise 10,378 172.97 50.0\n\u001f 20,756 252.87 100.0\n6\nVol:.(1234567890)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\norder to solve the final n-class classification problem the 512 hidden units of the fully-connected layer are processed onto an output layer consisting of n output nodes depending on the classification task (e.g., two classes\nfor target/noise detection, or multiple classes for species/call type classification). ANIMAL-SPOT is capable of\nhandling any number of output classes, and consequently dealing with multi-class classification scenarios as\nwell. Moreover, ANIMAL-SPOT integrates a refactored version of the entire data parsing and pre-processing\npipeline of ORCA-SPOT34, next to additional normalization techniques, in order to handle and fulfill all needed\nprerequisites required for dealing with various animal data sources (see Fig. 1 and Tables 1, 2, 3, 4), in combination with varying classification scenarios. Although advances in neural network structures have been made in\nrecent years, the focus of ANIMAL-SPOT is not a specific type of architecture (e.g., ResNet18, etc.). Instead, the\naim of ANIMAL-SPOT is to provide an open-source, animal-independent, and expandable machine learning\nframework, together with a robust and efficient data preprocessing pipeline. We add support by profound user\nguidelines to address the broadest possible audience. In addition, the capacity of the model used here is comparatively small, which facilitate researchers who may not have the opportunity to access powerful hardware,\nto train and evaluate their own animal- and task-specific models. Within the ANIMAL-SPOT framework it is\npossible to integrate any kind of architectural model designs, allowing the deployment of other novel and userpreferred deep neural network concepts.\nData preprocessing. Independent of the classification scenario all species and their corresponding data\nrepositories (see Tables 1, 2, and 3), followed the same generic data preprocessing pipeline. The core functions\nare applicable for all animals, however each species requires an animal-specific parameter set (see Supplementary Table S2) in order to guarantee valid data preparation and representation of the corresponding signal characteristics (e.g., typical vocalization duration, frequency range, sampling rate, Fast-Fourier-Transform (FFT)\nparameters, etc.). The entire preprocessing pipeline of ANIMAL-SPOT consists of the following steps: (1) conversion to mono and re-sampling, (2) Short-Time-Fourier-Transform (STFT) to convert the time signal into\na F  ×  T-large power-spectrogram using an animal-characteristic FFT window-length and step-size, where F\ncharacterizes the frequency domain and T describes the time domain, (3) integrated and concurrent signal augmentation with respect to the previous derived F × T-large power-spectrogram applying uniformly distributed\nrandom scalings including intensity, pitch, and time augmentation within given intervals (see Supplementary\nTable S2), where the default interval might slightly vary from species to species, (4) linear frequency compression (nearest neighbor, 256 frequency bins) representing a frequency range between fmin and fmax, while ignoring other frequency regions, chosen according to the typical spectral vocalization areas of the corresponding\nanimals, resulting in a 256 × T compressed power spectrogram, (5) noise augmentation by adding a pitch-/\ntime-augmented and frequency-compressed noise spectrogram utilizing a uniformly distributed randomly chosen signal-to-noise ratio (SNR), (6) power-spectrogram conversion to decibel (dB) scale, (7) 0/1-min/max- or\n0/1-dB-normalization, either using the spectral minimum and maximum, or applying a minimum and reference\ndecibel level, dependent on the respective target species, to normalize the spectral envelope, and (8) random\nsub-sampling or zero-padding of the spectrogram according to the chosen sequence length, leading to a final\n256 × T augmented and normalized spectral clip being used as network input. Figure 3 visualizes example network input spectrograms for each species, preprocessed according to the illustrated pipeline.\nNetwork training and evaluation. Due to ANIMAL-SPOT’s ResNet18-based feature extraction and\ncompression path (see Fig. 2), each input spectrogram is compressed by a factor of 16 during encoding, both\nin time T and in frequency F domain. The remaining F × T features for each of the 512 channels are mapped\nto the corresponding fully connected layer, conducting global average pooling, followed by a projection to the\nnumber of parametrizable output nodes/classes. During training, random data augmentation and sub-sampling/\npadding, can be enabled. However, to compare validation and test set results across various models, random\ndata augmentation and sub-sampling/padding was disabled. Validation and test samples were centered and\neither zero-padded or sub-sampled, in case the original length did not match the chosen sequence length34.\nANIMAL-SPOT was implemented in PyTorch17 using a cross entropy loss in combination with a batch-size of\n8 for all animals, together with an Adam optimizer applying an initial learning of 10−5, β1 = 0.5, and β2 = 0.999.\nAdditionally, ANIMAL-SPOT integrates a learning rate decay of 1/2 after 4 epochs without any improvement\non the validation set. The training was stopped after an animal-specific number of epochs (see Supplementary\nFigure 2.  ANIMAL-SPOT Network Architecture (created via Inkscape39, Version 0.92.3).\n7\nVol.:(0123456789)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\nTable S2) if no improvement was achieved on the validation set (early stopping). The accuracy was chosen as an\nappropriate network validation criterion. ANIMAL-SPOT integrates an intelligent data split mechanism, capable of automatically identifying all class labels, assuming that data preparation was performed in the prescribed\nformat33, and ensures that samples of a particular recording are only present in one of the splits. By default, the\ndata split is 70% for training, 15% validation, and 15% test. However, it may differ depending on the original data\ndistribution in combination with the above mentioned recording restriction (see Supplementary Tables S4–S6).\nRegarding the two challenge datasets—BirdVox-Full-Night36 and ComParE-PRS37,38—the original predefined\ndata splits have been applied for reasons of comparison. Network training and evaluation was computed utilizing mid-range graphics processing units (GPUs) (e.g., Nvidia GTX 1080), as well as standard central processing\nunits (CPUs), showing the broad applicability of the training setup. Supplementary Table S2 reports all animalspecific network-hyperparameters.\nANIMAL-SPOT’s network performance was evaluated via the following experimental constellations: (1)\nanimal- and scenario-specific model evaluation for target/noise detection (see Supplementary Table S3) and\nmulti-class classification (see Supplementary Tables S4, S5, S6), reporting various performance metrics regarding training, validation, and unseen test set, (2) evaluation of animal-specific target/noise detection networks\non three fully-annotated unseen test recordings, performing a sliding window approach in combination with\na given window-length ǫ and step-size κ to frame-wise segment between target and noise, and (3) inspection\nand verification of the multi-class classification models for warblers and monk parakeets based on the machinesegmented and extracted signal parts of step 2, generated by the corresponding segmentation models.\nThe first evaluation scenario visualizes the following training, validation, and test metrics: accuracy (ACC),\ntrue-positive-rate (TPR), false-positive-rate (FPR), precision (PREC), F1-score (F1), and area under the ROC\ncurve (AUC). In case of the ComParE-PRS37,38 primate species recognition challenge, only the unweighted average recall (UAR) was reported due to comparability reasons.\nThe second evaluation procedure classifies audio sections for each of the three unseen, animal-specific test\nrecordings, depending on the defined window-length ǫ and step-size κ, affecting the signal overlap, as a whole.\nMachine-predicted audio chunks are compared frame-by-frame with the ground truth34. Each predicted frame/\nsegment of the unseen recordings, together with the respective frame-wise network confidence (probability),\nallow to present the ROC-curve44 and its respective AUC34. In addition, frames showing a larger value than a\nmodel confidence δ, are transformed into an annotation with its corresponding start and end time. Therefore,\nsuccessive frames of the same label (noise = 0 or target = 1) are concatenated and extracted as one annotation\nexcerpt34. Frame-wise smoothing was used to mark classified noise segments as target frames if the neighboring\nsignal chunks are exclusively labeled as target signals34. Neighboring frames are frames which include preceding or subsequent signal content of the current sound segment because of the respective overlap34. ANIMALSPOT-S refers to the smoothed version, whereas ANIMAL-SPOT corresponds to the non-smoothed variant\n(see Supplementary Figs. S2–S12). Additionally, the predicted, smoothed, and extracted network detections\nwith an exemplary model confidence of δ ≥ 50% and δ ≥ 90%, were used to calculate and report time-wise\nprecision (PREC) versus corresponding recall (TPR), in order to show intersection accuracy between machineand human-annotated labels. To calculate time-based precision and recall all ground truth annotations, which\nare not further apart than a merging factor ξ = ǫ\n2 (half of the prediction window in seconds), were combined to\none annotation, since such cases lead to sliding windows ǫ, while at least half of the window contains animal\nvocalizations. In case of time-wise precision calculation an additional overlapping factor \u001f = ǫ\n2 was introduced,\nFigure 3.  ANIMAL-SPOT preprocessed 256 × 128-large network input spectrograms (256 frequency bins, 128\ntime frames) utilizing animal-specific network-hyperparameters listed in Supplementary Table S2 (created via\nInkscape39, Version 0.92.3).\n8\nVol:.(1234567890)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\nextending the ground truth annotation start and end accordingly (start − \u001f, end + \u001f), covering overlapping\npredictions at the annotation borders.\nThe third and last evaluation scenario reports results on multi-class classification by presenting the following\nevaluation criteria on training, validation, and test data: (1) accuracy, (2) confusion matrix, and (3) UAR (only for\nthe ComParE-PRS37,38 dataset). Additionally, the model was evaluated on the corresponding machine-annotated\nwarbler and monk parakeet results, utilizing the same sliding window approach. However, during prediction of\nthe multiple classes, a noise identification was only considered as correct if the network confidence was higher\nthan > 85%, due to the assumption that the previous detection process has a low false positive rate. If the network’s\nconfidence regarding noise was lower than this boundary, but still the highest probability, it was ignored and the\nsecond largest confidence value was chosen as correct prediction. The chosen window length, in combination\nwith the input file duration, ends up in two potential cases: (1) window length is larger than the input file duration, leading to an updated window equal to the pre-segmented audio clip, and (2) window size is shorter than\nthe input file size, leading to a sliding, frame-wise classification approach determined by window- and step-size,\nwhile only considering full windows in order to avoid potential misclassification. If multiple sequentially-ordered\nframe-wise classifications per pre-segmented file exist, probabilities of each frame and predicted class are summed\nup cumulatively. Finally, the class providing the largest probability mass was selected. All detection and multiclass species/call type classification metrics are visualized and illustrated in Figs. 4, 5, 6, as well as Supplementary\nTable S7 and Supplementary Figs. S2–S12.\nANIMAL‑SPOT guide. The ANIMAL-SPOT Guide33 is a step-by-step and detailed user guide, publicly\navailable together with the source code33, which enables researchers to train and evaluate animal-specific deep\nneural networks on their own bioacoustic data corpora (see Supplementary Fig. S1). The guidelines involve:\n(1) operating-system independent installation, data preparation, and detailed documentation of the ANIMALSPOT source code33, (2) instructions and guidance in order to set up, train, and evaluate animal- and scenariospecific architectures, as well as (3) use-case dependent prediction of unseen data material utilizing stand-alone\nnoise/target detection models, species/call type classification networks, or a combined version of detection and\nsubsequent classification (see Supplementary Fig. S1). The ANIMAL-SPOT guide provides a detailed description with respect to the following three scenarios: (1) single-stage detection between animal vocalizations of\ninterest and noise, based on unseen data, (2) single-stage classification of animal species and/or specific call\ntypes directly on unseen raw audio material, and (3) a combined version of step 1 and 2 by firstly pre-segmenting\nunseen audio recordings, followed by subsequent classification (animal species, call types, etc.), while taking\nonly the respective pre-segmented target vocalizations as input.\nExperiments\nAnimal‑species target/noise segmentation. In a first experiment animal-species segmentation was\nperformed for all animal-specific (see Fig. 1) data volumes listed in Table 1. Data partitioning was conducted\nfor each animal-specific data archive (see Supplementary Table S3), whereas the training set comprises ≈70%,\nvalidation and test set each ≈15% of the total labeled data corpora listed in Table 1. Using the respective data\ndistributions in combination with the animal-specific network-hyper-parameters presented in Supplementary Table S2, different ANIMAL-SPOT architectures were trained and evaluated according to the previously\ndescribed network training and evaluation procedure. An exception is the BirdVox-Full-Night36 challenge dataset (see Table 1, last row), which used the same data distribution, training, and evaluation procedure as described\nin Lostanlen et al.36, in order to allow a meaningful comparison with the original results. Consequently, a leaveone-out testing procedure whereby one unit of the given six was utilized for testing and the other five were used\nfor training and validation. This results in exactly the same data split as reported, but also means that there exist\nno additional and unseen data available for further evaluation, as it was the case for all other data corpora listed\nFigure 4.  Overall summary across all 11 animal-specific segmentation models (10 different species, 1\nadditional genus), visualizing performance metrics with respect to the animal-specific, unseen, human-labeled\ntest data (1.1), ROC-curves and AUC-Range (1.2), as well as threshold-dependent precision/recall values (1.3),\nboth based on the animal-related unseen recording tapes (see also species-specific results in Supplementary\nFigs. S2–S12 and Supplementary Table S7) (created via Inkscape39, Version 0.92.3).\n9\nVol.:(0123456789)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\nTable 1. For reasons of comparison and the sake of completeness, parts of the already published results on killer\nwhales (see ORCA-SPOT34) are reported and visualized as well.\nMulti‑class species and call type classification. As baseline for the second experiment, results of the\nfirst target/noise (binary) animal-species segmentation were utilized, next to a pure stand-alone multi-species\nFigure 5.  Multi-class warbler species identification results, visualizing spectrogram examples of Blue-/Goldenwinged warblers (1.1,1.2), multi-class training/validation accuracy (1.3), confusion matrix regarding the\nunseen human-labeled test data (1.4), as well as confusion matrix concerning previous machine-based warbler\ndetection (1.5) (created via Inkscape39, Version 0.92.3).\nFigure 6.  Multi-class call type classification results, visualizing spectrogram examples of alarm, contact, and\nother call types (1.1–1.3), multi-class training/validation accuracy (1.4), confusion matrix regarding the unseen\nhuman-labeled test data (1.5), as well as confusion matrix concerning previous machine-based monk parakeet\ndetection (1.6) (created via Inkscape39, Version 0.92.3).\n10\nVol:.(1234567890)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\nclassification scenario without pre-segmentation. To demonstrate, prove and verify performance of the proposed\nmulti-step classification procedure, three of the 12 animal species (see Fig.  1) were utilized—Golden-/Bluewinged warblers (genus) and monk parakeets.\nIn case of the warblers a subsequent multi-class classification model, trained on the data and distribution listed\nin Supplementary Table S4, was used to further separate the genus-specific and previously machine-segmented\ndata pool into Golden-winged, Blue-winged warblers, other birds, and noise. To counteract possible false alarms\nfrom the segmentation phase, two classes—other birds and noise—were introduced besides the corresponding\nsignals of interest.\nThe proposed multi-stage approach was further evaluated training a multi-class classification network to differentiate between various monk parakeet call types, using the data and distribution in Supplementary Table S5,\nincluding contact, alarm, and other calls, as well as noise, in order to handle previous segmentation errors.\nIn case of the golden- and blue-winged warblers, a total of 210 machine-annotated audio segments were\nextracted utilizing a network confidence of ≥ 90%. Under identical conditions 103 monk parakeet machine\nsegmentations were predicted and extracted. Example spectrograms for golden-/blue-winged warbler vocalizations, as well as for the various monk parakeet call types, are visualized in Figs. 5 and 6.\nIn order to assess the efficacy of the multi-stage approach, instead of a single-stage multi-class approach, a\nmulti-class model was exemplarily trained to perform detection and classification of Golden-winged warbler,\nBlue-winged warbler, and noise (pure background noise, other birds) in one step, using exactly the same three\nunseen, manually labeled recordings for evaluation as during the detection phase within the multi-stage procedure (see Table 1). The three audio files contain either: (1) only Golden-winged warblers, (2) only Blue-winged\nwarblers, (3) a combination of both warbler types. The model used the same data distribution for Golden-/\nBlue-winged warbler as stated in Supplementary Table S4, together with the warbler noise distribution listed in\nSupplementary Table S3.\nNevertheless, in order to also show and demonstrate the possibility of directly training a multi-species classification network without previous segmentation, the ComParE-PRS dataset (see Table 4) was used to distinguish\nbetween 4 different primate species as well as background noise, trained on the given data distribution listed in\nSupplementary Table S6.\nResults\nAnimal‑species target/noise segmentation. ANIMAL-SPOT successfully segmented all 10 target species, as well as the additional genus, leading to an overall mean test set accuracy of 97.9% (range: 94.5–99.8%).\nAdditionally, an average area under the ROC curve (AUC) of 95.9% (range: 91.7–99.1%) across all 33 unseen\nanimal-specific recordings (3 tapes per detection scenario) was achieved (see Supplementary Table S7. Besides\nnetwork generalization on the unseen tapes, a detailed performance overview with respect to model training,\nvalidation, and testing is reported and visualized in Supplementary Figs.  S2–S12. In addition, all detection\nresults are summarized and available in Supplementary Table S7. Moreover, Fig. 4 summarizes detection results\nin a compressed version, averaged across all 11 animal-specific segmentation models (10 different species, 1\nadditional genus), visualizing: (1) network performance metrics based on the animal-specific human-annotated\ntraining, validation, and testing repositories (see Supplementary Table S3, Fig. 4—1.1), (2) model results across\nall 11 averaged Receiver-Operating-Characteristics44 (ROC) curves by visualizing 2 out of 11 curves, indicating\nthe minimum and maximum AUC, spanning the average AUC-range where all other remaining ROC-curves\nare located (Fig. 4—1.2), and (3) network output across all 11 averaged and threshold-dependent precision/\nrecall scores (Fig. 4—1.3). Apart from the segmentation results of all 11 animal-specific segmentation models\n(10 different species, 1 additional genus), ANIMAL-SPOT also successfully processed the BirdVox-Full-Night36\ndataset. In comparison to the results given by Lostanlen et  al.36, with the best performance coming from a\nCNN with noise augmentation, which resulted in an average accuracy of 94.9% and an average F1-Score of\n62.7%, ANIMAL-SPOT achieved a slightly better average accuracy of 95.4% and a significantly better F1-Score\nof 95.4%. These results were achieved by training 10 models for each unit and taking the average of the results\nwhen removing the best and worst two performing models, resulting in an average over six models for each unit.\nMulti‑class species and call type classification. Multi-class species classification was applied to the\nprevious warbler target/noise detection results (see Supplementary Fig.  S6), in order to further separate the\nmachine-segmented warbler species vocalizations, exemplarily visualized in Fig. 5—1.1,1.2, into Blue-winged\nand Golden-winged warbler, resulting in a multi-class (4-classes) species identification scenario. Therefore,\nANIMAL-SPOT, trained in a multi-class species classification scenario using the data listed in Supplementary\nTable S4, achieved an overall accuracy of 96.6% for the human-labeled unseen test set, as well as 95.2% with\nrespect to the total number of 210 previously machine-detected audio segments (see Supplementary Fig. S6).\nMoreover, training and validation accuracy is shown besides two confusion matrices (4 classes), visualizing the\naforementioned results achieved on the respective unseen human-labeled and machine-segmented test corpora\n(see Fig. 5—1.3–1.5, Supplementary Fig. S6).\nCompared to the results of the proposed two-stage approach, which includes target/noise detection and downstream multi-class identification, the single-stage method, which performs both, detection and classification, in a\nmulti-class model at once, performed significantly worse. Across all three unseen warbler recordings the Golden-/\nBlue-winged warbler detection model (threshold δ≥ 0.9) identified a total of 210 potential vocalizations of interest, resulting in a time-based precision of 95.3% (see Supplementary Fig. S6). All 210 of the segmented samples\nwere then used for downstream multi-class classification. In comparison, the 3-class single-stage approach\ndetected 233 warbler events, with 200 true predictions resulting in a sample-based precision of 85.8%, whereas\njust 77.3% (180 out of 233 vocal events) were detected and classified as the correct warbler species. In the case \n11\nVol.:(0123456789)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\nof the two-stage approach multi-species classification achieved an accuracy of 95.2% (see Fig. 5). This indicates\nthat a two-step approach, where the network can focus more on the distinguishing features of the individual bird\nspecies without also having to filter out as much noise, is preferable to a single-step approach.\nBesides all the results regarding warbler-species classification, ANIMAL-SPOT was also successfully deployed\nto identify various primate species using the Computational Paralinguistics Challenge Primate (ComParEPRS)37,38 data archive. The initial challenge investigation utilizes five different approaches for feature extraction\nand classification of primate vocalizations, namely openSMILE45, openXBOW46, DeepSpectrum47, AuDeep48,\nand End2You49 in conjunction with either an SVM (openSMILE, openXBOW, DeepSpectrum, AuDeep) or a\nrecurrent neural network (RNN) (End2You) for the final classification. The initial baseline for the challenge,\ncalculated by majority voting using the best configuration for each approach, reported the best unweighted\naverage recall (UAR) of 87.5%37,38. In comparison, ANIMAL-SPOT outperformed the baseline achieving a UAR\nof 89.3%. Multi-class call type classification was applied to the previous monk parakeet target/noise detection\nresults (see Supplementary Fig. S5), similarly to the warblers, however, with the aim to classify between different\ncall types visualized in Fig. 6—1.1–1.3, leading to a multi-class (4 classes) monk parakeet call type classification\nscenario. For this purpose, ANIMAL-SPOT was trained on the data listed in Supplementary Table S5. The final\nmodel achieved an overall test set accuracy of 92.7%, compared to 88.4% on the previous machine-based detection results (see Fig. 6—1.4–1.6, Supplementary Fig. S5).\nDiscussion and future outlook\nIn total, 10 different species and 1 extra genus (see Figs. 1, 4, Supplementary Figs. S2–S12), as well as the publiclyavailable BirdVox-Full-Night36 dataset, were analyzed in a binary detection scenario in order to prove ANIMALSPOT’s ability to generalize across a wide variety of sound-types and to assess the feasibility of the proposed\nmulti-stage detection/classification pipeline (see Figs. 5 and 6). As the results on the unseen recordings prove,\npromising time-wise and threshold-dependent recall/precision values were achieved, indicating an accurate\nintersection between ANIMAL-SPOT’s predictions and the actual ground truth (see Fig. 4—1.3, Supplementary\nTable S7 and Supplementary Figs. S2–S12). In addition, ROC-curves and corresponding AUC values show a\nsignificant reduction of the species-dependent and original noise-heavy data material (see Fig. 4—1.2). Thus,\nthreshold-dependent recall and false-positive-rate combinations can be derived according to the respective usecase, which in turn considerably speeds up and improves downstream data analysis. Furthermore, the combined\nstrong results seen in both unseen test set as well as unseen real-world recordings, suggest no indication of model\noverfitting and prove network generalization across all different animal species.\nThe improvements with respect to the publicly available BirdVox-Full-Night36 dataset are also very promising, as the detection accuracy was improved by 0.6%, which indicates an error reduction of about 12%, besides\na significant improvement of 32.7% regarding the F1-Score.\nANIMAL-SPOT integrates a large repertoire of distinct parameterization options for setting up data preprocessing and network training (see Supplementary Table S2). Thus, ANIMAL-SPOT performs equally across\nwide ranges of temporal contexts (e.g average vocalization duration of Pygmy pipistrelles compared to killer\nwhales), frequency ranges (e.g low-range Atlantic cod and Harbour seal vocalizations, mid-range bird sounds,\nand ultrasound bat signals), as well as spectral patterns (e.g., pulse-like structure of the Harbour seal or warbler\nsignals and harmonic properties of the killer whale, Atlantic cod, and chimpanzee vocalizations). It is even possible to learn and distinguish between spectral call structures which are very similar to noise, seen in ANIMALSPOT’s exemplary ability to distinguish Sulphur-crested cockatoo, Harbour seal, monk parakeet, and chimpanzee\nvocalizations from very similar background noise. In case of binary target/noise detection, ANIMAL-SPOT is\nespecially useful in recording situations where the noise characteristics are an order of magnitude larger than\nthe amount of valuable animal vocalizations.\nANIMAL-SPOT’s parameterization capacity also enables flexible adaptations regarding model architecture,\ndata preprocessing, and network training/evaluation, allowing researchers to address and answer various specific\nbioacoustic research questions. Furthermore, the binary-class target/noise detection process enables researchers\nto separate target species that show poor results in the single-stage binary target/noise detection scenario. This\ncan occur especially when the target species spectrally resemble other vocalizing species that are also found in\nthe unseen recordings. In such situations the primary focus is on a generic distinction between target vocalizations and superfluous noise, making subtle spectral differences of other species difficult to model, because of\ngeneralization properties across both classes leading to increasing mis-classifications. This phenomenon was\nobserved in case of Blue-winged and Golden-winged warblers, after both were individually trained and analyzed\non species level, which demonstrated significant performance variations. However, using a two-step identification scenario consisting of target/noise detection at genus level (see Supplementary Fig. S6), and subsequent\nmulti-class species classification, ANIMAL-SPOT achieved an overall test set accuracy of 96.6% on unseen test\ndata, which had been labeled by a human expert, and an accuracy of 95.2% on the target detections identified\nby ANIMAL-SPOT in the target/noise detection scenario (see Fig. 5).\nIn addition, the same two-step approach was successfully applied to distinguish and classify between different\nmonk parakeet call types, resulting in 92.7% test set accuracy for human-annotated samples, and 88.4% with\nrespect to the machine-performed detection results (see Supplementary Fig. S5, see Fig. 6). These combined\nresults demonstrate the wide range of biological scenarios which can be covered by ANIMAL-SPOT in combination with user- and animal-specific data material. In both multi-class classification scenarios—warbler species\nand monk parakeet call types—ANIMAL-SPOT extracts and classifies centered signal sections of the unseen\nnetwork test set samples according to the training sequence length (see Supplementary Table S2, see Figs. 5—1.4\nand 6—1.5). However, the pre-segmented audio chunks, different in length, were classified by utilizing a sliding\nwindow approach, together with the corresponding settings (see Figs. 5—1.5 and 6—1.6). At each frame the \n12\nVol:.(1234567890)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\nmaximum probability of all classes was chosen. The class showing the highest probability mass across the entire\nsignal was selected as the final network hypothesis. In both experiments, each of the confusion matrices show\ncomprehensible and similar results, being an auspicious indicator for model generalization across various data\n(see Figs. 5—1.4,1.5 and 6—1.5,1.6). Furthermore, sequence length and step-size are very important parameters\nto guarantee robust predictions. In terms of species and/or call type classification the step-size should be an\norder of magnitude smaller than the sequence length, in order to guarantee sufficient overlap and not to miss\nimportant spectral features during the prediction phase.\nANIMAL-SPOT demonstrated also great results in terms of single-stage multi-species primate classification, by outperforming the ComParE baseline system. The final result of 89.3% also exceeds the UAR of 88.3%\npresented by challenge competitor Illium et al.50, who applied a vision transformer to the classification problem.\nMüller et al.51 report the same UAR of 89.3% while applying a Deep Recurrent Neural Network. The remaining\ncompetitors who performed better than ANIMAL-SPOT utilized either ensembling of multiple classifiers, as in\nthe case of Egas-López et al.52, who achieved a UAR of 89.8%, or data augmentation techniques such as SpecAugment or MixUp and training tricks such as exponential moving average of the model weights, as presented by\nThomas Pellegrini53, who achieved a UAR of 92.5% on the test set. ANIMAL-SPOT is therefore placed squarely\nin the middle of the top challenge performers despite using only a single, relatively simple classifier and basic\naugmentation techniques.\nIn order to robustly train and report promising results, data volume, distribution, and variation is crucial.\nMoreover, the data corpus must be representative with respect to unseen real-world data. If these criteria are not\nfulfilled, models often lead to significantly worse results, despite promising training, validation, and test metrics.\nIn order to enlarge data variation, especially for small animal corpora, various embedded spectral augmentations\nwere computed (see Supplementary Table S2). However, such augmentation variants and corresponding values\nmust be determined independently for each animal species and can therefore not be generalized. In particular,\nnoise augmentation must be applied carefully, because of differing Signal-to-Noise-Ratio (SNR) between the\noriginal sounds and utilized noise data, particularly in case of animal vocalizations being very similar to noise\ndata (e.g., Sulphur-crested cockatoo, distant chimpanzee pant-hoot versus bird vocalizations in the same frequency range). Therefore, it is essential to ensure that noise samples, chosen for augmentation, are representative\nand independent from training, validation, and test noise excerpts. Despite promising scenario- and animalspecific results on the unseen test data, audio recordings, machine-driven pre-detections, and challenge datasets\n(see Figs. 4, 5, 6, Supplementary Figs. S2–S12, and Supplementary Table S7), the performance may still vary to\na certain extent, due to the following reasons: (1) non-representative data and/or insufficient training data, (2)\nrecording artifacts introducing spectral outliers which are difficult to interpret by the network, (3) other animal\nvocalizations or noise characteristics showing a similar spectral envelope as the target sounds, (4) strong deviation\nof the signal intensities compared to the chosen reference and minimum dB-values of the 0/1-dB-normalization\nduring training (see Supplementary Table S2), (5) overlapping animal signals and human narrations, (6) vocalization types of a given species which have significant spectral and temporal differences between each other, and\n(7) window-length ǫ and step-size κ used during prediction phase. Figure 7 visualizes different examples of such\nanimal- and task-specific misclassifications, caused by the previously illustrated error sources, which significantly\ninfluence network prediction results.\nFigure 7.  Spectrogram examples visualizing potential error sources leading to performance drops of ANIMALSPOT (created via Inkscape39, Version 0.92.3).\n13\nVol.:(0123456789)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\nFurthermore, during multi-class classification, special attention needs to be paid towards correct machinebased detection outputs indicating one of the following scenarios: (1) multiple vocalizations of the same and/\nor different species/call types within a single segment, (2) truncated signals, either at the beginning or end of\na segment, and (3) overlapping vocalizations. Examples of the above mentioned and remaining challenges are\nvisualized in Fig. 8.\nIn addition, data collection should be conducted via a consistent recording setup. Including data material\noriginating from varying recording environments and/or setups will result in spurious outputs unless sufficient\nexamples of this variation is represented in the training and validation datasets.\nANIMAL-SPOT’s performance and network training stabilization strongly correlates with the chosen hyperparameter setup, respective data structure and distribution, as well as model initialization. In order to identify\nthe best fitting training setup for a certain species and classification procedure, a parametric search within the\ntarget-specific value range (with regards to e.g., signal frequency range, average sound duration, type of vocalization) should be performed. Additionally suitable prediction settings—window length ǫ and step-size κ—as well as\nnetwork parameters (see Supplementary Table S2) are very important. Window length ǫ has to be approximately\nin the same dimension as the network training length. To ensure adequate prediction settings, ANIMAL-SPOT\nshould be evaluated on a small portion of unseen manually labeled recordings, before processing large unseen\ndata archives. Moreover, network initialization, as well as random augmentations during training, may impact\nnetwork performance, especially in case of small training corpora, affecting final model performance despite\nsimilar training, validation, and test set metrics.\nResearchers face various acoustic detection scenarios, namely simple target/noise segmentation, identification\nof target signals among other distinct animal-specific vocalizations, and the recognition of target vocalizations\namong other similar animal-related vocalizations. All these scenarios can be addressed by the ANIMAL-SPOT\nframework and its underlying methods. For the simple case of identifying a target signal among nondescript\nbackground noise a simple one-step procedure can be applied as well as utilization of the framework-supplied\nnoise augmentation to account for differences in signal-to-noise ratios in varying real-world conditions. Similarly,\nin the case where the target signal is dissimilar to the to other known vocalizations, a one-step model application\nprocedure can be applied, and the classification is altered from a binary target-noise scenario to a multi-class\nproblem which includes vocalizations from other known species present in the recordings. Finally, when dealing\nwith the scenario in which the target vocalization exhibits similar characteristics such as to make them difficult\nto discern from each other, a multi-step approach can be taken, as was shown when attempting to accurately\ndistinguish between Blue and Golden-winged warblers (see Fig. 5) or different monk parakeet call types (see\nFig. 6). The same applies to the recognition of species-specific dialects and single individuals. The first task is\nto eliminate to the fullest extent the background noise (pure noise, other dissimilar animal vocalizations) from\nthe classification problem. After background noise is removed from the data, it appears that the model is more\ncapable of distinguishing between similar acoustic features through the focus on other spectral characteristics\nand features. Note that, due to the relatively small model sizes used here, a two-step approach could also be\napplied to the case where vocalizations are dissimilar without incurring a significant penalty with respect to\ncomputation time.\nBesides the animal-independent target/noise and multi-step/class identification results (see Figs. 4, 5, and 6),\nthis study also puts special emphasis on the proposed ANIMAL-SPOT guide33 (see Supplementary Fig. S1), which\nenables researchers to setup their own user-specific deep learning framework, without the need of prior machinelearning knowledge. The ANIMAL-SPOT guide33 describes the entire software framework from beginning to\nend, including OS-specific installation manuals regarding all necessary software components, data preparation\nand processing guidelines, as well as detailed descriptions on how to setup, train, and run the final network\nprediction/evaluation on unseen data (see Supplementary Fig. S1).\nThe entire deep learning framework, as well as user- and animal-specific setup, can be verified and evaluated\nthrough the additionally provided example data archive on monk parakeets54 , which is publicly available33, next\nto all the source code and user-friendly instruction manual. This guide enables the bioacoustic community to\nindependently train/evaluate task- and animal-specific deep models in order to gain deeper insights into animal\ncommunication and understanding.\nFigure 8.  Machine-segmented spectrograms for Blue-/Golden-winged warblers and monk parakeets,\nvisualizing various challenging scenarios for a potential subsequent multi-class classification (created via\nInkscape39, Version 0.92.3).\n14\nVol:.(1234567890)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\nMany different fields of potential future follow-up work arise, such as (1) animal-specific representation55\nand/or transfer learning, utilizing larger labeled/unlabeled bioacoustic data corpora and/or other data archives\n(e.g., ImageNet56), (2) investigation regarding various deep network architectures (e.g., CNN-LSTM25, ResNeXt57,\nInception/Inception-ResNet58, or Transformer-based approaches59), and (3) animal-independent signal enhancement/denoising60, acting as additional data preprocessing option. To the best of the authors knowledge, ANIMAL-SPOT is the first open-source33 machine learning approach, capable of handling various bioacoustic signal\nidentification scenarios (binary target/noise detection, multi-class species/call type classification), verified on a\nwide portfolio of animal vocalizations from different animal taxa and challenge datasets. In combination with a\ndetailed user guide, ANIMAL-SPOT allows the broader bioacoustic research community to develop their own\ntask-specific deep neural networks, on virtually any animal species.\nData availibility\nThe acoustic data archives supporting the findings of this study are available from the respective data owners\nupon reasonable request. Contact details can be obtained from the corresponding author. Upon acceptance,\nthe code for ANIMAL-SPOT, besides the entire ANIMAL-SPOT guidelines, all together with an example data\ncorpus54, will be made publicly available at https://github.com/ChristianBergler.\nReceived: 12 September 2022; Accepted: 14 December 2022\nReferences\n1. Sugai, L. S. M., Silva, T. S. F., Ribeiro, J., José, Wagner & Llusia, D. Terrestrial passive acoustic monitoring: Review and perspectives.\nBioScience 69, 15–25. https://doi.org/10.1093/biosci/biy147 (2018).\n2. Symes, L. B. et al. Analytical approaches for evaluating passive acoustic monitoring data: A case study of avian vocalizations. Ecol.\nEvol. 12, e8797. https://doi.org/10.1002/ece3.8797 (2022).\n3. Van Hoeck, R. V. et al. Passive acoustic monitoring complements traditional methods for assessing marine habitat enhancement\noutcomes. Ecosphere 12, e03840. https://doi.org/10.1002/ecs2.3840 (2021).\n4. Ness, S. The Orchive : A system for semi-automatic annotation and analysis of a large collection of bioacoustic recordings. Ph.D.\nthesis, Department of Computer Science, University of Victoria, 3800 Finnerty Road, Victoria, British Columbia, Canada, V8P\n5C2 (2013).\n5. Allen, A. N. et al. A convolutional neural network for automated detection of humpback whale song in a diverse, long-term passive\nacoustic dataset. Front. Mar. Sci. https://doi.org/10.3389/fmars.2021.607321 (2021).\n6. Pérez Granados, C. & Schuchmann, K.-L. Passive acoustic monitoring of chaco chachalaca (Ortalis canicollis) over a year: Vocal\nactivity pattern and monitoring recommendations. Trop. Conserv. Sci. https://doi.org/10.1177/19400829211058295 (2021).\n7. Davis, G. et al. Long-term passive acoustic recordings track the changing distribution of North Atlantic right whales (Eubalaena\nglacialis) from 2004 to 2014. Sci. Rep. https://doi.org/10.1038/s41598-017-13359-3 (2017).\n8. Melo, I., Llusia, D., Bastos, R. P. & Signorelli, L. Active or passive acoustic monitoring? Assessing methods to track anuran communities in tropical savanna wetlands. Ecol. Indic. 132, 108305. https://doi.org/10.1016/j.ecolind.2021.108305 (2021).\n9. Håkansson, G. & Westander, J. Communication in Humans and Other Animals (John Benjamins Publishing Company, 2013).\n10. Hill, A. et al. AudioMoth: Evaluation of a smart open acoustic device for monitoring biodiversity and the environment. Methods\nEcol. Evol. https://doi.org/10.1111/2041-210X.12955 (2017).\n11. Wall, C. et al. The next wave of passive acoustic data management: How centralized access can enhance science. J. Acoust. Soc. Am.\n150, A79–A79. https://doi.org/10.1121/10.0007688 (2021).\n12. Browning, E., Gibb, R., Glover-Kapfer, P. & Jones, K. E. Passive acoustic monitoring in ecology and conservation, https://doi.org/\n10.13140/RG.2.2.18158.46409 (2017).\n13. Gibb, R., Browning, E., Glover-Kapfer, P. & Jones, K. E. Emerging opportunities and challenges for passive acoustics in ecological\nassessment and monitoring. Methods Ecol. Evol. 10, 169–185. https://doi.org/10.1111/2041-210X.13101 (2019).\n14. Hilbert, M. & López, P. The world’s technological capacity to store, communicate, and compute information. Science 332, 60–65\n(2011).\n15. Sood, D., Kour, H. & Kumar, S. Survey of computing technologies: Distributed, utility, cluster, grid and cloud computing. JNCET\n6 (2016).\n16. Géron, A. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to bBuild Intelligent Systems 2nd edn. (O’Reilly Media, 2019).\n17. Paszke, A. et al. Automatic differentiation in PyTorch. In NIPS 2017 Workshop (2017).\n18. Mittal, S. & Vaishay, S. A survey of techniques for optimizing deep learning on GPUs. J. Syst. Archit. 99, 101635. https://doi.org/\n10.1016/j.sysarc.2019.101635 (2019).\n19. Wu, Z., Sun, J., Zhang, Y., Wei, Z. & Chanussot, J. Recent developments in parallel and distributed computing for remotely sensed\nbig data processing. Proc. IEEE 109, 1282–1305. https://doi.org/10.1109/JPROC.2021.3087029 (2021).\n20. Howard, J. & Gugger, S. Fastai: A layered API for deep learning. Information 11, 108. https://doi.org/10.3390/info11020108 (2020).\n21. Stowell, D. Computational bioacoustics with deep learning: A review and roadmap. PeerJ 10, e13152 (2022).\n22. Bianco, M. J. et al. Machine learning in acoustics: Theory and applications. J. Acoust. Soc. Am. 146, 3590–3628. https://doi.org/10.\n1121/1.5133944 (2019).\n23. Shiu, Y. et al. Deep neural networks for automated detection of marine mammal species. Sci. Rep. 10, 607. https://doi.org/10.1038/\ns41598-020-57549-y (2020).\n24. Bermant, P., Bronstein, M., Wood, R., Gero, S. & Gruber, D. Deep machine learning techniques for the detection and classification\nof sperm whale bioacoustics. Sci. Rep. 9, 1–10. https://doi.org/10.1038/s41598-019-48909-4 (2019).\n25. Madhusudhana, S. et al. Temporal context improves automatic recognition of call sequences in soundscape data. J. Acoust. Soc.\nAm. 148, 2442. https://doi.org/10.1121/1.5146737 (2020).\n26. Thomas, M., Martin, B., Kowarski, K., Gaudet, B. & Matwin, S. Marine mammal species classification using convolutional neural\nnetworks and a novel acoustic representation. In Joint European Conference on Machine Learning and Knowledge Discovery in\nDatabases, 290–305 (Springer, 2019).\n27. Priyadarshani, N., Marsland, S. & Castro, I. Automated birdsong recognition in complex acoustic environments: A review. J. Avian\nBiol. 49, jav01447. https://doi.org/10.1111/jav.01447 (2018).\n28. Stowell, D., Wood, M., Pamuła, H., Stylianou, Y. & Glotin, H. Automatic acoustic detection of birds through deep learning: The\nfirst bird audio detection challenge. Methods Ecol. Evol. 10, 368–380 (2018).\n29. Mac Aodha, O. et al. Bat detective-deep learning tools for bat acoustic signal detection. PLoS Comput. Biol. 14, 1–19. https://doi.\norg/10.1371/journal.pcbi.1005995 (2018).\n15\nVol.:(0123456789)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\n30. Khalighifar, A. et al. Application of deep learning to community-science-based mosquito monitoring and detection of novel species. J. Med. Entomol. 59, 355–362. https://doi.org/10.1093/jme/tjab161 (2021).\n31. Bravo Sanchez, F. J., Hossain, M. R., English, N. B. & Moore, S. T. Bioacoustic classification of avian calls from raw sound waveforms\nwith an open-source deep learning architecture. Sci. Rep. 11, 1–12 (2021).\n32. Zhang, L., Wang, D., Bao, C., Wang, Y. & Xu, K. Large-scale whale-call classification by transfer learning on multi-scale waveforms\nand time-frequency features. Appl. Sci. 9, 1020. https://doi.org/10.3390/app9051020 (2019).\n33. Bergler, C. Github-Repository. https://github.com/ChristianBergler.\n34. Bergler, C. et al. Orca-spot: An automatic killer whale sound detection toolkit using deep learning. Sci. Rep. 9, 1–17. https://doi.\norg/10.1038/s41598-019-47335-w (2019).\n35. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 770–778 (2016).\n36. Lostanlen, V., Salamon, J., Farnsworth, A., Kelling, S. & Bello, J. P. Birdvox-full-night: A dataset and benchmark for avian flight\ncall detection. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 266–270, https://doi.\norg/10.1109/ICASSP.2018.8461410 (2018).\n37. Schuller, B. W. et al. The INTERSPEECH 2021 computational paralinguistics challenge: COVID-19 cough, COVID-19 speech,\nescalation & primates. In Interspeech 2021, https://doi.org/10.21437/interspeech.2021-19 (ISCA, 2021).\n38. Zwerts, J. A. et al. Introducing a Central African primate vocalisation dataset for automated species classification. In Proc. Interspeech 2021, 466–470, https://doi.org/10.21437/Interspeech.2021-154 (2021).\n39. Inkscape Project. Inkscape. https://inkscape.org (March 2018), Version 0.92.3.\n40. LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE 86, 2278–2324\n(1998).\n41. Maier, A., Syben, C., Lasser, T. & Riess, C. A gentle introduction to deep learning in medical image processing. Zeitschrift für\nMedizinische Physik 29, 86–101 (2019).\n42. Nair, V. & Hinton, G. E. Rectified linear units improve restricted Boltzmann machines. In Proceedings of the 27th International\nConference on International Conference on Machine Learning, 807–814 (2010).\n43. Ioffe, S. & Szegedy, C. Batch normalization: accelerating deep network training by reducing internal covariate shift. In Proceedings\nof the 32nd International Conference on International Conference on Machine Learning, vol. 37, 448–456 (2015).\n44. Fawcett, T. Roc graphs: Notes and practical considerations for researchers. Mach. Learn. 31, 1–38 (2004).\n45. Eyben, F., Wöllmer, M. & Schuller, B. Opensmile: the munich versatile and fast open-source audio feature extractor. In Proceedings\nof the 18th ACM international conference on Multimedia (2010).\n46. Schmitt, M. & Schuller, B. Openxbow: Introducing the passau open-source crossmodal bag-of-words toolkit. J. Mach. Learn. Res.\n18, 3370–3374 (2017).\n47. Zhao, Z. et al. Deep spectrum feature representations for speech emotion recognition. In Proceedings of the Joint Workshop of the\n4th Workshop on Affective Social Multimedia Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia\nData, ASMMC-MMAC’18, 27–33, https://doi.org/10.1145/3267935.3267948 (Association for Computing Machinery, New York,\nNY, USA, 2018).\n48. Freitag, M., Amiriparian, S., Pugachevskiy, S., Cummins, N. & Schuller, B. Audeep: Unsupervised learning of representations from\naudio with deep recurrent neural networks. J. Mach. Learn. Res. 18, 6340–6344 (2017).\n49. Tzirakis, P. End2you: Multimodal profiling by end-to-end learning and applications. In Proceedings of the 1st International on\nMultimodal Sentiment Analysis in Real-Life Media Challenge and Workshop, MuSe’20, 9, https://doi.org/10.1145/3423327.34235\n13 (Association for Computing Machinery, 2020).\n50. Illium, S., Müller, R., Sedlmeier, A. & Popien, C.-L. Visual Transformers for Primates Classification and Covid Detection. In Proc.\nInterspeech 2021, 451–455, https://doi.org/10.21437/Interspeech.2021-273 (2021).\n51. Müller, R., Illium, S. & Linnhoff-Popien, C. A Deep and Recurrent Architecture for Primate Vocalization Classification. In Proc.\nInterspeech 2021, 461–465, https://doi.org/10.21437/Interspeech.2021-1274 (2021).\n52. Egas-López, J. V., Vetráb, M., Tóth, L. & Gosztolya, G. Identifying conflict escalation and primates by using ensemble X-vectors\nand Fisher vector features. In Proc. Interspeech 2021, 476–480, https://doi.org/10.21437/Interspeech.2021-1173 (2021).\n53. Pellegrini, T. Deep-learning-based central African primate species classification with MixUp and SpecAugment. In Proc. Interspeech\n2021, 456–460, https://doi.org/10.21437/Interspeech.2021-1911 (2021).\n54. Smeele, S. Q., Tyndel, S. A., Aplin, L. M. & McElreath, M. B. Multi-level analysis of monk parakeet vocalisations shows emergent\ndialects between cities in the European invasive range. bioRxiv https://doi.org/10.1101/2022.10.12.511863 (2022).\n55. Bergler, C. et al. Deep representation learning for orca call type classification. In Text, Speech, and Dialogue, 22nd International\nConference, TSD 2019, Ljubljana, Slovenia, September 11–13, 2019, Proceedings, vol. 11697 LNAI, 274–286, https://doi.org/10.\n1007/978-3-030-27947-9_23 (Springer Verlag, 2019).\n56. Deng, J. et al. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, 248–255 (IEEE, 2009).\n57. Xie, S., Girshick, R., Dollár, P., Tu, Z. & He, K. Aggregated residual transformations for deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 5987–5995, https://doi.org/10.1109/CVPR.2017.634 (2017).\n58. Szegedy, C., Ioffe, S., Vanhoucke, V. & Alemi, A. Inception-v4, inception-resnet and the impact of residual connections on learning.\nAAAI Conference on Artificial Intelligence (2016).\n59. Wang, Y. et al. Transformer-based acoustic modeling for hybrid speech recognition. In ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 6874–6878 (IEEE, 2020).\n60. Bergler, C. et al. ORCA-CLEAN: A Deep Denoising Toolkit for Killer Whale Communication. In Proc. Interspeech 2020, 1136–1140,\nhttps://doi.org/10.21437/Interspeech.2020-1316 (2020).\n61. ORCALAB. Orcalab—a whale research station on Hanson Island. http://orcalab.org (2022).\n62. Ness, S. Orchive. http://orchive.cs.uvic.ca/ (2020).\nAcknowledgements\nWe are grateful for support with—data collection of cockatiel data: Lucy Aplin and Gustavo Alarcón-Nieto; data\ncollection and annotations of killer whale data (orcalab.org61,62): Helena Symonds, Paul Spong, and Steven Ness4\n(formely UVIC); logistical support: Lucy Aplin (cockatiels, Sulphur-crested cockatoos, monk parakeets); John\nMartin, Anastasia Dalziell, and Justin Welbergen (Sulphur-crested cockatoos); Tim Wilder, Susan Vos and Fort\nMcCoy Natural Resource Branch and Range Control (Blue- and Golden-winged warblers); access to field sites:\nWisconsin and Illinois Department of Natural Resources (Blue- and Golden-winged warblers): The Bikuben\nFoundation and Mols Bjerge National Park (Pygmy pipistrelles); co-hosting the Heidelberg Academy of Sciences\nworkshop: Jens Koblitz and Nora Carlson; design of the ANIMAL-SPOT network architecture image (part of\nFig. 2): Michael Weber. We thank Joeri Zwerts for granting access to the ComParE-PRS primates dataset. All\nauthors complied with the legislation in the respective countries were fieldwork was conducted. Ethical approval \n16\nVol:.(1234567890)\nScientific Reports | (2022) 12:21966 | https://doi.org/10.1038/s41598-022-26429-y\nwww.nature.com/scientificreports/\n(where necessary) was obtained as follows: Data from cockatiels was collected by Lucy Aplin and Gustavo\nAlarcón-Nieto under ethical permission from the Regierungspräsidium Freiburg. Az. 35-9185.81/G-18/009; data\nfrom Sulphur-crested cockatoos was collected by Barbara Klump and John Martin under ethical permission from\nthe Ethics Council of the Max Planck Society (application no. 2018_12; permit given to Lucy Aplin); data from\nBlue- and Golden-winged warblers was collected by Stephen Tyndel under ethical permission from the University\nof Illinois at Urbana-Champaign’s IACUC committee (protocol #16022); data from chimpanzees was collected\nby Ammie Kalan under ethical permission from the Ministère de la Recherche Scientifique, the Ministère de\nl’Environnement et des Eaux et Forêts, and the Office Ivorien des Parcs et Reserves in Côte d’Ivoire (Ref: 11/\nMINEF/OIPR/DT/CAT). Research was conducted with funding from: German Research Council (DFG; grant\nMA-4898/18-1 to CB); Paul G. Allen Frontier’s Group (CB); Max Planck Society (AKK; Mary Brooke McElreath\nfunded SQS); Max Planck Society Independent Group Leader Fellowship to Lucy Aplin (funded SQS, SAT, BCK);\nInternational Max Planck Research School (IMPRS) for Organismal Biology (SQS, SAT, STO); German Academic Exchange Service (DAAD PhD scholarship to SAT); United States Department of Defense, Environmental\nSecurity Technology Certification Program (ESTCP grant #RC 201615 to Jinelle Sperry, Michael Ward and Brett\nDegregorio, U.S. Army Corps of Engineers, funded SAT); Animal Minds Project e.V. (STO); Carlsberg Foundation Semper Ardens grant to Peter Teglberg Madsen (funded SB); Danish Environmental Protection Agency\n(ANO, JT); Dansk Akustisk Selskab (FJ); University of Southern Denmark (Research grant to FJ); Office of Naval\nResearch (MW); SDU Lighthouse Project (MW); Heidelberg Academy of Sciences (workshop grant to BCK).\nAuthor contributions\nC.B., B.C.K., S.Q.S., S.A.T., and E.N. conceived of, initiated and led the project; data was collected by B.C.K.,\nS.Q.S., S.A.T., A.K.K., S.B., S.T.O., F.J., J.T., M.W., A.N.O.; data was prepared (labeling, extraction and formatting) by: C.B., B.C.K., S.Q.S., S.A.T., A.N.O., F.J., A.K.K., R.X.C., S.T.O., S.B.; the ANIMAL-SPOT software was\ndeveloped by C.B.; data preparation procedures for all animal species were verified by C.B., A.B., S.Q.S., S.A.T.,\nB.C.K.; verification of the experiments were performed by C.B., A.B., S.Q.S., S.A.T.; the manuscript was written by: C.B., B.C.K., S.Q.S., S.A.T., and A.B.; with support and feedback from A.M. and E.N., and edited by all\nauthors. E.N., B.C.K. and A.M. supervised the project; B.C.K. secured funding for the workshop during which\nthis project was conceived.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https://doi.org/\n10.1038/s41598-022-26429-y.\nCorrespondence and requests for materials should be addressed to C.B. or B.C.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International\nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not\npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2022", "affiliations": [{"university": "University of Erlangen-Nuremberg", "country": "Germany", "discipline": "Computer Science"}, {"university": "Max Planck Institute", "country": "Germany", "discipline": "Ecology"}, {"university": "University of Konstanz", "country": "Germany", "discipline": "Biology"}, {"university": "University of Illinois Urbana-Champaign", "country": "United States", "discipline": "Environmental Science"}, {"university": "Max Planck Institute", "country": "Germany", "discipline": "Biology"}, {"university": "University of Victoria", "country": "Canada", "discipline": "Anthropology "}, {"university": "Leibniz Institute for Zoo and Wildlife Research", "country": "Germany", "discipline": "Zoology"}, {"university": "Aarhus University", "country": "Denmark", "discipline": "Biology"}, {"university": "University of Gdańsk", "country": "Poland", "discipline": "Ecology"}, {"university": "University of Southern Denmark", "country": "Denmark", "discipline": "Biology"}], "species_categories": ["Bird", "Fish", "Marine Mammal", "Terrestrial Mammal"], "specialized_species": ["cockatiel", "sulphur-crested cockatoo", "peach-fronted conure", "monk parakeet", "blue-winged warbler", "golden-winged warbler", "chinstrap penguin", "atlantic cod", "harbour seal", "killer whale", "pygmy pipistrelle", "chimpanzee"], "computational_stages": ["Data Collection", "Pre-processing", "Sequence Representation", "Meaning Identification", "Generation"], "linguistic_features": ["Vocal Auditory Channel and Turn-taking", "Broadcast and Direct Reception", "Semanticity", "Specialization", "Discreteness and Syntax"], "status": "saved", "created_at": "2026-01-13T12:49:59.887461", "updated_at": "2026-01-13T14:15:27.104125", "committed_at": "2026-01-13T14:15:30.983063"}
