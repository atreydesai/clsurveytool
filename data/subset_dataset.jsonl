{"id": "83d4c932-0dfd-4c09-9025-0d809ac41604", "doi": "https://doi.org/10.1101/2022.06.14.496047", "title": "The application of neural networks to classify dolphin echolocation clicks", "authors": ["Vahid Seydi", "Lucille Chapuis", "Gemma Veneruso", "Sudha Balaguru", "Noel Bristow", "David Mills", "Lewis Le Vay"], "year": "2022", "journal": "", "abstract": "Abstract Passive acoustic monitoring (PAM) is a common approach to monitor marine mammal populations, for species of dolphins, porpoises and whales that use sound for navigation, feeding and communication. PAM produces large datasets which benefit from the application of machine learning algorithms to automatically detect and classify the vocalisations of these animals. We present a deep learning approach for the classification of dolphins’ echolocation clicks into two species groups in an environment with high background noise. We compare the use of Convolutional Neural Networks (CNN) and Recurrent Neural Network (RNN), in which we feed the models the raw waveform data and spectrograms. We show that both models perform well, with the highest performance achieved by a CNN fed with spectrograms (F1 score 97 %) and an RNN fed with raw data (F1 score 96%) fitted with Gated Recurrent Units (GRU). We recommend the use of such models to classify echolocation clicks in marine environments where background noise levels exhibit high spatial and temporal variance. In particular, the RNN showed excellent performance, while being fed with raw data, in terms of reduced processing time and storage. Deep learning automatically extracts effective features from the raw waveform in the training process through multiple layers of the model, without the need to rely on feature extraction in a separate pre-processing step.", "pdf_filename": "https_doi.org_10.1101_2022.06.14.496047.pdf", "data_source": "subset", "status": "saved", "created_at": "2026-01-27T20:15:21.709738", "analysis_notes": "bioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nTitle: The application of neural networks to classify dolphin echolocation clicks\nAuthors: Vahid Seydi1*, Lucille Chapuis1, Gemma Veneruso1, Sudha Balaguru1, Noel\nBristow1, David Mills1, Lewis Le Vay1.\n1: Centre for Applied Marine Sciences, School of Ocean Sciences, Bangor University, Menai\nBridge, United Kingdom\n*: corresponding author: v.seydi@bangor.ac.uk\nAbstract:\nPassive acoustic monitoring (PAM) is a common approach to monitor marine mammal\npopulations, for species of dolphins, porpoises and whales that use sound for navigation,\nfeeding and communication. PAM produces large datasets which benefit from the application\nof machine learning algorithms to automatically detect and classify the vocalisations of these\nanimals. We present a deep learning approach for the classification of dolphins’ echolocation\nclicks into two species groups in an environment with high background noise. We compare\nthe use of Convolutional Neural Networks (CNN) and Recurrent Neural Network (RNN), in\nwhich we feed the models the raw waveform data and spectrograms. We show that both\nmodels perform well, with the highest performance achieved by a CNN fed with spectrograms\n(F1 score 97 %) and an RNN fed with raw data (F1 score 96%) fitted with Gated Recurrent\nUnits (GRU). We recommend the use of such models to classify echolocation clicks in marine\nenvironments where background noise levels exhibit high spatial and temporal variance. In\nparticular, the RNN showed excellent performance, while being fed with raw data, in terms of\nreduced processing time and storage. Deep learning automatically extracts effective features\nfrom the raw waveform in the training process through multiple layers of the model, without\nthe need to rely on feature extraction in a separate pre-processing step.\n1. INTRODUCTION\nPassive acoustic monitoring (PAM) is a popular method used for monitoring odontocetes\n(toothed whales) in their ecosystems (Mellinger et al., 2007). Odontocetes produce series of\nclicks, whistles, and other complex sounds to survey their surroundings, hunt for food, and\ncommunicate with each other. Recording these signals with hydrophones allows the\nexamination of species occurrence, distribution, density, behaviour, and the consequences of\ndisturbance at specific spatial and temporal scales (Booth et al., 2020; Mellinger et al., 2007).\nSuch information is critical to monitor and understand populations of odontocetes, which are\notherwise difficult to monitor, and for which some species are declining and/or under\nthreatened conservation status (Nelms et al., 2021).\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nIn particular, delphinids (dolphins) produce clicks with their biosonar system and utilise their\nreturning echoes to find and identify prey and discern environmental structure. Clicks are\nimpulsive short pulses of substantial strength, lasting for microseconds. Delphinids produce\nmultiple clicks in a row with diverse cue rates ranging from 0.5 to 2 clicks per second (Au,\n1993). Clicks from delphinids are important vocalisations, predominant in comparison with\nwhistles or other calls, and are used for navigation and prey detection (Au et al., 2000).\nTherefore, detecting and identifying delphinid clicks is an important aspect of PAM\napplications, for example when monitoring cetacean interactions with offshore infrastructure\ndevelopments. In particular, the development and future operation of marine renewable\nenergy (MRE) infrastructure raises the question of potential interactions of local populations\nof delphinids with individual turbines and turbine arrays, making the determination of the\npresence or absence of key species of critical importance if harm to cetaceans is to be avoided\n(see e.g., Gillespie et al., 2021; Palmer et al., 2021). Similarly, the development of delphinid\ndeterrents to avoid by-catch or interactions with anthropogenic structures requires accurate\nmonitoring procedures to assess their specific performance (Schakner and Blumstein, 2013).\nSpectral and temporal properties of delphinid echolocation clicks have been investigated to\nhelp classify these vocalisations to species (Roch et al., 2011; Soldevilla et al., 2008). Clicks\nare typically classified based on a finite number of features, such as their spectral content,\nduration and inter-click intervals. However, interspecific patterns are not always identified, and\nthe intraspecific variability of the clicks make any generalisation difficult. More recently,\nautomatic methods have been proposed to automatically classify clicks produced by whales\nand dolphins, with mixed results (Bermant et al., 2019; Frasier et al., 2017; Gillespie and\nChappell, 2002; Griffiths et al., 2020; Jarvis et al., 2008; Luo et al., 2017; Roch et al., 2008,\n2011).\nWith the advancement of data storage capabilities and requirement for long-term monitoring,\nPAM can generate terabytes of data, and manual classification of delphinid clicks is time-\nconsuming and may be inconsistent among analysts. For the last decade, machine learning\ntechniques have provided improved detection and classification methods for PAM of\ncetaceans, with varying performance outputs (Usman et al., 2020). The cetacean\nvocalisations are usually pre-processed via the extraction of features, followed by the\ndetection and classification stages. Furthermore, these stages are commonly processed\nretrospective to the collection of the recordings, as they do not allow real-time processing.\nDeep neural networks are the extension of shallow artificial neural networks that allow a\nmachine to be fed with raw data and to automatically discover the representations, or features,\nneeded for detection or classification (LeCun et al., 2015). Convolutional neural networks\n(CNN) and recurrent neural networks (RNN) are two popular structures in Deep learning\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nmodels. Although primarily used in visual recognition contexts, CNNs are becoming more and\nmore widespread for audio-related tasks (Piczak, 2015; Valenti et al., 2017), and have been\nused successfully to detect and classify marine mammal vocalisations (Allen et al., 2021;\nBermant et al., 2019; Ibrahim et al., 2021; Shiu et al., 2020; Zhong et al., 2020). In parallel,\nRNNs are excellent for the extraction of patterns in time series or signals (LeCun et al., 2015)\nand have recently been used to classify sperm whale clicks to recognise vocal clans and\nindividual whales (Bermant et al., 2019). Delphinid clicks have been previously classified\nsuccessfully by unsupervised neural networks (Frasier, 2021; Frasier et al., 2016, 2017),\nsuggesting the ability of these networks to recognise different patterns and/or features in\necholocation clicks. However, the classes could not all be identified as a taxonomic group (ie.\nspecies), and the complex workflow included many succeeding steps of clustering, labelling\nand classification (Frasier, 2021).\nGenerally, detection and classification tasks are affected by the amount of background noise\npresent at the recording sites, and the varying signal-to-noise ratio when considering marine\nmammal vocalisations. For example, tidal sites are affected by periods of high current speeds,\nwhich significantly increase the background noise, sometimes with a difference of more than\n30 dB re 1 µPa between the tidal phases (Willis et al., 2013). Similarly, underwater structures\nand machinery, like tidal turbines, offshore windfarms or other MRE installations, and other\nanthropogenic activities can also alter background noise in regular or irregular patterns. PAM\ndetection and classification of animals’ signals in such environments may suffer from a low\nsignal-to-noise ratio and in the case of machine learning models, a low generalisation\nperformance.\nIn this study, we investigate the use of CNN and RNN to efficiently extract the informative\nfeatures from delphinid clicks, recorded at a strong tidal flow site with an MRE infrastructure\nin place, for classification and similarity analysis. We test both methods by feeding the raw\ndata (click waveforms) and spectrograms (image representation of the clicks) to the networks.\nClicks from three species of dolphins common to the region were collected, labelled and used\nto train four different models: (i) CNN with raw data; (ii) CNN with spectrograms; (iii) RNN with\nraw data; (iv) RNN with spectrograms. The models were used to automatically classify the\nclicks into two groups: Risso’s dolphin (Grampus griseus) clicks, and delphinid clicks from\nspecies that produce broadband clicks, combining clicks from common (Delphinus delphis)\nand bottlenose dolphins (Tursiops truncatus). The combination of click data for these two\nspecies (bottlenose and common dolphins) is typical for traditional PAM studies, as they are\nnot easily distinguished by human operators due to the similarities in click properties (Au,\n2002), hence the training of supervised machine learning models is not currently possible.\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\n2. METHODS\n2.1. Description of PAM system and deployment\nThe PAM system was deployed in Wales, UK, in Holyhead Deep (53° 17.796 ‘ N, 4° 47.948’\nW), at 85 m depth for one month in total (August – September 2019). The site included an\nMRE infrastructure which typically produced anthropogenic noise, notably due to the\nmovement of metallic components, such as mooring gear including chains. A Sonar Point\nrecorder (Desert Star Systems, USA) was connected to four HTI 99-UHF hydrophones (HTI,\nHigh Tech Inc, USA), recording at 312,500 samples per second. Only one data channel (i.e.\none hydrophone) was used for analyses in this study. The change in background noise was\ninvestigated by calculating the RMS (root-mean-squared) sound level for each hour of\nrecordings and is presented in supplementary material Fig. S1.\n2.2. Click detection and data labelling\nPAMGuard (Gillespie et al., 2009; www.pamguard.org) was used to process the acoustic data\nand detect all clicks. The click detector was configured to be triggered by transient signals with\npeak frequencies above 20 kHz that rose 10 dB above a continuous measure of background\nnoise. When triggered, the detector stores short clips (~ 1 ms) of unfiltered data. An\nexperienced analyst (GV) then manually classified 16,520 clicks into Risso’s dolphin clicks\n(10,367) and broadband click species events (6,153), based on the click frequency\ncharacteristics (peak frequency and bandwidth) and waveforms (Palmer et al., 2017;\nSoldevilla et al., 2008).\n2.3. Feature extraction\nTwo approaches were considered to prepare the data for the deep learning models. In the first\napproach, we used the ability of neural networks to learn inherent rules from the input raw\ndataset: the model automatically extracts the appropriate features required to classify the data.\nIn the second approach, the click frequencies are extracted using a short-time Fourier\ntransform (STFT) and given to the model in the form of a spectrogram. In this approach, the\nSTFT window size and the distance between adjacent windows are considered as\nhyperparameters. In the hyperparameter tuning process, varying values were examined to\nfind near-optimal values for both.\n2.4 Machine learning classifier\nWe tested two popular classification models in the deep learning domain, CNN and RNN, to\nclassify acoustic signals (Usman et al., 2020). The main characteristic of CNN models is\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nautomatic feature extraction by applying convolution filters based on local correlations. On the\nother hand, RNN models are specialised for processing a sequence of values and use outputs\nfrom past or future inputs of the temporal sequence to inform the current prediction\n(Goodfellow et al., 2016). However, standard RNN cannot work efficiently with long-term\ndependencies in data where the distance between the relevant information and the place\nwhere it is required is large. Therefore, a Long Short-Term Memory (LSTM) network, a special\ncase of RNN that takes long-term dependencies into account, was tested, as well as a Gated\nRecurrent Unit (GRU) network, which represents a moderate version of recurrent gate, and\nhas been used previously for human speech recognition (Shewalkar et al., 2019).\nAs with most supervised learning tasks, the process comprises three phases: (1) the training\nphase in which the weights or parameters of the model are trained on labelled data, (2) the\nvalidation phase in which the hyper parameters are tuned, and (3) the test phase where the\nmodel is evaluated as a classifier for unseen labelled data. In the test and validation phases,\nthe model parameters do not change, and the model is evaluated based on their achieved\nvalues. Therefore, these two phases are also referred to as the evaluation phase.\n2.5 Convolution Neural Networks architecture\nTwo models of CNN were used, depending on the input type: we used one-dimensional\nconvolution filters on the raw data and two-dimensional convolution filters on the spectrogram\ndataset. As shown in Figure 1, a CNN consists of the following blocks:\na. The Input layer, is the raw signal waveform for one-dimensional convolutional networks\nand is the spectrogram for two-dimensional convolutional networks. The dimensions of the\nspectrograms are determined by the size of the STFT window and the distance between\nadjacent windows.\nb. The Convolution layer consists of several convolution filters (1D or 2D). They represent\nweights or model parameters, whose optimal values are attained in the training process. The\ndimension and stride of the filters are considered as hyperparameters of the model and their\noptimal value is obtained in the validation process. Convolution filters play the key role in the\nfeature extraction process.\nc. The Batch Normalisation layer keeps the mean output close to 0 and the output standard\ndeviation close to 1.\nd. The Activation function is a nonlinear function that takes the output of the batch\nnormalisation layer as input, aiming to capture the nonlinearity of the samples. ReLU and\nLeaky-ReLU are two popular activation functions which are traditionally used in most CNN\nmodels.\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\ne. The Pooling layer has the role of extracting the most important features from the extracted\nfeatures of the convolution layer. In this layer, a down-sampling mechanism is performed by\naveraging or maximising. The size of filter and stride are considered as hyperparameters.\nf. The Fully Connected layers apply a weighted linear mapping so that the features move\ncloser to the target space.\ng. The SoftMax or Logistic layer is the last layer of CNN and appends at the end of the fully\nconnected layer. The Logistic class and the SoftMax class are chosen for binary classification\nand multi-class classification, respectively. Since our problem is binary classification, Logistic\nis used in the proposed CNN model.\nAll the hyperparameters used in both CNN models are summarised in Table 1.\nFigure 1: Architecture of the Convolutional Neural Network: a) general view of the CNN model\nb) enlarged view of a one-dimensional convolution neuron c) enlarged view of a two-\ndimensional convolution neuron d) enlarged view of a fully connected neuron.\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\n2.5 Recurrent Neural Networks architecture\nThe general structure of an RNN can be seen in Figure 2. A sample is divided into a sequence\nof consecutive states. The states are given to the first layer of the model, which involves\nseveral RNN/GRU/LSTM gates (neurons), one after the other. The standard RNN gate is a\nbasic neuron which is fed with two inputs: one directly from the input and the other one comes\nfrom the output of its previous state. Depending on how long history can be captured, two\ngeneral extensions have been utilised to the structure of the RNN neuron: the GRU gate and\nthe more complex LSTM gate. To add more flexibility to the model several recurrent layers\ncan be stacked. A Fully Connected layer and a Logistic layer reside at the end of recurrent\nlayers to transform extracted features to the target label. In this way, the history of the input\nsequence is considered to extract features.\nFigure 2: Architecture of the Recurrent Neural Network: a) General view of the RNN b)\nenlarged view of a recurrent neuron c) enlarged view of an unrolled recurrent neuron.\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nIn this structure, the number of gates in each layer, the number of recurrent layers, and the\ntype of gates are considered as hyperparameters. Also, since we used two different forms of\ninput, we used two distinct methods to transform a sample into a sequence of states, as\nfollows:\n- In the case that the inputs are given to the model in the form of a raw wave with N\nsample points, n (n ≤ N) consecutive samples are considered as a state, n referring to\nthe dimension of input. Therefore, each sample contains N/n states (the length of\nsequence). Since the maximum click length is 512, the value of N is 512 and n was\nconsidered as a hyperparameter so that the near-optimal value could be achieved.\n- In the case of spectrograms as inputs, each column of the matrix, which is the\nmagnitude of frequency coefficient of the click signal in a window of STFT, is\nconsidered as a state. The length of the input vector and the length of the sequence\ndepend on both the size of the window and the distance between two adjacent\nwindows in STFT. These values were obtained in the hyperparameter tuning phase\nand are listed in the hyperparameters of the model.\nAll used hyperparameters of both RNN models are summarised in Table 1.\nTable 1: Description of the four models tested and their hyperparameters.\nModel Category Parameters Values\n1. CNN, Number of CNN\n4\nraw signals blocks\nNumber of channels [16, 32, 64, 48]\nConvolution filter\n[5, 5, 9, 5]\nsize\nConvolution Blocks\nConvolution filter\n[1, 3, 1, 1]\nstride\nActivation function ReLU\nMax-pool filter size* [2, 3, 1, 2]\nMax-pool stride 1\nnumber of fully\n4\nLinear blocks connected layers\nNumber of neurons [512, 512, 256, 64]\nDrop out probability 0.4\nOthers\nLearning rate 1e-4\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\n2. CNN, n-fft 128\nSpectrogram\nspectrograms Hop size 32\nNumber of blocks 5\nNumber of channels [48, 128, 64, 16, 32]\nConvolution filter\n[3, 3, 3, 3, 3]\nsize\nConvolution Blocks Convolution filter\n1\nstride\nActivation function ReLU\nMax-pool filter size* [1, 1, 1, 2, 1]\nMax-pool stride 1\nnumber of layers 4\nLinear blocks\nNumber of neurons [1024, 64, 64, 64]\nDrop out probability 0.2\nOthers\nLearning rate 1e-4\n3. RNN, Number of layers 3\nraw signals Number of neurons 60\nNeural network\nType of gate GRU\nLearning rate 0.000723\n4. RNN, n-FFT 16\nSpectrogram\nspectrograms Hope size 8\nNumber of layers 4\nNumber of neurons 40\nNeural network\nType of gate LSTM\nLearning rate 0.000975\n* for max-pool filter size, number 1 means not using max-pool filter in that block\n2.6 Data processing and performance metrics\nAs previously mentioned, the extracted dataset consisted of 16,520 samples, of which 10,367\nbelong to Risso’s dolphin and 6,153 belong to the broadband species. Each sample consists\nof the click's waveform and its label. All waveforms were normalised and zero-padded to a\nlength of 512 points.\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nThe dataset was divided into six folds, one of which was separated for hyperparameter tuning.\nThe other five folds were used in the form of a five-fold-cross-validation structure to train and\nevaluate the models. The models were trained five times independently over about 11,000\nsamples and evaluated on about 2,750 distinct samples.\nTo prevent overfitting, the early stopping condition was applied to the training loops using 15%\nof the training data as a validation set. Training was stopped if the validation loss increased\ncontinuously for 30 epochs and according to the validation loss, the best model was returned.\nIn the training phase, we balanced the dataset by oversampling the broadband species. We\nused the binary classification loss function and Adam optimisation algorithm. The training\ndataset was divided into batches containing 16 samples. The Risso’s dolphin class was\nconsidered the positive class, and we calculated the following performance metrics:\n- Accuracy, is a metric that generally describes how the model performed across all\nclasses. It was calculated as the ratio between the number of correct Risso’s dolphin\npredictions to the total number of click predictions.\n- Precision, is a metric that measures the model’s accuracy in classifying a click as\nRisso’s. It was calculated as the ratio between the number of Risso’s clicks correctly\nclassified to the total number of clicks classified as Risso’s (either correctly or\nincorrectly).\n- Recall, is a metric that measures the model’s ability to detect Risso’s dolphin clicks. It\nis calculated as the ratio between the number of correctly classified Risso’s clicks to\nthe total number of Risso’s clicks.\n- F1- score is the harmonic mean between precision and recall.\n- Binary Cross-Entropy Loss (or Log Loss) is another metric to assess the\nperformance of a classification problem, and which measures the difference between\nthe two probability distributions.\n3. RESULTS\nTable 2 shows the average performance metrics (accuracy, precision, recall, F1 score, loss)\nfor the four models tested. The individual training and validation loss curves are presented for\neach model and each fold in Figure S2 (supplementary material). These learning curves\ndemonstrate the performance of the neural network over the iteration while training. The CNN\nall converged quickly, showing signs of overfitting (Fig. S2A & B). The inflection points on the\ngraph (‘best epoch’) show the point at which training could be halted to avoid overfitting.\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nSimilarly, the RNN also showed signs of overfitting, although it took more epochs to achieve\ntraining in the case in which raw data was used as input (Fig. S2C & D).\nThe average performance metrics and the number of parameters used for each model are\npresented in Table 2. The individual performance metrics for each model and each fold are\nillustrated in Figure S3. All models showed high performance, with almost all metrics above\n90 %. The CNN fed with the spectrograms and the RNN fed with raw data both showed a\nprecision, accuracy, recall and F1 score of more than 95 %. This indicates that these models\nachieved many correct positive classifications while minimising false positives. A good\nbalance between precision and recall is indicated by high F1 scores (97 % for both models).\nWe used binary cross-entropy (log loss) as our loss function. As the classes were unbalanced\n(prevalence 63%) the log loss metrics for both models also showed good performance (0.48\nand 0.49, respectively).\nTable 2: Performance metrics (accuracy, precision, recall, F1 score) statistics (mean,\nminimum, maximum and standard deviation) and number of parameters for each model\nmetric 1) CNN-raw 2) CNN- 3) RNN-raw 4) RNN-\nSpectrogram Spectrogram\nAccuracy max 0.9241 0.9669 0.9571 0.8747\nmean 0.9091 0.9567 0.9473 0.8466\nmin 0.8965 0.9452 0.9324 0.8083\nstd 0.010043 0.007827 0.010317 0.028896\nPrecision max 0.9468 0.975 0.9626 0.9027\nmean 0.9347 0.9707 0.9548 0.8735\nmin 0.9054 0.9617 0.9452 0.8297\nstd 0.016755 0.005617 0.006885 0.030346\nRecall max 0.9473 0.974 0.9803 0.9074\nmean 0.9199 0.9599 0.9615 0.8842\nmin 0.8889 0.9369 0.9473 0.8472\nstd 0.023345 0.014887 0.013296 0.024051\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nF1 Score max 0.9394 0.9736 0.9663 0.8998\nmean 0.9269 0.9652 0.9581 0.8786\nmin 0.9151 0.9554 0.9462 0.8512\nstd 0.008621 0.006602 0.008335 0.022133\nLoss max 0.5247 0.4849 0.4954 0.5692\nmean 0.5159 0.4790 0.4852 0.5442\nmin 0.5090 0.4730 0.4802 0.5257\nstd 0.005615 0.004534 0.006533 0.018491\nNumber of\nparameters 4,126,833 10,835,985 18,541 47,561\n4. DISCUSSION\nWe demonstrate here the effective application of deep neural networks for classifying\ndelphinid clicks between Risso’s dolphins and two broadband species (common or bottlenose\ndolphins). CNN and RNN models were trained on manually labelled clicks and generalised\nwell to all clicks recorded in the dataset derived from an environment with high tidal current\nspeeds and an MRE infrastructure in place and thus varying background noise. Both CNN and\nRNN automatically learned the representations needed from the input raw data and the\nspectrograms, bypassing the pre-processing phase. Feature extractions were thus not\nrequired in the case where the raw data was fed to the models, and clicks could be classified\nbased on their raw waveforms by the CNN and the RNN with accuracy, precision and recall\nhigher than 90 %. The highest performance was achieved by CNN fed with spectrograms (F1\nscore 97 %) and the RNN fed with raw data (F1 score 96 %).\nWhile CNN with spectrograms give a slightly better performance result, we prefer and\nrecommend the RNN model with raw data, taking into account the lower number of parameters\nthat is preferable in terms of operability and processing effort. The performance of the RNN\nconfirms the ability of recurrent models to process and understand consecutive data.\nFurthermore, the choice of RNN with raw data avoids any data pre-processing procedures,\nmaking the workflow simpler than when using spectrograms.\nWhile artificial neural networks have already been successfully used to classify odontocete\nclicks between sperm whales and long-finned pilot whales (Jiang et al., 2018), this is the first\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\napplication of supervised neural networks to classify delphinid clicks into taxonomic groups.\nDelphinid clicks are particularly challenging to classify due to their high variability, notably due\nto variable source levels and beam width, and rapid attenuation of the clicks (Au and Benoit-\nBird, 2003; Finneran et al., 2016; Kloepper et al., 2012; Moore et al., 2008). Only non-\nsupervised networks have so far been tested successfully to classify delphinid clicks (Frasier,\n2021; Frasier et al., 2017). Although this methodology was successful in separating group\nclicks into different classes, only one taxonomic group of delphinid was clearly identifiable\n(Risso’s dolphin) in the latest study (Frasier, 2021). Here, we show that supervised networks\nallow click classification from taxonomically different species.\nTraditionally, acoustic recordings are taken in conditions where the background noise is limited\nor can be filtered out. However, noise is sometimes inevitable and may exhibit elevated\nvariability, for example in a region prone to storms, near a submerged anthropogenic structure\nproducing noise, or as in this study, in strongly tidal sites where current speed fluctuates\nconsiderably and includes an inherently noisy MRE infrastructure. Our results show that neural\nnetworks are resilient to varying levels of background noise and offer good generalisation\nperformance. Therefore, such an automatic classifier could be used in coastal regions with\nstrong tidal currents as well as sites with MRE infrastructures presenting varying levels of\nbackground noise (see e.g., Benjamins et al., 2017; Gillespie et al., 2021; Palmer et al., 2021).\nThis study focused on the classification of two groups of delphinid species in one region, using\none month of PAM data. The classifier could be generalised by including clicks detected by\nthe automatic detector produced by additional background noise sources, such as snapping\nshrimps or other organisms and anthropogenic clicks derived from maritime transport sources\nor noise derived from other types of undersea infrastructure. Also, clicks from a certain species\ncan sometimes present different features and can be subclassified interspecifically, as done\nfor the sperm whale Physeter macrocephalus with artificial networks (Schaar et al., 2009,\n2007). Ecological insights may thus be gained from being able to distinguish click patterns\nwithin a species with non-supervised neural networks fed with raw data. Furthermore, as clicks\nare usually emitted in groups, or ‘click trains’ (Au, 1993, 2002), they could be fed in this form\nto the machine, rather than individually, so that the temporal and spectral features of the click\ntrains may be considered, which may improve the machine learning model performance. More\ngenerally, future research should explore the use of such models to classify echolocation\nclicks from other species, recorded in different geographic regions and over different seasons.\nDeep neural networks have previously been shown to generalise well to recordings taken at\ndifferent spatial and temporal scales, even if not represented in the training data (Shiu et al.,\n2020).\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nAdvances in automatic sound classification allow rapid processing of large volumes of data.\nThe classification of marine mammal clicks is especially valuable for research to improve\nunderstanding and management of marine ecosystems. With the increase in anthropogenic\nnoise in the ocean (Duarte et al., 2021) and the development of underwater energy sources,\nit is especially important to monitor the presence of these animals despite the presence of\nbackground noise. Furthermore, although PAM data is commonly analysed retrospectively,\nthere are emerging policy requirements to develop real-time in situ monitoring and\nclassification of marine mammals to estimate collision risk as new tidal stream MRE\ninfrastructures are developed and operated. Our work demonstrates that state-of-the-art\nneural networks can successfully achieve classification tasks under these conditions.\nDATA AVAILABILITY STATEMENT\nThe trained machine learning model is openly available at XXXXXXXXX. Further inquiries can\nbe directed to the corresponding author.\nACKNOWLEDGMENTS\nThis work was funded by the SEACAMS2 and SEEC (Smart Efficient Energy Centre) projects,\nboth part-funded by the European Regional Development Fund through the Welsh\nGovernment.\nREFERENCES\nAllen, A. N., Harvey, M., Harrell, L., Jansen, A., Merkens, K. P., Wall, C. C., Cattiau, J., et al. (2021). “A\nConvolutional Neural Network for Automated Detection of Humpback Whale Song in a Diverse, Long-\nTerm Passive Acoustic Dataset,” Frontiers Mar Sci, 08, 607321. doi:10.3389/fmars.2021.607321\nAu, W., Popper, A., and Fay, R. (2000). “Hearing by Whales and Dolphins,” Spr Hdb Aud, , doi: 10.1007/978-\n1-4612-1150-1. doi:10.1007/978-1-4612-1150-1\nAu, W. W. L. (1993). “The Sonar of Dolphins,” , doi: 10.1007/978-1-4612-4356-4_10. doi:10.1007/978-1-\n4612-4356-4_10\nAu, W. W. L. (2002). “Echolocation,” In W. F. Perrin, B. Wursig, and J. G. M. Thewissen (Eds.), Encyclopedia\nof Marine Mammals, Academic Press, San Diego, USA, pp. 358–367.\nAu, W. W. L., and Benoit-Bird, K. J. (2003). “Automatic gain control in the echolocation system of dolphins,”\nNature, 423, 861–863. doi:10.1038/nature01727\nBenjamins, S., Geel, N. van, Hastie, G., Elliott, J., and Wilson, B. (2017). “Harbour porpoise distribution can\nvary at small spatiotemporal scales in energetic habitats,” Deep Sea Res Part Ii Top Stud Oceanogr, 141,\n191–202. doi:10.1016/j.dsr2.2016.07.002\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nBermant, P. C., Bronstein, M. M., Wood, R. J., Gero, S., and Gruber, D. F. (2019). “Deep Machine Learning\nTechniques for the Detection and Classification of Sperm Whale Bioacoustics,” Sci Rep-uk, 9, 12588.\ndoi:10.1038/s41598-019-48909-4\nBooth, C. G., Sinclair, R. R., and Harwood, J. (2020). “Methods for Monitoring for the Population\nConsequences of Disturbance in Marine Mammals: A Review,” Frontiers Mar Sci, 7, 115.\ndoi:10.3389/fmars.2020.00115\nDuarte, C. M., Chapuis, L., Collin, S. P., Costa, D. P., Devassy, R. P., Eguiluz, V. M., Erbe, C., et al. (2021).\n“The soundscape of the Anthropocene ocean,” Science, 371, eaba4658. doi:10.1126/science.aba4658\nFinneran, J. J., Mulsow, J., Branstetter, B., Moore, P., and Houser, D. S. (2016). “Nearfield and farfield\nmeasurements of dolphin echolocation beam patterns: No evidence of focusing,” J Acoust Soc Am, 140,\n1346–1360. doi:10.1121/1.4961015\nFrasier, K. E. (2021). “A machine learning pipeline for classification of cetacean echolocation clicks in large\nunderwater acoustic datasets,” Plos Comput Biol, 17, e1009613. doi:10.1371/journal.pcbi.1009613\nFrasier, K. E., Henderson, E. E., Bassett, H. R., and Roch, M. A. (2016). “Automated identification and\nclustering of subunits within delphinid vocalizations,” Mar Mammal Sci, 32, 911–930.\ndoi:10.1111/mms.12303\nFrasier, K. E., Roch, M. A., Soldevilla, M. S., Wiggins, S. M., Garrison, L. P., and Hildebrand, J. A. (2017).\n“Automated classification of dolphin echolocation click types from the Gulf of Mexico,” Plos Comput Biol,\n13, e1005823. doi:10.1371/journal.pcbi.1005823\nGillespie, D., and Chappell, O. (2002). “An automatic system for detecting and classifying the vocalisations of\nharbour porpoises,” Bioacoustics, 13, 37–61. doi:10.1080/09524622.2002.9753485\nGillespie, D., Mellinger, D. K., Gordon, J., McLaren, D., Redmond, P., McHugh, R., Trinder, P., et al. (2009).\n“PAMGUARD: Semiautomated, open source software for real‐time acoustic detection and localization of\ncetaceans.,” J Acoust Soc Am, 125, 2547–2547. doi:10.1121/1.4808713\nGillespie, D., Palmer, L., Macaulay, J., Sparling, C., and Hastie, G. (2021). “Harbour porpoises exhibit localized\nevasion of a tidal turbine,” Aquat Conservation Mar Freshw Ecosyst, , doi: 10.1002/aqc.3660.\ndoi:10.1002/aqc.3660\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning, Adaptive computation and machine\nlearning series, The MIT Press, Cambridge, Massachusetts.\nGriffiths, E. T., Archer, F., Rankin, S., Keating, J. L., Keen, E., Barlow, J., and Moore, J. E. (2020). “Detection\nand classification of narrow-band high frequency echolocation clicks from drifting recorders,” J Acoust Soc\nAm, 147, 3511–3522. doi:10.1121/10.0001229\nIbrahim, A. K., Zhuang, H., Chérubin, L. M., Erdol, N., O’Corry-Crowe, G., and Ali, A. M. (2021). “A\nmultimodel deep learning algorithm to detect North Atlantic right whale up-calls,” J Acoust Soc Am, 150,\n1264–1272. doi:10.1121/10.0005898\nJarvis, S., Dimarzio, N. A., Morrissey, R. P., Ward, J., and Moretti, D. (2008). “Automated classification of\nodontocetes in open ocean environments using a novel multiclass suport vector machine,” J Acoust Soc Am,\n123, 2984–2984. doi:10.1121/1.2932507\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nJiang, J., Bu, L., Wang, X., Li, C., Sun, Z., Yan, H., Hua, B., et al. (2018). “Clicks classification of sperm whale\nand long-finned pilot whale based on continuous wavelet transform and artificial neural network,” Appl\nAcoust, 141, 26–34. doi:10.1016/j.apacoust.2018.06.014\nKloepper, L. N., Nachtigall, P. E., Donahue, M. J., and Breese, M. (2012). “Active echolocation beam focusing\nin the false killer whale, Pseudorca crassidens,” J Exp Biol, 215, 1306–1312. doi:10.1242/jeb.066605\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). “Deep learning,” Nature, 521, 436–444.\ndoi:10.1038/nature14539\nLuo, W., Yang, W., Song, Z., and Zhang, Y. (2017). “Automatic Species Recognition Using Echolocation\nClicks from Odontocetes,” 2017 Ieee Int Conf Signal Process Commun Comput Icspcc, , doi:\n10.1109/icspcc.2017.8242503. doi:10.1109/icspcc.2017.8242503\nMellinger, D., Stafford, K., Moore, S., Dziak, R., and Matsumoto, H. (2007). “An Overview of Fixed Passive\nAcoustic Observation Methods for Cetaceans,” Oceanography, 20, 36–45. doi:10.5670/oceanog.2007.03\nMoore, P. W., Dankiewicz, L. A., and Houser, D. S. (2008). “Beamwidth control and angular target detection in\nan echolocating bottlenose dolphin (Tursiops truncatus),” J Acoust Soc Am, 124, 3324–3332.\ndoi:10.1121/1.2980453\nNelms, S., Alfaro-Shigueto, J., Arnould, J., Avila, I., Nash, S. B., Campbell, E., Carter, M., et al. (2021).\n“Marine mammal conservation: over the horizon,” Endanger Species Res, 44, 291–325.\ndoi:10.3354/esr01115\nPalmer, K. J., Brookes, K., and Rendell, L. (2017). “Categorizing click trains to increase taxonomic precision in\necholocation click loggers,” J Acoust Soc Am, 142, 863–877. doi:10.1121/1.4996000\nPalmer, L., Gillespie, D., MacAulay, J. D. J., Sparling, C. E., Russell, D. J. F., and Hastie, G. D. (2021).\n“Harbour porpoise (Phocoena phocoena) presence is reduced during tidal turbine operation,” Aquat\nConservation Mar Freshw Ecosyst, , doi: 10.1002/aqc.3737. doi:10.1002/aqc.3737\nPiczak, K. J. (2015). “Environmental Sound Classification with Convolutional Neural Networks,” 2015 Ieee\n25th Int Work Mach Learn Signal Process Mlsp, , doi: 10.1109/mlsp.2015.7324337.\ndoi:10.1109/mlsp.2015.7324337\nRoch, M. A., Klinck, H., Baumann-Pickering, S., Mellinger, D. K., Qui, S., Soldevilla, M. S., and Hildebrand, J.\nA. (2011). “Classification of echolocation clicks from odontocetes in the Southern California Bight,” J\nAcoust Soc Am, 129, 467–475. doi:10.1121/1.3514383\nRoch, M. A., Soldevilla, M. S., Hoenigman, R., Wiggins, S. M., and Hildebrand, J. A. (2008). “Comparison of\nMachine Learning Techniques for the Classification of Echolocation Clicks from Three Species of\nOdontocetes,” Canadian Acoustics, 36, 41–47.\nSchaar, M. V. der, Delory, E., and André, M. (2009). “Classification of Sperm Whale Clicks (Physeter\nMacrocephalus) with Gaussian-Kernel-Based Networks,” Lect Notes Comput Sc, 2, 1232–1247.\ndoi:10.3390/a2031232\nSchaar, M. van der, Delory, E., Català, A., and André, M. (2007). “Neural network-based sperm whale click\nclassification,” J Mar Biol Assoc Uk, 87, 35–38. doi:10.1017/s0025315407054756\nSchakner, Z. A., and Blumstein, D. T. (2013). “Behavioral biology of marine mammal deterrents: A review and\nprospectus,” Biol Conserv, 167, 380–389. doi:10.1016/j.biocon.2013.08.024\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.06.14.496047; this version posted June 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is\nmade available under aCC-BY 4.0 International license.\nShewalkar, A., Nyavanandi, D., and Ludwig, S. A. (2019). “Performance Evaluation of Deep Neural Networks\nApplied to Speech Recognition: RNN, LSTM and GRU,” J Artif Intell Soft Comput Res, 9, 235–245.\ndoi:10.2478/jaiscr-2019-0006\nShiu, Y., Palmer, K. J., Roch, M. A., Fleishman, E., Liu, X., Nosal, E.-M., Helble, T., et al. (2020). “Deep\nneural networks for automated detection of marine mammal species,” Sci Rep-uk, 10, 607.\ndoi:10.1038/s41598-020-57549-y\nSoldevilla, M. S., Henderson, E. E., Campbell, G. S., Wiggins, S. M., Hildebrand, J. A., and Roch, M. A.\n(2008). “Classification of Risso’s and Pacific white-sided dolphins using spectral properties of echolocation\nclicks.,” J Acoust Soc Am, 124, 609–24. doi:10.1121/1.2932059\nUsman, A. M., Ogundile, O. O., and Versfeld, D. J. J. (2020). “Review of Automatic Detection and\nClassification Techniques for Cetacean Vocalization,” Ieee Access, 8, 105181–105206.\ndoi:10.1109/access.2020.3000477\nValenti, M., Squartini, S., Diment, A., Parascandolo, G., and Virtanen, T. (2017). “A Convolutional Neural\nNetwork Approach for Acoustic Scene Classification,” 2017 Int Jt Conf Neural Networks Ijcnn, , doi:\n10.1109/ijcnn.2017.7966035. doi:10.1109/ijcnn.2017.7966035\nWillis, M. R., Broudic, M., Haywood, C., Masters, I., and Thomas, S. (2013). “Measuring underwater\nbackground noise in high tidal flow environments,” Renew Energ, 49, 255–258.\ndoi:10.1016/j.renene.2012.01.020\nZhong, M., Castellote, M., Dodhia, R., Ferres, J. L., Keogh, M., and Brewer, A. (2020). “Beluga whale acoustic\nsignal classification using deep learning neural network models,” J Acoust Soc Am, 147, 1834–1841.\ndoi:10.1121/10.0000921", "affiliations": [{"university": "", "country": "", "discipline": ""}, {"university": "", "country": "", "discipline": ""}], "species_categories": ["Marine Mammal"], "specialized_species": ["Risso's dolphin", "common dolphin", "bottlenose dolphins"], "computational_stages": ["Data Collection", "Generation"], "linguistic_features": [], "updated_at": "2026-01-27T20:58:41.655589", "committed_at": "2026-01-27T20:58:44.889652"}
{"id": "94abccc4-bee8-4eac-9e27-114efd0fba6e", "doi": "https://doi.org/10.1038/s41598-019-48909-4", "title": "Deep Machine Learning Techniques for the Detection and Classification of Sperm Whale Bioacoustics", "authors": ["Peter C. Bermant", "Michael M. Bronstein", "Robert J. Wood", "Shane Gero", "David F. Gruber"], "year": "2019", "journal": "Scientific Reports", "abstract": "", "pdf_filename": "https_doi.org_10.1038_s41598_019_48909_4.pdf", "data_source": "subset", "status": "saved", "created_at": "2026-01-27T20:15:24.132328", "analysis_notes": "www.nature.com/scientificreports\nCorrected: Publisher Correction\nopen Deep Machine Learning techniques\nfor the Detection and Classification\nof Sperm Whale Bioacoustics\nReceived: 15 April 2019 peter c. Bermant1, Michael M. Bronstein1,2,7, Robert J. Wood 3,4, Shane Gero 5 &\nAccepted: 15 August 2019 David f. Gruber 1,6\nPublished online: 29 August 2019\nWe implemented Machine Learning (ML) techniques to advance the study of sperm whale (Physeter\nmacrocephalus) bioacoustics. this entailed employing convolutional neural networks (cnns) to\nconstruct an echolocation click detector designed to classify spectrograms generated from sperm\nwhale acoustic data according to the presence or absence of a click. The click detector achieved 99.5%\naccuracy in classifying 650 spectrograms. The successful application of CNNs to clicks reveals the\npotential of future studies to train CNN-based architectures to extract finer-scale details from cetacean\nspectrograms. Long short-term memory and gated recurrent unit recurrent neural networks were\ntrained to perform classification tasks, including (1) “coda type classification” where we obtained 97.5%\naccuracy in categorizing 23 coda types from a Dominica dataset containing 8,719 codas and 93.6%\naccuracy in categorizing 43 coda types from an Eastern Tropical Pacific (ETP) dataset with 16,995 codas;\n(2) “vocal clan classification” where we obtained 95.3% accuracy for two clan classes from Dominica\nand 93.1% for four ETP clan types; and (3) “individual whale identification” where we obtained 99.4%\naccuracy using two Dominica sperm whales. these results demonstrate the feasibility of applying ML to\nsperm whale bioacoustics and establish the validity of constructing neural networks to learn meaningful\nrepresentations of whale vocalizations.\nWhile human language epitomizes a peak in communicative complexity across biological taxa, cetaceans repre-\nsent an important taxon for testing hypotheses relating to the evolution and development of sophisticated com-\nmunication systems. Despite the absolute magnitude and highly-developed neuroanatomical structure of the\ncetacean brain, the extent to which cetaceans possess a grammatical-syntactical language remains uncertain1.\nRegardless, cetaceans have cognitive abilities2 and societies3 that are commensurate with those of human phy-\nlogenetic relatives4, but their ocean habitat provides a difference in ecology, which can be revealing from a com-\nparative perspective, especially with regards to evolutionary adaptations required for aquatic communication5.\nIn contrast to the relative ease of investigating the communicative capabilities of humans and other terrestrial\norganisms, working with non-captive animals in the ocean environment presents numerous technical and logis-\ntical difficulties6. Given these challenges, the development of improved computational techniques for analyzing\ncetacean sounds plays an important role in enabling researchers to address questions pertaining to cetacean\nvocal behavior. In particular, automated methods are increasingly employed to detect, classify, and identify ceta-\ncean sounds and to efficiently process acoustic data without human bias7, which has significantly expedited and\nadvanced the study of cetacean bioacoustics.\nModern analysis of human speech and language often takes advantage of Machine Learning (ML) techniques,\nwhich entail the design and development of self-learning algorithms that allow computers to evolve behaviors\nbased on empirical data, such as from sensors and databases. In general, ML problems can be divided into three\nmajor subcategories: supervised learning, unsupervised learning, and reinforcement learning. Unsupervised\n1Radcliffe Institute for Advanced Study, Harvard University, Cambridge, MA, USA. 2Department of Computing,\nImperial College, London, MA, UK. 3Wyss Institute for Biologically Inspired Engineering, Harvard University,\nCambridge, MA, USA. 4Harvard John A. Paulson School of Engineering and Applied Sciences, Harvard University,\nCambridge, MA, USA. 5Department of Zoophysiology, Institute for Bioscience, Aarhus University, C.F., Møllers\nAllé 3, Aarhus, 8000, Denmark. 6Department of Natural Sciences, Baruch College and The Graduate Center, PhD\nProgram in Biology, City University of New York, New York, NY, USA. 7Twitter, 20 Air Street, London W1B 5DL, United\nKingdom. Shane Gero and David F. Gruber contributed equally. Correspondence and requests for materials should be\naddressed to D.F.G. (email: David.Gruber@baruch.cuny.edu)\nScientific RepoRtS | (2019) 9:12588 | https://doi.org/10.1038/s41598-019-48909-4 1\n\nwww.nature.com/scientificreports/ www.nature.com/scientificreports\nlearning is concerned with uncovering structure within a dataset and extracting meaningful information, often-\ntimes without prior knowledge of how the data are organized8. Supervised learning relies on prior knowledge\nregarding an example dataset in order to make predictions about new, unseen data points (“generalization”). This\nis accomplished through the use of a training set comprised of labeled data, whose ground truth response values\nare known, in order to train the predictor. Since supervised learning typically requires abundant labeled data, var-\nious techniques can be employed to alleviate this requirement, including pretraining on some different (“proxy”)\ntask for which labeled data are easier to obtain9. Reinforcement learning involves the development of software\nagents that interact with the environment so as to maximize a numerical reward signal10.\nThe predominant modern ML paradigm is Deep Learning (DL), a representation learning method in which\nthe machine automatically discovers the representations that are required for carrying out a feature detection or\nclassification task using raw input data. In particular, DL enables multilayer computational models to learn rep-\nresentations of data through the hierarchical composition of relatively simple non-linear modules that transform\nfeatures into progressively higher levels of abstraction11. Effectively, by proceeding from low-level to high-level\nfeature abstraction, deep networks—which can be constructed from many layers and many units within lay-\ners—are able to learn increasingly complex functions12. Importantly, the higher-level representations allow deep\nnetworks to extract relevant features from input data11, which can be used to accurately perform discrimination,\nclassification, and detection tasks. Artificial Neural Networks (ANNs) are popular realizations of such deep mul-\ntilayer hierarchies, acting as highly non-linear parametric mappings from the input to the output space, with\nparameters (“weights”) determined by the optimization of some objective function (“cost”). This optimization\nis performed by backpropagating the error through the layers of the network in order to produce a sequence\nof incremental updates of the neural network weights13. Popular among a plethora of different ANN architec-\ntures are Multi-Layer Perceptrons (MLPs) typically used for general classification and regression problems,\nConvolutional Neural Networks (CNNs) for image classification tasks, and Recurrent Neural Networks (RNNs)\nfor time series sequence prediction problems.\nIn the past decade, growing computational powers and the enhanced availability of large collections of labeled\ndata have enabled the successful construction and training of neural networks with many layers and degrees of\nfreedom11, yielding innovative breakthroughs in a variety of tasks in speech recognition9,14, machine translation15,\nimage analysis, and computer vision16. As of today, revolutionary algorithms in these fields are primarily based on\nDL techniques. Furthermore, DL has matured into a technology that is widely used in commercial applications,\nincluding Apple’s Siri speech recognition, Google’s text translation, and Mobileye computer vision-based technol-\nogy for autonomously-driving cars17.\nCetacean bioacoustics has been an area of increasing study since the first recordings of marine mammals in\nthe late 1940s18. Recently, ML-based methods have been applied to cetacean vocalizations, including the use of\nunsupervised self-organizing networks to categorize the bioacoustic repertoire of false killer whale (Pseudorca\ncrassidens) vocalizations19. Gaussian Mixture Models (GMMs) and Support Vector Machine (SVM) algorithms\nhave been used to construct cetacean species detectors to discriminate between signals produced by Blainville’s\nbeaked whales (Mesoplodon densirostris), short-finned pilot whales (Globicephala macrorhynchus), and Risso’s\ndolphins (Grampus griseus)20; and similar computational techniques have been implemented in an effort to esti-\nmate sperm whale size distributions21. It has been demonstrated that a radial basis function network could effec-\ntively distinguish between six individual sperm whales22; and similarly, Hidden Markov Models (HMMs) and\nGMMs have performed the automatic identification of individual killer whales (Orcinus orca)23. In addition,\nANNs have been constructed to classify the bioacoustic signals of killer whales based on call type, individual\nwhale identity, and community dialect24,25. Most recently, Google obtained 90% accuracy using a ResNet-50\narchitecture, typically used for image classification, to develop a CNN-based humpback whale (Megaptera novae-\nangliae) detector for classifying non-speech audio26.\nAmong cetaceans, the sperm whale is known to have a sophisticated communication system27–29, based largely\ncodas, which are stereotyped sequences of 3–40 broadband clicks that are, in general, transmitted between social-\nizing whales27,29. The sperm whale communication system is thought to mediate a multileveled social structure30.\nFemale and immature sperm whales live in small social units with stable, long-term membership often made up\nof kin29,31–34. In the Caribbean, units show decade-long social preferences to associate with specific other units\ninto “Groups”35, while in the Pacific, no such preferences among units have been found34. All units which share\na vocal dialect, made up of over 20 different coda types, are said to belong to the same vocal clan. Socially segre-\ngated, but sympatric, clans have been found in both the Pacific and Atlantic Oceans29,36. Clans appear to demon-\nstrate differences in horizontal movement, social behavior, dive synchronies, diet, and foraging tactics, which may\naffect fitness of constituent units37–42. Sperm whale codas appear not only to encode clan identity, but they also\nmay contain unit- and individual-level acoustic specificity43–45, which suggests that codas could potentially serve\nto identify individuals, social units, and the dialect as a whole. Differential coda production might function to\ndelineate socially segregated, sympatric clans44. In addition to the communicative codas, sperm whales generate\nan extensive variety of click-based sounds, including (1) usual clicks for echolocation and foraging46–48, (2) buzzes\nfor short-range prey detection and feeding49–51, and (3) reverberating slow clicks, also known as clangs, produced\nonly by mature males47, in concert with other rare call types that have been poorly characterized. Given the com-\nplexity of its acoustic behavior, the sperm whale serves as an ideal species for constructing, applying, and testing\nnovel computational methods to improve the analysis of cetacean bioacoustics.\nHowever, the study of sperm whale communication has been slowed by the immense time investment\nrequired to collect high-quality audio recordings in the field and then subsequently to analyze and annotate these\nrecordings manually prior to being able to answer novel questions regarding communicative function of signals\nor information exchanged between animals. Here, we use Neural Network (NN)-based ML techniques pioneered\nin the study of human speech and language9,52 and apply them to sperm whale vocalizations to accelerate the\nabilities of researchers to address questions about cetacean communication systems. Specifically, we undertake\nScientific RepoRtS | (2019) 9:12588 | https://doi.org/10.1038/s41598-019-48909-4 2\n\nwww.nature.com/scientificreports/ www.nature.com/scientificreports\nFigure 1. Input testing spectrogram images with the trained network’s predicted output labels of (a) Click and\n(b) No Click. The lack of labeled axes and the image resolution reflect that these are the images that are used\npurely as input data when training the CNN. The resolution suffices for training a CNN-based echolocation\nclick detector.\nfour primary tasks, including (1) detection of echolocation clicks using a CNN-based approach, (2) classification\nof codas into categorical types using a Long Short-Term Memory (LSTM) RNN-based method, (3) recognition\nof vocal clan coda dialects with LSTM RNNs, and (4) identification of individual whales based on coda produc-\ntion with LSTM RNNs. LSTM RNNs are effective for this purpose since their architecture enables the network\nto preserve the non-independent sequential order of the input data8. As an alternative to LSTM, we also report\nresults with a Gated Recurrent Unit (GRU) architecture, a similar but simpler RNN with lower computational\ncomplexity and fewer parameters than LSTM (the latter being important in our setting of little data due to a lower\nrisk of overfitting). This study serves as a first step towards the development of an end-to-end system capable of\nautomated detection, classification, and prediction based on audio recordings of sperm whale sounds.\nResults\nUsing a cnn-based approach to detect sperm whale echolocation clicks. We achieve a training\ndetection accuracy of 99.5% using 650 spectrogram images, including 325 from each class (click vs. no click) (see\nSupplementary Fig. S1). We optimize the model by performing an exhaustive grid search through the high-di-\nmensional hyperparameter space. Using 100 unseen images (50 belonging to each class) reserved for testing the\ntrained network, the model categorizes all 100 images according to the correct annotation with zero mislabeled\nspectrograms (100% accuracy) (Fig. 1).\nUsing LStM and GRU Rnn-based approach to classify sperm whale codas. One of the challenges\nof supervised learning is the need for labeled data that, in some cases, might be hard to obtain. An approach\nrecently gaining popularity (e.g. in computer vision) is that of “self-supervised” learning (see53), wherein one\ndefines a “proxy task” bearing some similarity to the main problem, in order to exploit large-scale data for which\nthe labels are easier to obtain. The neural network trained this way often produces features relevant for the main\ntask (“transfer learning”) and can be fine-tuned on a smaller dataset, optimizing the cost function associated with\nthe main task. This procedure involves training a “base” network to carry out the proxy task and transferring the\nlearned representations to a “target network” that is subsequently trained to perform the main task54.\nWe follow a similar philosophy here, initially pretraining a deep LSTM base model to perform a proxy task of\ntime series prediction of the temporal distance between clicks n and (n-1) in an n-click coda, given the first (n-1)\nclicks in the coda (see Supplementary Fig. S2). Training this network involves minimizing the mean squared error\n(MSE) between the ground truth and predicted Inter-Click Interval (ICI) values (see Supplementary Fig. S3); the\nground truth labels (ICI values) are abundantly available for this proxy task. Prior to training, the relative error of\nthe prediction can exceed 300% while following training, the error can be reduced to as low as 12.4%. This sug-\ngests that this pretraining procedure enables the base network to extract meaningful features and representations\nfrom the time series coda sequence inputs. However, the task of predicting the temporal position of the last click\nin the coda is not necessarily of express interest in the study of sperm whale vocal behavior and is used as a proxy\ntask only55.\nAfter pretraining the model, we implement the transfer learning procedure, in which we save the layers of the\npretrained network as fixed (untrainable) parameters and add shallow trainable layers, the weights of which are\ndetermined during subsequent training on the main tasks of coda type, vocal clan, and individual whale identity\n(ID) classification. The use of pretrained layers greatly simplifies the architecture, reduces the number of parame-\nters, and makes it possible to train the model for the main task given the constraints associated with using a rela-\ntively small labeled dataset. With three annotated labels (coda type, vocal clan, individual whale ID) present in the\ndataset, we construct three different models, each optimized for the particular task. During the training phase, the\ntarget network adjusts the trainable parameters in such a way as to minimize the objective loss function, which,\nin this case, is the conventional categorical cross entropy function. As training progresses, accuracy (defined as\nthe fraction of coda inputs labeled appropriately) increases (see Supplementary Fig. S4). Following training using\nScientific RepoRtS | (2019) 9:12588 | https://doi.org/10.1038/s41598-019-48909-4 3\n\nwww.nature.com/scientificreports/ www.nature.com/scientificreports\nFigure 2. t-SNE visualization of the coda type classifier with colors denoting different coda type. Standard PCA\nand t-SNE techniques are used to plot the coda type hidden features in a three-dimensional feature space.\nthe Caribbean dataset, the losses for the coda type, clan class, and whale ID networks are 0.018, 0.160, and 0.071,\nrespectively, and the respective accuracies are 99.4%, 95.1%, and 98.3%. Evaluating the networks using 8,032\ncodas labeled by coda type, 1,898 codas labeled by clan class (949 belonging to each class “EC1” and “EC2”), and\n516 codas labeled by whale identity (258 belonging to each whale #5722 and #5727), we obtain respective accura-\ncies of 97.5%, 95.3%, and 99.4%. For the clan class and whale ID tasks, we ensure to normalize the datasets such\nthat each of the two labels present comprise half of the total dataset as a means to remove potential bias. In an\neffort to address possible overfitting, we repeat the analysis, ensuring to segment the codas into separate training\nand testing datasets. For the coda type network, we train the model using 90% of the 8,032 codas and obtain an\naccuracy of 99.7%, and we test the model on 804 unseen codas, yielding an accuracy of 99.9%, suggesting that\nthe trained model generalizes effectively to data. Carrying out this procedure for the clan classifier, we train the\nmodel on 1,708 codas (95.1% accuracy) and test on 190 unseen codas, achieving an accuracy of 96.8% using the\ntwo clans “EC1” and “EC2”. Given the relatively small number of codas labeled by whale ID, we opt to evaluate the\nmodel using the entire dataset. These results suggest the validity of employing ML-based techniques to advance\nthe study of sperm whale bioacoustics and provide a practical technique to deal with the scarcity of labeled data\nby using self-supervised pretraining on a proxy task. In addition, the results of the clan class and whale ID anal-\nyses indicate clan-level and individual-level characteristics of sperm whale vocalizations, in accordance with the\nfindings of43,44. Thus, not only do our results provide novel computational methods and techniques; they also offer\nimportant biological insight into sperm whale vocal behavior.\nTo visualize the activations of the networks, we employ the t-Distributed Stochastic Neighbor Embedding\n(t-SNE) algorithm to reduce the dimensionality of the hidden features within the models and to plot the outputs\nof the models in lower-dimensional feature spaces. We use Principal Component Analysis (PCA) to reduce the\ndimensionality of the feature space from 256 to 20, followed by t-SNE to further reduce the dimensionality from\n20 to three (for the coda type model, Fig. 2) or two (for the vocal clan, Fig. 3, and whale ID, Fig. 4, models). The\nconspicuous clustering provides qualitative insight into the behavior and efficacy of the trained models, as cluster-\ning shows the ability of the NNs to extract features distinguishing different signals. Lastly, we carry out a PCA to\nbetter understand the output of the networks and to examine how the networks might be using extracted features\nto perform the classification tasks (see Supplementary Fig. S5).\nGRU and LSTM are similar recurrent network architectures that share the ability to retain memory from\nprevious allocations. However, GRU-based networks are known to be more lightweight and to require fewer\nparameters, which is advantageous in our situation due to the risk of overfitting. For the coda type and vocal clan\nclassification prediction tasks, we obtain respective training accuracies of 99.6% and 94.1% and testing accuracies\nof 99.6% and 95.3%. For the whale ID classification problem, we once again evaluate the trained model using\nthe entire training dataset, yielding an accuracy of 99.6%. Noting the similar accuracies as those obtained using\nthe LSTM approach, we investigate the architectures of the various models. The coda type, vocal clan, and whale\nID LSTM models respectively contain 932,887, 855,810, and 855,810 total parameters, while the correspond-\ning GRU models involve 735,511, 658,434, and 658,434 parameters. While the classification performance of the\nGRU-based model is comparable to that of the LSTM-based network, the simpler architecture of the GRU RNN\n(which is reflected in its relatively fewer trainable parameters) suggests that the GRU approach might provide an\nimproved means to achieve high classification accuracies while avoiding overfitting.\nWe repeat the LSTM methods using the Eastern Tropical Pacific (ETP) dataset, which contains codas labeled\naccording to type and vocal clan. For the coda type analysis, we obtain an accuracy of 93.6% using 43 distinct\ncoda types. While the entire dataset includes 4,071 codas categorized as non-noise signals, there are a number\nScientific RepoRtS | (2019) 9:12588 | https://doi.org/10.1038/s41598-019-48909-4 4\n\nwww.nature.com/scientificreports/ www.nature.com/scientificreports\nFigure 3. t-SNE visualization of the vocal clan classifier with the two classes representing the two clans\nidentified in the eastern Caribbean (purple points “EC1”, red points “EC2”). We implement PCA and t-SNE to\nvisualize the hidden features of the clan class network.\nFigure 4. t-SNE visualization of the whale ID type classifier distinguishing between two identified whales from\nthe “EC1” clan recorded off Dominica (purple points indicate codas produced by whale #5722, and the red\npoints are codas generated by whale #5727). We implement PCA and t-SNE to visualize the hidden features in\nthe whale ID classifier network.\nof relatively rare coda types, which might be challenging to classify correctly. Partitioning the dataset into a\ndisjoint 90/10 training/testing, we train the model on 3,663 codas and obtain 94.2% accuracy, and we test on\nthe remaining 408 unseen codas, yielding an accuracy of 94.4%. For the vocal clan classification task, the model\nyields an accuracy of 93.1% using 6,044 total codas, 1,511 belonging to each of the four vocal clan classes (two of\nthe vocal clans present in the ETP dataset were discarded due to an insufficient number of recordings present).\nRepeating the analysis on segmented training (5,741 codas) and testing (303 codas) datasets yields accuracies of\n92.5% and 90.4% for training and testing, respectively. These findings demonstrate that the models are able to\ngeneralize to unseen datasets with a high degree of robustness.\nLastly, in an effort to address the ability of the NN-based methods to accommodate data from different data-\nsets, we initially pretrain a coda type classification base model using the Dominica dataset and proceed to train\nScientific RepoRtS | (2019) 9:12588 | https://doi.org/10.1038/s41598-019-48909-4 5\n\nwww.nature.com/scientificreports/ www.nature.com/scientificreports\nand test the network using ETP coda data. Explicitly, using 7,847 Dominica ICI coda vectors with nine or fewer\nelements, we pretrain a network to perform the usual proxy task, resulting in a mean relative error in final ICI\nvalue prediction of 12.4%. After implementing the transfer learning procedure, we prepare segmented training/\ntesting datasets using codas from the ETP (restricting the analysis to coda vectors consisting of fewer than nine\nelements), and we train the target network to perform the coda type classification. Evaluating the model on train-\ning (3,012 codas) and testing (335 codas) datasets yields accuracies of 99.1% and 97.9%, respectively. The results\nof our study show the feasibility and advantages of using automated ML and NN-based computational approaches\nto analyze sperm whale acoustic signals.\nMethods\nWe implement two distinct methods for applying machine learning techniques to sperm whale vocalizations: the\nfirst involves a CNN-based approach with spectrogram image inputs to construct an echolocation click detec-\ntor; and the second uses LSTM and GRU RNNs with time series sequence inputs of coda interclick intervals to\nclassify coda types and to recognize both vocal clans and individual whales. These classification tasks involve\nsupervised learning and transfer learning procedures, during which we train the models using high-quality\nmanually-annotated time-series data.\nUsing a cnn-based approach to detect sperm whale echolocation clicks. For the CNN-based\nmethod, we construct a sperm whale echolocation click detector designed to label a spectrogram image as “click”\nor “no click” depending on the network’s classification of the input image. This requires a multi-step proce-\ndure involving: (1) processing the raw acoustic data; (2) developing a threshold-based click detector in the time\ndomain; (3) generating an image dataset consisting of appropriately labeled spectrogram images; (4) constructing\nand training a CNN; (5) testing the trained CNN to assess its ability to generalize to new data; (6) analyzing the\nrobustness of the network and investigating its feature extraction behavior.\nWe process the raw acoustic data using two publicly available audio datasets: (1) the ‘Best Of’ cuts from the\nWilliam A. Watkins Collection of Marine Mammal Sound Recordings database from Woods Hole Oceanographic\nInstitution (https://cis.whoi.edu/science/B/whalesounds/index.cfm), and (2) sample sperm whale acoustic\nrecordings from the Centro Interdisciplinare di Bioacustica e Ricerche Ambientali (CIBRA) Cetacean Sound\nArchive (http://www-3.unipv.it/cibra/res_cesar_uk.html). We initially apply a Butterworth bandpass filter of\norder five with a low-frequency cutoff f = 2 kHz and a high-frequency cutoff f = 20 kHz; this enables us to\nl h\nremove unwanted (low-frequency) noise while preserving the broadband structure of clicks. We subtract the\nmean and normalize the peak amplitude to process the signal prior to developing our threshold-based click\ndetector algorithm. For the threshold-based detector, we employ a local maximum search algorithm to identify\nsuprathreshold (th = 0.15) peaks separated from nearest neighbors by a temporal distance greater than some\ncanonical minimum ICI value (th_ICI = 0.2 s) determined by inspecting a histogram of detected inter-click time\nintervals, which allows us to identify clicks while neglecting interferences such as reflections, echoes, and subse-\nquent click pulses. However, the primary motivation for devising this threshold-based method is to generate an\nappropriately-labeled spectrogram image dataset.\nTo construct the spectrogram image dataset, we perform the standard spectral analysis and transform to\nthe time-frequency domain. The algorithm automatically labels spectrograms (created using a tapered cosine\nTukey window with shape parameter of 0.25, optimized for transient signals), as “click” or “no click” depending\non whether or not the threshold-based detector has identified a click as present or absent over the time interval\n(0.5 s) of the spectrogram. With this approach, we generate a training dataset consisting of 650 images and a\ntesting dataset with 100 spectrograms, ensuring to normalize these datasets with equal numbers of images from\neach class (“click” and “no click”). Note, however, that a potential source of classification error could be derived\nfrom label noise in the training dataset, which should be addressed in a more comprehensive study investigating\nCNN-based click detectors.\nWe then proceed to construct the CNN by designing a custom architecture involving three consecutive con-\nvolutional and max pooling layers, followed by fully-connected dense layers56 (see Supplementary Fig. S6). We\ninclude a dropout regularization to prevent potential overfitting. Minimizing the conventional categorical cross\nentropy objective loss function, we train the network using the annotated training dataset. Performing an exhaus-\ntive grid search through the hyperparameter space, we obtain a dictionary of optimal hyperparameters so as to\nmaximize accuracy while still avoiding potential overfitting. For this study, these include filter size 5, batch size\n64, a sigmoid activation function, the ‘adam’ optimizer with learning rate 1 × 10−3, and 50 training epochs (see8\nfor machine learning fundamentals). Following training, we use the trained model to evaluate the testing dataset\nas a means to investigate the network’s ability to generalize to unseen data. Finally, after having trained and tested\nthe model, we use standard computational techniques to analyze the feature extraction behavior of the network.\nNamely, we sequester the high-dimensional hidden features within the network, and we employ a number of\ndimensionality reduction methods in order to visualize the ability of the network to cluster and classify input\ndata. Firstly, we use PCA to understand how the different features contribute to the variance of the data. Also, we\nemploy PCA to reduce the dimensionality of the hidden feature data from 80 to 20, and then we apply the t-SNE\nalgorithm57 to further reduce the dimensionality in order to observe the clustering behavior of the trained model\nin a two-dimensional plot.\nUsing LStM and GRU Rnns to classify codas, determine vocal clan, and recognize individual\nwhales. Using LSTM RNNs, we construct deep artificial neural networks, which we train to perform a num-\nber of classification tasks based on high-quality manually annotated datasets. Data used in this study comes\nfrom two long-term field studies on sperm whales: (1) off the island of Dominica in the eastern Caribbean31; and\n(2) across the ETP but with a focus on the Galapagos Islands58. The two study sites employed similar methods\nScientific RepoRtS | (2019) 9:12588 | https://doi.org/10.1038/s41598-019-48909-4 6\n\nwww.nature.com/scientificreports/ www.nature.com/scientificreports\ncommon in the study of this species to detect, follow, and record sperm whales as well as similar analytical tech-\nniques to define coda types, delineate vocal clans, and identify individual whales (details in29,44,59). The Dominica\ndataset contains ~9,000 annotated codas that were collected across 3,834 hours with whales on 406 days over 530\ndays of effort from 2005–2016. This represents recordings from over 10 different social units who are members of\ntwo vocal clans referred to as “EC1” and “EC2”31,44. The ETP dataset contains ~17,000 annotated codas collected\nbetween 1985–2014 based on recordings of over 89 groups who are members of six vocal clans29,59. The annotated\ndatasets contain inter-click interval (the absolute time between clicks in a coda, ICI) vectors classified according\nto categorical coda type, vocal clan membership, and (in the case of the Dominica dataset) individual whale\nidentity using the methods outlined in the publications listed above. While codas tend to be comprised of 3–40\nclicks27, we restrict the analysis to codas with at most 10 clicks using the Dominica dataset or at most 12 clicks\nusing the ETP dataset.\nWhereas the click detector CNN described above is optimized for image inputs, the LSTM RNN used here\naccepts time-series sequence inputs. In this case, we use vectors whose elements consist of the ICI values between\nsuccessive clicks within a given coda. The basic procedure for this time-domain approach to ML-based sperm\nwhale bioacoustic analysis involves (1) pretraining an initial base model to perform a related proxy task and (2)\ntrain a model comprising the fixed neural network pretrained on the proxy task and a small trainable neural\nnetwork to carry out the relevant classification tasks (coda type, vocal clan, and whale identity classification).\nAfter training the model, we proceed to test the model and visualize the networks’ activations, primarily by\nimplementing dimensionality reduction algorithms. Initially, we pretrain a custom-built model to perform the\nproxy task (see Supplementary Fig. S7). In this case, we construct an architecture consisting of two LSTM layers\nwith 256 hidden units followed by a fully connected layer and train the network (by minimizing the root mean\nsquare error using the ‘adam’ optimizer with a learning rate 1 × 10−3 for 20 epochs) to carry out a supervised\ntime-series forecast. For each coda ICI vector of length n, we train the network to predict the nth ICI value given\nthe first (n-1) values in the sequence. This pretraining procedure enables the network to extract features from the\ninput data, and these features become relevant as we proceed with the transfer learning procedure. To implement\ntransfer learning, we save the deepest (first) layers of the architecture of the pretrained model, and we fix the\ntrained weights as untrainable parameters. After removing the shallowest (last) fully connected layers from the\nnetwork, we add a supplemental LSTM layer with 256 hidden units (as well as additional fully connected layers)\nwhich involve randomly initialized trainable weight parameters. We use both rectified linear unit (ReLU) (ex.60)\nand softmax activation functions (see8), and we appropriately adjust the hyperparameters for each task. Using the\nground truth labels (coda type, vocal clan, and whale identity) annotated by human experts, we train the model to\nperform the particular classification task of interest. We seek to minimize the categorical cross entropy loss func-\ntion in which the network’s predicted label for the coda is compared with the ground truth annotated label. After\ntraining the model, we once again test the model on unseen data (so as to address the possibility of overfitting the\ntraining dataset), and we also use the standard PCA and t-SNE algorithms to investigate and visualize the feature\nextraction behavior of the trained model. Finally, we repeat the analysis replacing LSTM layers with GRU layers,\nand we compare the performances and architectures of the trained models.\nDiscussion\nThe large amounts of high-resolution data collected from increasingly wide regions of the oceans demand\nnovel tools to automate the detection and classification of signals, which can accelerate cetacean bioacoustics\nresearch and promote species population assessment and management. Our results show that the sperm whales’\nclick-based sonar and communication system is well-suited for ML-based techniques. In particular, we estab-\nlish that CNNs, neural network architectures used for computer vision tasks, can successfully be used to detect\nsperm whale echolocation clicks from spectrograms. This introduces the prospect of future studies aiming to con-\nstruct CNN-based models that automatically carry out finer-scale classification tasks (i.e. coda type, vocal clan,\nwhale ID classification) by directly using spectrogram image inputs and avoiding the pre-processing stage of click\nextraction. This approach to sperm whale classification problems, however, demands large amounts of labeled\nraw acoustic data, which are not presently available. Instead, we opt to employ RNN-based methods to perform\nclassification tasks on time-series of inter-click intervals. We demonstrate that LSTM and GRU, RNN architec-\ntures used for speech recognition and text translation, are able to classify codas into recognizable types and to\naccurately predict the clan membership and individual identity of the signaler. This is a significant improvement\nover previous methods used to classify codas as it minimizes the role of the observer in determining any param-\neters a priori, which was required for coda type statistical clustering techniques involved in prior studies43,44,59.\nNonetheless, the high coda type classification accuracies we obtain using novel ML-based methods support the\nprevious categorization of sperm whale codas into existing, predefined types. In addition, our results show the\nfeasibility of using “self-supervised” learning on proxy tasks and applying trained neural networks to label unseen\ncoda data according to type, clan, and individual whale, which could drastically expedite the analysis of recorded\nsperm whale signals.\nThe datasets in this study collectively contain ~26,000 annotated sperm whale codas and represent the largest\navailable labeled data of sperm whale coda signals. Using these datasets, this study exhibits the potential appli-\ncations of ML to sperm whale vocalizations by demonstrating the ability of NNs to perform detection and clas-\nsification tasks with high degrees of accuracy. However, because large volumes of high-quality data are required\nfor training deep neural networks–which aim to achieve state-of-the-art classification accuracy–our analysis is\nlimited to the coda types, clan classes, and individual whales with sufficiently sizable volumes of data to train the\nnetworks. In order to improve our methods, even larger datasets would be computationally and methodologi-\ncally optimal, especially when performing more specialized tasks (e.g. individual whale identification). With that\nsaid, such large quantities of high-quality sperm whale data are difficult to obtain (due to the intensive nature of\nmarine fieldwork) as well as to annotate/analyze since such processing tends to require trained human experts.\nScientific RepoRtS | (2019) 9:12588 | https://doi.org/10.1038/s41598-019-48909-4 7\n\nwww.nature.com/scientificreports/ www.nature.com/scientificreports\nPresently, sperm whale bioacoustics research studies require significant time and oversight by researchers (e.g.\nto define dialects and extract codas), but the ML-based approaches remain somewhat restricted by the paucity\nof readily accessible data (including the bulk available data in addition to certain components of the data such as\nrare coda types). Given the constraints imposed by the limited available data, we show the use of pretraining with\nproxy tasks as a valid alternative. Our novel techniques achieve high classification accuracies and robustly and\neffectively generalize to new, unseen data while avoiding overfitting.\nThis study establishes the numerous advantages that ML-based methods provide for sperm whale bioacous-\ntics research. The techniques presented in this study can also be helpful in the design of next-generation sensors\nand systems, which should directly intake and analyze sperm whale acoustic data from raw audio recordings. By\nautomating the detection and classification steps, large-scale audio datasets could potentially be processed in near\nreal-time, enabling ML approaches to successfully and efficiently address questions regarding sperm whale vocal\nbehavior and communication (e.g. the cultural distinction between codas produced by different vocal clans) while\nsimultaneously reducing the need for manual oversight.\nGiven that culture is important for the conservation of wide-ranging species61 and that human impacts can\ngreatly erode the cultural diversity within a species62, it is critical to have a complete understanding of the cultural\nboundaries between clans of sperm whales in order to design a framework for the conservation of cultural diver-\nsity.The novel ML-based techniques employed in this study enhance the scientific community’s understanding\nof sperm whale bioacoustics and provide an efficient means for analyzing large datasets, which can facilitate the\ndevelopment of improved conservation and management strategies designed to protect global sperm whale (and\nother cetacean) populations.\nData Availability\nThe Dominica coda dataset used to train and evaluate the neural networks is available as Supplementary File S6\nand the ETP coda dataset used to train and evaluate the neural networks is available as Supplementary File S7.\nThe custom-written algorithms (Python 3.0) used in this study are available at: https://github.com/dgruber212/\nSperm_Whale_Machine_Learning. All data generated or analyzed during this study are included in this pub-\nlished article and its Supplementary Information files. The machine learning code is available on Github.\nReferences\n1. Roth, G. & Dicke, U. Evolution of the brain and intelligence. Trends Cogn. Sci. 9, 250–257 (2005).\n2. Marino, L. et al. Cetaceans have complex brains for complex cognition. PLoS Biology, https://doi.org/10.1371/journal.pbio.0050139\n(2007).\n3. Connor, R. C., Mann, J., Tyack, P. L. & Whitehead, H. Social evolution in toothed whales. Trends in Ecology and Evolution, https://\ndoi.org/10.1016/S0169-5347(98)01326-3 (1998).\n4. Marino, L. Cetacean Brain Evolution: Multiplication Generates Complexity. Int. J. Comp. Psychol. 17, 1–16 (2004).\n5. Herman, L. M. & Tavolga, W. N. In Cetacean Behavior: Mechanisms and Functions 149–209 (Wiley & Sons, Inc., 1980).\n6. Tyack, P. L. In Cetacean Societies: Field Studies of Dolphins and Whales (eds Mann, J., Connor, R. C., Tyack, P. L. & Whitehead, H.)\n270–307 (The University of Chicago Press, 2000).\n7. Putland, R. L., Ranjard, L., Constantine, R. & Radford, C. A. A hidden Markov model approach to indicate Bryde’s whale acoustics.\nEcol. Indic. 84, 479–487 (2018).\n8. Raschka, S. & Mirjalili, V. Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and Tensor Flow\n(2017).\n9. Hinton, G. et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE\nSignal Process. Mag., https://doi.org/10.1109/MSP.2012.2205597 (2012).\n10. Sutton, R. & Barto, A. Reinforcement Learning. (MIT Press, 2018).\n11. Lecun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature, https://doi.org/10.1038/nature14539 (2015).\n12. Goodfellow, I., Bengio, Y. & Courville, A. Deep Learning. (2016).\n13. Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by back-propagating errors. Nature 323, 533–536 (1986).\n14. Mikolov, T., Deoras, A., Povey, D., Burget, L. & Černocký, J. Strategies for training large scale neural network language models. In\n2011 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2011, Proceedings, https://doi.org/10.1109/\nASRU.2011.6163930 (2011).\n15. Sutskever, I., Vinyals, O. & Le, Q. V. In Advances In Neural Information Processing Systems 3104–3112 (2014).\n16. Taigman, Y., Yang, M., Ranzato, M. & Wolf, L. DeepFace: Closing the gap to human-level performance in face verification. In\nProceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, https://doi.org/10.1109/\nCVPR.2014.220 (2014).\n17. Bronstein, M. M., Bruna, J., Lecun, Y., Szlam, A. & Vandergheynst, P. Geometric Deep Learning: Going beyond Euclidean data. IEEE\nSignal Process. Mag. 34, 18–42 (2017).\n18. Schevill, W. E. & Lawrence, B. Underwater listening to the white porpoise (Delphinapterus leucas). Science (80-.), https://doi.\norg/10.1126/science.109.2824.143 (1949).\n19. Murray, S. O., Mercado, E. & Roitblat, H. L. The neural network classification of false killer whale (Pseudorca crassidens)\nvocalizations. J. Acoust. Soc. Am. 104, 3626–3633 (1998).\n20. Roch, M. A., Soldevilla, M. S., Hoenigman, R., Wiggins, S. M. & Hildebrand, J. A. Comparison of machine learning techniques for\nthe classification of echolocation clicks from three species of odontocetes. Can. Acoust. 36, 41–47 (2008).\n21. Beslin, W. A. M., Whitehead, H. & Gero, S. Automatic acoustic estimation of sperm whale size distributions achieved through\nmachine recognition of on-axis clicks. J. Acoust. Soc. Am. 144, 3485–3495 (2018).\n22. Van Der Schaar, M., Delory, E., Català, A. & André, M. Neural network-based sperm whale click classification. J. Mar. Biol. Assoc.\nUnited Kingdom 87, 35–38 (2007).\n23. Brown, J. C., Smaragdis, P. & Nousek-McGregor, A. Automatic identification of individual killer whales. J. Acoust. Soc. Am. 128,\nEL93–EL98 (2010).\n24. Gaetz, W., Jantzen, K., Weinberg, H., Spong, P. & Symonds, H. A neural network method for recognition of individual Orcinus orca\nbased on their acoustic behaviour: phase 1. In Proceedings of OCEANS ’93, https://doi.org/10.1109/OCEANS.1993.325960 (1993).\n25. Deecke, V. B., Ford, J. K. B. & Spong, P. Quantifying complex patterns of bioacoustic variation: Use of a neural network to compare\nkiller whale (Orcinus orca) dialects. J. Acoust. Soc. Am. 105, 2499–2507 (1999).\n26. Harvey, M. Acoustic Detection of Humpback Whales Using a Convolutional Neural Network. Google AI Blog, at, https://\nai.googleblog.com/2018/10/acoustic-detection-of-humpback-whales.html (2018).\nScientific RepoRtS | (2019) 9:12588 | https://doi.org/10.1038/s41598-019-48909-4 8\n\nwww.nature.com/scientificreports/ www.nature.com/scientificreports\n27. Watkins, W. A. & Schevill, W. E. Sperm whale codas. J. Acoust. Soc. Am. 62, 1486–1490 (1977).\n28. Weilgart, L. & Whitehead, H. Group-specific dialects and geographical variation in coda repertoire in South Pacific sperm whales.\nBehav. Ecol. Sociobiol, https://doi.org/10.1007/s002650050343 (1997).\n29. Rendell, L. E. & Whitehead, H. Vocal clans in sperm whales (Physeter macrocephalus). Proc. R. Soc. B Biol. Sci., https://doi.\norg/10.1098/rspb.2002.2239 (2003).\n30. Whitehead, H. et al. Multilevel Societies of Female Sperm Whales (Physeter macrocephalus) in the Atlantic and Pacific: Why Are\nThey So Different? Int. J. Primatol. 33, 1142–1164 (2012).\n31. Gero, S. et al. Behavior and social structure of the sperm whales of Dominica, West Indies. Mar. Mammal Sci. 30, 905–922 (2014).\n32. Konrad, C. M., Gero, S., Frasier, T. & Whitehead, H. Kinship influences sperm whale social organization within, but generally not\namong, social units. R. Soc. Open Sci. 5, 180914 (2018).\n33. Mesnick, S. L. Genetic relatedness in sperm whales: Evidence and cultural implications. Behav. Brain Sci. 24, 346–347 (2001).\n34. Christal, J., Whitehead, H. & Lettevall, E. Sperm whale social units: variation and change. Can. J. Zool. 76, 1431–1440 (1998).\n35. Gero, S., Gordon, J. & Whitehead, H. Individualized social preferences and long-term social fidelity between social units of sperm\nwhales. Anim. Behav., https://doi.org/10.1016/j.anbehav.2015.01.008 (2015).\n36. Gero, S., Bøttcher, A., Whitehead, H. & Madsen, P. T. Socially segregated, sympatric sperm whale clans in the Atlantic Ocean. R. Soc.\nOpen Sci. 3 (2016).\n37. Aoki, K. et al. Diel diving behavior of sperm whales off Japan. Mar. Ecol. Prog. Ser. 349, 277–287 (2007).\n38. Amano, M., Kourogi, A., Aoki, K., Yoshioka, M. & Mori, K. Differences in sperm whale codas between two waters off Japan: possible\ngeographic separation of vocal clans. J. Mammal. 95, 169–175 (2014).\n39. Whitehead, H. & Rendell, L. Movements, habitat use and feeding success of cultural clans of South Pacific sperm whales. J. Anim.\nEcol. 73, 190–196 (2004).\n40. Marcoux, M., Whitehead, H. & Rendell, L. Sperm whale feeding variation by location, year, social group and clan: Evidence from\nstable isotopes. Mar. Ecol. Prog. Ser. 333, 309–314 (2007).\n41. Marcoux, M., Rendell, L. & Whitehead, H. Indications of fitness differences among vocal clans of sperm whales. Behav. Ecol.\nSociobiol. 61, 1093–1098 (2007).\n42. Cantor, M. & Whitehead, H. How does social behavior differ among sperm whale clans? Mar. Mammal Sci. 31, 1275–1290 (2015).\n4 3. Oliveira, C. et al. Sperm whale codas may encode individuality as well as clan identity. J. Acoust. Soc. Am., https://doi.\norg/10.1121/1.4949478 (2016).\n44. Gero, S., Whitehead, H. & Rendell, L. Individual, unit and vocal clan level identity cues in sperm whale codas. R. Soc. Open Sci.,\nhttps://doi.org/10.1098/rsos.150372 (2016).\n45. Antunes, R. et al. Individually distinctive acoustic features in sperm whale codas. Anim. Behav. 81, 723–730 (2011).\n46. Backus, R. H. & Schevill, W. E. In Whales, Dolphins and Porpoises (ed. Norris, K. S.) 510–527 (University of California Press, 1966).\n47. Weilgart, L. S. & Whitehead, H. Distinctive vocalizations from mature male sperm whales (Physeter macrocephalus). Can. J. Zool. 66,\n1931–1937 (1988).\n48. Møhl, B., Wahlberg, M., Madsen, P. T., Miller, L. A. & Surlykke, A. Sperm whale clicks: Directionality and source level revisited. J.\nAcoust. Soc. Am. 107, 638–648 (2000).\n49. Worthington, L. V. & Schevill, W. E. Underwater sounds heard from sperm whales. Nature 4850, 291 (1957).\n50. Jaquet, N., Dawson, S. & Douglas, L. Vocal behavior of male sperm whales: Why do they click? J. Acoust. Soc. Am. 109, 2254–2259\n(2001).\n51. Miller, P. J. O., Johnson, M. P. & Tyack, P. L. Sperm whale behaviour indicates the use of echolocation click buzzes ‘creaks’ in prey\ncapture. Proc. R. Soc. B Biol. Sci. 271, 2239–2247 (2004).\n52. Graves, A. & Jaitly, N. Towards End-to-End Speech Recognition with Recurrent Neural Networks. Proc. 31st Int. Conf. Mach. Learn.\nPMLR 32, 1764–1772 (2014).\n53. Mahendran, A., Thewlis, J. & Vedaldi, A. Cross Pixel Optical Flow Similarity for Self-Supervised Learning. In Proc. Asian Conference\non Computer Vision (ACCV) (2018).\n54. Yosinski, J., Clune, J., Bengio, Y\n\n[TRUNCATED...]", "affiliations": [{"university": "", "country": "", "discipline": ""}, {"university": "", "country": "", "discipline": ""}], "species_categories": ["Marine Mammal"], "specialized_species": ["Sperm Whale"], "computational_stages": ["Data Collection", "Pre-processing", "Sequence Representation", "Meaning Identification"], "linguistic_features": ["Semanticity", "Tradition and Cultural Transmission"], "updated_at": "2026-01-27T20:59:02.432751", "committed_at": "2026-01-27T20:59:12.667752"}
{"id": "f767e41f-3b76-4cfb-82a7-37fd5d7cc3f9", "doi": "https://doi.org/10.5670/oceanog.2007.03", "title": "An Overview of Fixed Passive Acoustic Observation Methods for Cetaceans", "authors": ["David K. Mellinger", "Kathleen M. Stafford", "Sue E. Moore", "Robert P. Dziak", "Haru Matsumoto"], "year": "2007", "journal": "Oceanography", "abstract": "", "pdf_filename": "https_doi.org_10.5670_oceanog.2007.03.pdf", "data_source": "subset", "status": "saved", "created_at": "2026-01-27T20:15:26.684260", "analysis_notes": "SPeCiAl iSSue ON OCeAN exPlOrAtiON\nAn Overview of\nFixed Passive Acoustic\nObservation Methods\nfor Cetaceans\nBy DAViD K. MelliNger, KAthleeN M. StAFFOrD, Sue e. MOOre,\nrOBert P. DziAK, AND hAru MAtSuMOtO\n3366 OOcceeaannooggrraapphhyy VVooll.. 2200,, NNoo.. 44\nor\ncollective\nredistirbution\nof\nany\nportion\nof\nthis\narticle\nby\nphotocopy\nmachine,\nreposting,\nor\nother\nmeans\nis\npermitted\nonly\nwith\nthe\napproval\nof\nThe\nOceanography\nSociety.\nSend\nall\ncorrespondence\nto:\ninfo@tos.org\nor\nTh\ne Oceanography\nSociety,\nPO\nBox\n1931,\nrockville,\nMD\n20849-1931,\nuSA.\nThis\narticle\nhas\nbeen\npublished\nin\nOceanography,\nVolume\n20,\nNumber\n4,\na\nquarterly\njournal\nof\nThe\nOceanography\nSociety.\nCopyright\n2007\nby\nThe\nOceanography\nSociety.\nAll\nrights\nreserved.\nPermission\nis\ngranted\nto\ncopy\nthis\narticle\nfor\nuse\nin\nteaching\nand\nresearch.\nrepublication,\nsystemmatic\nreproduction,\n\nCetaceans are increasingly being included from the surrounding environment. In statistical methods, and interpretation of\nas top trophic-level predators in models recent years, passive acoustic methods results, and then provide an example of\nof ecosystem dynamics (Baumgartner have become increasingly widespread for the results from a fixed passive acoustic\nand Mate, 2003; Tynan, 2004; Redfern et cetacean observation (e.g., Moore et al., survey of Bransfield Strait, Antarctica,\nal., 2006). Traditional visual survey meth- 2006). In joint visual-acoustic surveys, supported by the National Oceanic and\nods for cetaceans detect only a fraction of acoustic modalities have detected one Atmospheric Administration’s (NOAA’s)\nthe animals present, both because visual to ten times as many cetacean groups Office of Ocean Exploration.\nobservers can see them only during the as visual ones (McDonald and Moore,\nvery short period when they are at the 2002; Širovic´ et al., 2004; Barlow and MethODS\nsurface, and because visual surveys can be Taylor, 2005; Rankin et al., 2007), and Fixed passive acoustic surveys require\nundertaken only during daylight hours acoustic observation has the additional several steps, including survey design,\nin relatively good weather (Mellinger advantages that it can continue at night, placement and sometimes recovery of\nand Barlow, 2003). Perhaps more impor- in poor weather, and under other condi- recording instruments, extraction of\ntantly, visual survey results can be highly tions in which visual observation cannot. vocalizations of interest from recorded\nvariable, due both to clumping of ceta- Another distinction in passive acous- data, statistical analysis of vocalizations,\nceans into large groups and to their rela- tic survey methods is that of fixed versus and interpretation of the results.\ntively limited spatial and temporal scales. mobile acoustic sensors. Hydrophones\nSurveys are typically performed using a may be towed behind a ship or affixed to instrumentation\nsmall number of observation points— an ocean glider or other mobile platform Two types of passive acoustic equip-\none or a few vessels—for a few weeks to a to sample a large area. Alternatively, the ment are used widely for capturing\nfew months of the year. hydrophone instruments may be left in sound—cabled hydrophones and auton-\nomous recorders. Cabled hydrophones\nare typically deployed in permanent or\nsemi-permanent installations. Because of\nthe expense of cabled systems, they are\nin recent years, passive acoustic methods\nin widespread use mainly by navies or\nhave become increasingly widespread other governmental agencies; examples\ninclude the Sound Surveillance System\nfor cetacean observation.\nof the US Navy; the hydrophone arrays\non US Navy test ranges in the Bahamas,\nsouthern California, and Hawaii; and the\nhydrophones of the Comprehensive Test\nAcoustic observation may be active place for long time periods. Advantages Ban Treaty Organization. The benefits\nor passive. Active acoustics, in which a of the mobile approach include large of these systems for scientific research\nsound is transmitted and the returning areal coverage and simplicity in com- are that they provide data continuously\necho analyzed, is widely used for obser- bining acoustic detection with a visual in near-real time (so that rapid response\nvation of zooplankton and micronekton survey, while the principal advantages of to unusual events is possible), they have\n(primarily fishes, squids, and shrimp), the fixed approach are that observation hydrophones in pelagic areas where\nas well as in fisheries research. Here, we usually spans a longer time period and is marine mammal surveys are otherwise\ndescribe passive acoustic observation, frequently less expensive. rare, and their operation and mainte-\nwhich is used more widely for cetacean In this article, we describe the meth- nance is funded by external sources.\nobservation. In this type of observation, odology of fixed passive acoustic obser- However, these systems typically have\nthe instrument used does not produce vations, including instrumentation, access restrictions because of their mili-\nany sounds itself; it only captures sounds software for detection of vocalizations, tary or sensitive nature, so that the data\nOceanography December 2007 37\n\nare not easily accessible. Further, the years. Depending on the instrument sensor on a mooring (Clark et al., 2007)\nrecording bandwidth is often restricted configuration and deployment duration, or on shore-fast ice (Clark et al., 1996)\nto fairly low frequencies due to the sound capture happens either continu- with a radio link to a shore station or\nnature of the signals for which they were ously or according to a sampling plan. ship (e.g., Rankin et al., 2005). As with\ndesigned. Cabled systems operated by Autonomous hydrophones are typically cabled systems, data are captured con-\ntinuously in real time.\nA final variant consists of using\nmarine mammals themselves as plat-\nforms for acoustic sensors. By miniatur-\nSome species are more amenable to\nizing the sensor and electronics package\naccurate acoustic surveys than others.\nto fit into an attachable tag, the instru-\nment can record acoustic data from\nareas where the animal itself is exposed.\nSuch tags have been deployed on larger\nnongovernmental organizations often deployed in arrays of three to ten instru- marine mammals, including elephant\nconsist of one or a few hydrophones ments to provide areal coverage and to seals (Mirounga angustirostris; Fletcher\nplaced within several kilometers of shore. allow for localization of sound sources. et al., 1996; Burgess et al., 1998) and sev-\nTheir data are more openly accessible A number of laboratories have designed eral species of mysticete and odontocete\nbut typically cover only relatively small and used such instruments since the whales (Madsen et al., 2002; Johnson\nshelf areas. The advent of cabled ocean mid-1990s (Calupca et al., 2000; Fox et and Tyack, 2003).\nobservatories (e.g., Barnes et al., 2007; al., 2001; Wiggins, 2003; Lammers et al., A key difference in choice of instru-\nORION Program Office, 2007) promises in press), and more recently a commer- mentation is whether hydrophones are\nto extend the capabilities of such non- cial version has become available (http:// deployed in isolation from one another,\nmilitary systems to larger offshore areas. www.totalsat.qc.ca/mte/aural_en.htm). in distributed small-area arrays for local-\nAutonomous recorders consist of These instruments store acoustic data ization, or in large coherent arrays to\na hydrophone and a battery-powered internally, so they must be recovered allow beamforming. When several single\ndata-recording system. These instru- before data analysis can begin. sensors are placed tens to hundreds of\nments are moored on the seafloor, some- In addition to the widely used cabled kilometers apart, they are usually too far\ntimes with a cable and flotation to buoy hydrophones and autonomous recorders, apart to detect an individual animal on\nthe hydrophone sensor up in the water radio-linked hydrophones are occasion- multiple instruments, so this configura-\ncolumn (e.g., at the depth of the deep ally used for marine mammal acoustic tion may be considered to be multiple\nsound channel) for periods of up to two surveys. These combine a hydrophone isolated instruments. When instruments\nare placed closely enough that three or\nDAViD K. MelliNger (David.Mellinger@oregonstate.edu) is Associate Professor, more can detect a vocalizing animal,\nSenior Research, Cooperative Institute for Marine Resources Studies, Oregon State then the animal can be located using\nUniversity, and Pacific Marine Environmental Laboratory (PMEL), National Oceanic and time-of-arrival differences; localiza-\nAtmospheric Administration (NOAA), Newport, OR, USA. KAthleeN M. StAFFOrD tion of successive vocalizations allows\nis Research Scientist, Applied Physics Laboratory, University of Washington, Seattle, WA, the individual to be tracked as it moves\nUSA. Sue e. MOOre is Principal Oceanographer and Senior Scientist, Alaska Fisheries (Clark et al., 1996). When approximately\nScience Center, NOAA, Seattle, WA, USA. rOBert P. DziAK is Associate Professor, Senior 10 or more hydrophones are deployed\nResearch, Cooperative Institute for Marine Resources Studies, Oregon State University, and in a tightly spaced array, a sound wave\nPMEL/NOAA, Newport, OR, USA. hAru MAtSuMOtO is Research Associate, Cooperative from an animal arrives coherently at all\nInstitute for Marine Resources Studies, Oregon State University, and PMEL/NOAA, of the hydrophones, allowing beamform-\nNewport, OR, USA. ing techniques to be used (Johnson and\n38 Oceanography Vol. 20, No. 4\n\nDudgeon, 1993; Stafford et al., 1998). sured over 185 dB RMS re 1 µPa @ 223 dB re 1 µPa peak equivalent RMS\nBeamforming increases the signal-to- 1 m (Cummings and Thompson, @ 1 m (Møhl et al., 2000). In contrast,\nnoise ratio (SNR) of sound arriving 1971; McDonald et al., 2001; Thode bottlenose dolphin (Tursiops trunca-\nfrom certain directions such that an et al., 2000; Širovic´ et al., 2007), while tus) tonal sounds (whistles) have been\nN-element hydrophone array provides on-axis sperm whale clicks have been measured at source levels up to 169 dB\nan “array gain” of approximately √N in measured at instantaneous levels up to re 1 µPa RMS @ 1 m (Janik, 2000),\nSNR, equivalent to an increase in acous-\ntic detection area of approximately N.\nBehavioral Considerations\na. Frequencies of cetacean moans and whistles\nSome species are more amenable to\naccurate acoustic surveys than others. blue whale\nfin whale\nSpecies-specific factors influencing fixed\ngray whale\npassive acoustic surveys include these: Bryde’s whale\n• Frequency. Sounds below 1 kHz have right whales\npygmy right whale\nsignificantly less seawater absorp-\nminke whale\ntion loss than sounds above 10 kHz bowhead whale\n(François and Garrison, 1982), and humpback whale\nsei whale\nthus can be detected at greater dis-\ntrue dolphins, including....\ntances. The former frequencies are Cephalorhynchinae\ntypical of mysticetes, while the latter Globicephalinae\nDelphininae\nare typical of odontocetes. Figure 1\nSteninae*\nshows the frequency ranges of cetacean\nbeaked whales*\nvocalizations. river dolphins\n• Vocal behavior. Some cetaceans vocal- narwhal, beluga\n101 102 103 104 105\nize more frequently or more con-\nFrequency (Hz)\nsistently than others, making them\nbetter subjects for acoustic surveys. b. Frequencies of cetacean clicks\nVocalizing behavior varies with gender,\nsperm whale\nage, and season. For instance, adult beaked whales\nmales of many baleen whale species true dolphins, including....\nLissodelphinae*\nvocalize regularly and loudly during\nDelphininae\nthe breeding season. Globicephalinae\n• Source level. The larger cetaceans, Cephalorhynchinae\nriver dolphins\nincluding mysticete whales and sperm\nnarwhal, beluga\nwhales, produce intense vocalizations porpoises\nthat can be detected at distances of pygmy sperm whale\nseveral tens of kilometers on a single\n101 102 103 104 105\nFrequency (Hz)\nhydrophone (Barlow and Taylor, 2005;\nFigure 1. Known frequency ranges of cetacean sounds. large whales are listed by species, while\nStafford et al., in press) and much far-\ntoothed whales are grouped into families. The thick bar shows the range of the most common\nther—hundreds of kilometers—on types of vocalizations, while the thinner line shows recorded extremes of frequency. An aster-\nhydrophone arrays (Clark, 1995). For isk (*) indicates that the upper frequency is unknown because of recording equipment limita-\ntions. (a) tonal sounds—moans and whistles—with most baleen whale species shown separately.\ninstance, blue whale (Balaenoptera\n(b) echolocation clicks. Baleen whales do not produce high-frequency echolocation clicks, while\nmusculus) tonal calls have been mea- some toothed whales, dolphins, and porpoises do not produce tonal sounds.\nOceanography December 2007 39\n\nwhile their clicks have been measured 2002) or spectrogram (Mellinger et al., from the desired species (Mellinger et\nat 210–213 dB re 1 µPa RMS @ 1 m 2004a), and many more. al., 2004b; Munger et al., 2005). For a\n(Au et al., 1986). Whatever the method used, two issues survey of a common species such as fin\n• Directionality. High-frequency click are paramount. The first is determin- whales, for which determining an accu-\nsounds of some odontocetes are highly ing the type(s) of vocalizations to be rate index of call occurrence is para-\ndirectional. For instance, the direction- detected and the amount of variability in mount, detection can be configured to\nality index for bottlenose dolphins is these vocalizations. Some species, such as be relatively insensitive, so that there are\nat least 26 dB (Au, 1993), and sperm populations of fin whales (B. physalus), few wrong detections and a very high\nwhale sound emission is at least 35 dB have highly stereotyped vocalizations. proportion of correct detections. For a\nlouder in some directions than oth- These are amenable to detection using survey using the cue-counting statisti-\ners (Møhl et al., 2000). In contrast, one of the template-matching methods cal methods discussed below, it may\nlow-frequency baleen whale sounds mentioned above. Other species, such as be important to have the number of\nare believed to be emitted essentially common dolphins (Delphinus delphis), missed calls be as equal as possible to\nomnidirectionally, in part because the produce highly variable tonal sounds the number of false detections, so an\nlong wavelengths make directional (Oswald et al., 2004). These typically intermediate sensitivity is used.\nsound emission all but impossible. require band-limited energy summation\nfor detection, possibly followed by sta- APPliCAtiONS AND\nDetection of Vocalizations tistical classification techniques for spe- ANAlytiCAl MethODS\nVocalizations of a target species can be cies classification. Other species produce Determining range and\ndetected manually, with specialists listen- sounds with intermediate levels of vari- Seasonality\ning to sounds and/or looking at spec- ability that can be detected using neural Acoustic surveys have been used many\ntrograms to find occurrences of these networks (Potter et al., 1994) and filter times to measure the range and seasonal\nspecies’ vocalizations (Clark et al., 1996; banks (Urazghildiiev and Clark, 2006). occurrence of cetaceans. One advan-\nStafford et al., 1999, 2001). The volumes The second issue is the desired accu- tage of fixed passive acoustic methods is\nof data involved, however, more often racy of detection. In a perfect world, that they can be performed year round\ndictate using automatic detection. a detection method would find all at relatively low cost (e.g., Thompson\nMany methods for detection and instances of a certain call type, and and Freidl, 1982). Also, they can be car-\nclassification have been developed and nothing more. This ideal is never met, ried out in remote areas that are difficult\ntested. Most are based on detection in part because there are inevitably to survey other ways, such as far from\neither in a time series or in a spectro- faint calls that are difficult to classify, land (Clark, 1995; Stafford et al., 1999;\ngram, though other methods like wave- even by the best human specialists. The Nieukirk et al., 2004), polar regions\nlets are used as well. Techniques involved issue then becomes one of configuring (Širovic´ et al., 2004, 2007; Munger et\ninclude matched filters (Stafford, 1995), the detector’s sensitivity, or threshold, al., 2005; Moore et al., 2006; Stafford et\nenergy summation in a certain band to achieve a certain trade-off between al., 2007; and see “An Example Survey”\nfollowed by statistical classification missed calls (false negatives) and wrong below), or where weather is poor, and\n(Fristrup and Watkins, 1994; Oswald detections (false positives). For a survey visual surveys impossible, in some sea-\net al., 2004), image-processing tech- of a relatively rare species such as right sons (Mellinger et al., 2007).\nniques in spectrograms (Gillespie, 2004), whales (Eubalaena spp.), for which one In such studies, the number of vocal-\nspectrogram-based template match- wishes to miss no calls, detection can be izations in each time period (e.g., each\ning (Mellinger and Clark, 2000), neural configured at a relatively sensitive level day, each month, each ten-day period) is\nnetworks (Potter et al., 1994), wavelet- so that there are no or few missed calls counted, providing a rough indication of\nbased decomposition (Lopatka et al., but a large number of false detections; the number of animals in an area (e.g.,\n2005), band-limited amplitude in either the resulting detections can be checked Širovic´ et al., 2004). Another method is\nthe time series (Gillespie and Chappell, manually to determine which really were to measure the amount of energy in the\n40 Oceanography Vol. 20, No. 4\n\nfrequency band of the vocalization type, Abundance Nearly all of these methods require\ncorrect it for background noise level, and Using a set of detected vocalizations to acoustic estimation of group size, a field\nuse that as an indication of the number estimate the abundance of a species in a of study still in its infancy. Although\nof calls (e.g., Burtenshaw et al., 2004). given area may be done in several ways. many species have different vocal behav-\nUnfortunately, the connection between One is to derive the probability of detec- ior in the presence of different numbers\nthe number of vocalizations and the tion as a function of range. This prob- of conspecifics (e.g., Parks et al., 2005),\nnumber of animals is tenuous at best; ability density function (PDF) may then the relationship between vocal behavior\nsometimes a single animal produces be inverted using point-transect statis- and group size is rarely hard and fast,\na rapid sequence of vocalizations in a tical methods (Buckland et al., 2001), and the consequent errors in abundance\nshort time, sometimes only an occasional which essentially extrapolate from the estimates can be large.\nsound. To correct for these behavioral number of animals detected near the A second general approach relies on\ndifferences, many studies have assessed sensor to the number of animals pres- cue-counting statistical techniques. Here\nthe number of hours (or number of ent and vocalizing in some larger area. the total number of vocalizations—\ndays) that contain at least one vocaliza- The PDF can be estimated either by “cues”—is combined with an estimate of\ntion; this method greatly reduces the (1) acoustically locating the animals, the average cue rate per animal per unit\nbias of a rapidly vocalizing individual, such as recordings from multiple hydro- time to estimate the number of animals\nbecause one vocalization in a given time phones and using time-of-arrival differ- detected (Buckland et al., 2001). This\nperiod has as much weight as many in ences to estimate position (Cummings figure is then extrapolated to estimate\nthat period. However, this method also et al., 1964), (2) estimating range to the number of animals in the study area.\neffectively ignores multiple individuals a vocal animal using acoustic mul- This method requires detailed behavioral\nvocalizing in the same time period, so it tipath propagation effects (Cato, 1998; information on the rate of cue produc-\nis better suited for surveys of relatively McDonald and Fox, 1999; McDonald tion, and for most species little informa-\ntion is available.\nTo estimate density or abundance in\na given area using fixed instruments,\ninstrument positions should be chosen\n...there are still numerous hurdles to be\nwithout any bias toward any part of the\novercome before acoustic methods can be\narea. This can be done by random posi-\nreliably used to estimate abundance, which tioning of individual instruments or by\npositioning a regular grid of instruments\nis the ultimate goal for both ecosystem\nwith a randomly chosen origin.\nstudies and management purposes. Another approach is to perform joint\nvisual and acoustic surveys, then com-\nbine the results statistically to achieve\na better estimate than is possible for\nrare species, such as blue or right whales, and Moore, 2002; Širovic´ et al., 2007), either method alone (Fristrup and Clark,\nthan relatively common ones, such as fin or (3) using acoustic propagation mod- 1997). Notably, this has been done for\nwhales or common dolphins. In any case, els and distributions of source levels migrating bowhead whales, for which\nthese methods provide at best an index to estimate range from received lev- the visual observers count silent whales\nof occurrence, and are perhaps best els (Cato, 1991). Point-transect sam- and the acoustic observers determine\nemployed to determine when through- pling requires behavioral information the proportion of unseen whales (Clark\nout the year a given species is present in on rates of animal movement through et al., 1996; Raftery and Zeh, 1998).\nan area (see, e.g., Clark, 1995; Mellinger the monitored area to avoid double Another example comes from a survey\net al., 2004a, 2004b; Munger et al., 2005). counting of individuals. of sperm whale abundance in the east-\nOceanography December 2007 41\n\nern Pacific, for which the visual com- area in December 2006. mates of the number of animals present\nponent of the survey performed best at A cursory review of spectrograms beyond a minimum of animals vocal-\nestimating group size, while the acoustic revealed known signature calls of blue, izing at the same time. Nevertheless, in\ncomponent performed best at detecting fin, and humpback whales as well as remote regions, this method can provide\ngroups (Barlow and Taylor, 2005). earthquakes and ice noise. To assess useful, novel information on species\nAn additional use of acoustic moni- the seasonal presence of Antarctic blue occurrence that can be achieved with a\ntoring in joint surveys is acoustic species whales, data were automatically scanned few, widely spaced instruments.\nidentification (Oswald et al., 2003). This by use of a spectrogram correlation To better estimate the number of\nbecomes useful when shipboard surveys routine (Mellinger and Clark, 2000) to vocal animals of a particular species in\ntarget species that are difficult to identify detect their characteristic 28-Hz calls. In a region, the ability to track the animals\nvisually at a distance. This method has order to minimize false detections, the is required. This method provides esti-\nbeen used on eastern tropical Pacific dol- detection threshold was set sufficiently mates of the vocal behavior of individu-\nphin abundance cruises where dolphin high that only these calls were detected. als as well as an idea of how many other\npods often show ship avoidance at dis- Figure 3 shows the seasonal occurrence vocal animals are present at the same\ntances greater than those at which they of blue whales recorded by the Drake time (e.g., Širovic´ et al., 2007). To do\ncan be reliably identified using visual Passage hydrophone. This type of analy- this requires a minimum of three, and\ncues. The ability to use acoustic char- sis provides information on the sea- ideally four, instruments spaced closely\nacteristics to determine species/species sonal and geographic patterns of calling enough that the same call is recorded on\ncomposition may determine whether or whales but cannot provide reliable esti- all of them but widely enough that ani-\nnot the ship turns towards the target spe- mals may be located within a broad area.\ncies or continues along its track to search The six instruments in the Bransfield\nfor that species (Oswald et al., 2007). Strait region fit these requirements for\n-75°\n-70° -65° -60° -55°\n-50°\n-50° South -50°\nAN exAMPle SurVey America\nIn November 2005, with support from\nN\nau\nO\nto\nA\nn\nA\no\n’s\nm\nO\no\nf\nu\nfi\ns\nc\nh\ne\ny\no\nd\nf\nr\nO\nop\nc\nh\nea\no\nn\nn e\nE\ns\nx\nw\npl\ne\no\nr\nr\ne\na\nd\nti\ne\no\np\nn\nlo\n, s\ny\ni\ne\nx\nd\n-55°\nDrake\nPassage\n-55°\nF\nd\ni\no\ng\nt\nu\ns\nr e\nre\n2\np\n.\nr\nTh\nese\ne\nn\ns\nt\nt u\nsi\nd\nte\ny\ns\na\no\nre\nf\na\na u\no\nt\nff\no n\nth\no\ne\nm\nA\no\nn\nu\nt\ns\na\nh\nrc\ny\nt\nd\nic\nro\nP\np\ne\nh\nn\no\nin\nn\ns\ne\nu\ni\nl\nn\na.\ns t\nB\nr\nl\nu\na\nm\nck\ne nts.\nin the Bransfield Strait near the Antarctic\n-60° -60°\nPeninsula and one in the Drake Passage\nto monitor seismicity and large whale -65° -65°\noccurrence around the South Shetland -62° P A e n n t i a n r s c u ti l c a -60° -58° -56°\nIslands (Figure 2). The bottom-anchored -70° -70°\nmoorings placed the instruments in the -75° -70° -65° -60° -55° -50°\nsound channel. Six of the seven instru-\n-62° -62°\nments recorded acoustic data from\n0.1–110 Hz and the seventh sampled\nfrom 0.1–840 Hz. Although the band-\nwidth of the first six instruments was\ns\nc\nu\nal\nf\nl\nfi\ns,\nc i\nt\ne\nh\nn\ne\nt\ns\nt\ne\no\nv e\nr\nn\nec\nth\nor\nw\nd\na\nb\ns\nl u\nse\ne\nt\na\nt\nn\no\nd\nr e\nfi\nc\nn\no r\nw\nd\nh\no\na\nt\nl\nh\ne\ner\nBransfield\nStrait\nspecies known to occur in the area, such -63° -63°\nas humpback (Megaptera novaeangliae)\nand right whales. The instruments were\nrecovered and redeployed in the same\n-62° -60° -58° -56 °\n42 Oceanography Vol. 20, No. 4\n\nboth blue and fin whales. A 15-hour Seasonal occurrence of blue whale vocalizations\ntrack from a single vocalizing blue 14000\nwhale shows movement from the north-\n12000\neast to the southwest in the array on\nFebruary 22, 2006 (Figure 4). 10000\nThese are only two examples of how a\n8000\nlong-term multi-instrument data set can\nbe exploited. Other possibilities include\n6000\ncomparing acoustic data with ice cover\nto determine if the latter appears to 4000\ninfluence the former. Multiyear acoustic\n2000\nmonitoring can provide information on\ninterannual and interseasonal patterns in 0\ncall reception of all vocal species. These Dec- 0 5 Ja n- 0 6 Fe b- 0 6 Mar- 0 6 A pr- 0 6 May- 0 6 J u n- 0 6 J ul- 0 6 A ug- 0 6 Se p- 0 6 Oct- 0 6 N ov- 0 6\npatterns may then be correlated with\nFigure 3. Seasonal occurrence of blue whale vocalizations at the southwestern hydro-\nlong-term measurements of physical and\nphone site. Blue whale calls occur in the largest numbers late in the austral autumn\nlower-trophic parameters to gain a better (April–May).\nunderstanding of the factors affecting,\nand affected by, cetaceans.\nDiSCuSSiON\nPassive acoustic methods have been\nincreasingly employed both as stand-\nalone surveys and in conjunction with\nvisual surveys. Their utility has become\nclear over the past decade, and many\ncetacean surveys now include some\nacoustic component. However, despite\nthe proliferation of such surveys, there\nare still numerous hurdles to be over-\ncome before acoustic methods can be\nFigure 4. track of a single blue whale moving through the study area. Whale calls were repeatedly\nreliably used to estimate abundance, located by time-of-arrival differences at the hydrophones; tracking was possible because it was the\nonly vocalizing blue whale in the area.\nwhich is the ultimate goal for both eco-\nsystem studies and management pur-\nposes. Perhaps the biggest hurdle is a\ngeneral lack of understanding of the\nbehavioral context of sound produc- at shorter distances. Therefore, assess- information includes distribution curves\ntion for many species, compounded ment modalities may need to be tailored for seasonal, sex, and age-class bias in\nby interspecies differences in acoustic for individual species or suites of spe- sound production, frequency range of\nbehavior. For instance, odontocetes tend cies. One way to alleviate this problem species-specific sound characteristics,\nto be highly vocal compared to baleen is to develop statistical models similar and source sound level variation.\nwhales; however, because their vocaliza- to those employed for visual surveys to There is a need to standardize meth-\ntions are higher in frequency than those account for variability in sound produc- ods among different projects. For exam-\nof most baleen whales, they are detected tion and reception distance. Required ple, recordings of blue whales around\nOceanography December 2007 43\n\nthe Antarctic have been made from The manuscript was improved thanks to ter hydrophone arrays for scientific research on\nwhales. Scientific Report, International Whaling\nnear-bottom hydrophones (Širovic´ et al., critical reviews by Jay Barlow and Sofie\nCommission 44:210–213.\n2004, 2007), near-surface sonobuoys Van Parijs. This work was supported by Clark, C.W., R.A. Charif, S.G. Mitchell, and J. Colby.\n(Rankin et al., 2005), and autonomous NOAA’s Office of Ocean Exploration 1996. Distribution and behavior of the bowhead\nwhale, Balaena mysticetus, based on analysis of\nhydrophones moored in the deep sound and by Naval Postgraduate School award\nacoustic data collected during the 1993 spring\nchannel (discussed in this article). A #N00244-07-1-0005. This is PMEL con- migration off Point Barrow, Alaska. Scientific\nReport, International Whaling Commission\ncomparison of data from these projects tribution #3116.\n46:541–552.\nwould require standardizing among Clark, C.W., D. Gillespie, D.P. Nowacek, and S.E. Parks.\nthem, a procedure that would necessar- reFereNCeS 2007. Listening to their world: Acoustics for moni-\ntoring and protecting right whales in an urban-\nily rely on understanding the acoustic Au, W.W.L. 1993. The Sonar of Dolphins. Springer- ized world. Pp. 333–357 in The Urban Whale, S.D.\nVerlag, New York, 277 pp.\npropagation environment in which the Kraus and R.M. Rolland, eds, Harvard University\nAu, W.W.L., P.W.B. Moore, and D. Pawloski. 1986.\nPress, Cambridge, MA.\nsounds travel from source to receiver. Echolocation transmitting beam of the Atlantic\nCummings, W.C., and P.O. Thompson. 1971.\nbottlenose dolphin. Journal of the Acoustical Society\nThe Acoustical Society of America Underwater sounds from the blue whale,\nof America 80:688–691.\nBalaenoptera musculus. Journal of the Acoustical\nhas recently convened a working group Barlow, J., and B.L. Taylor. 2005. Estimates of sperm\nSociety of America 50:1,193–1,198.\nto develop standards for hardware and whale abundance in the northeastern temperate Cummings, W.C., B.D. Brahy, and W.F. Herrnkind.\nPacific from a combined acoustic and visual survey.\nsoftware for marine mammal moni- 1964. The occurrence of underwater sounds of\nMarine Mammal Science 21:429–445.\nbiological origin off the west coast of Bimini,\ntoring during seismic surveys. At pres- Barnes, C.R., M.M.R. Best, B.D. Bornhold, S.K.\nBahamas. Pp. 27–43 in Marine Bio-acoustics, W.N.\nJuniper, B. Pirenne, and P. Phibbs. 2007. The\nent, only recommendations have been Tavolga, ed., Pergamon, NY.\nNEPTUNE Project—a cabled ocean observatory\nFletcher, S., B.J. Le Boeuf, D.P. Costa, P.L. Tyack, and\nmade, but these include the need to in the NE Pacific: Overview, challenges and sci-\nS.B. Blackwell. 1996. Onboard acoustic recording\ndefine necessary metadata and data entific objectives for the installation and opera- from diving northern elephant seals. Journal of the\ntion of Stage I in Canadian waters. Pp. 308–313\nAcoustical Society of America 100:2,531–2,539.\nneeded while at sea, the metrics needed\nin Symposium on Underwater Technology and\nFox, C.G., H. Matsumoto, and T.K.A. Lau. 2001.\nto quantify raw acoustic data, and infor- Workshop on Scientific Use of Submarine Cables Monitoring Pacific Ocean seismicity from an\nand Related Technologies, Tokyo, Japan. IEEE Press,\nmation that should be reported in the autonomous hydrophone array. Journal of\nPiscataway, NJ, doi:10.1109/UT.2007.370809. Geophysical Research 106(B3):4,183–4,206.\nopen literature (Aaron Thode, Scripps Baumgartner, M.F., and B.R. Mate. 2003. Summertime François, R.E., and G.R. Garrison. 1982. Sound\nInstitution of Oceanography, pers. comm., foraging ecology of North Atlantic right whales. absorption based on ocean measurements: Part\nMarine Ecology Progress Series 264:123–135. II: Boric acid contribution and equation for total\nDecember 2006).\nBuckland, S.T., D.R. Anderson, K.P. Burnham, J.L. absorption. Journal of the Acoustical Society of\nThe use of passive acoustics world- Laake, D.L. Borchers, and L. Thomas. 2001. America 72:1,879–1,890.\nwide, coupled with effective standard- Introduction to Distance Sampling: Estimating Fristrup, K.M., and C.W. Clark. 1997. Combining\nAbundance of Biological Populations (Oxford visual and acoustic survey data to enhance density\nization and development of statistical University, Oxford), 448 pp. estimation. Scientific Report, International Whaling\nmethods for estimating populations Burgess, W.C., P.L. Tyack, B.J. Le Boeuf, and D.P. Costa. Commission 47:933–936.\n1998. A programmable acoustic recording tag and Fristrup, K.M., and W.A. Watkins. 1994. Marine ani-\nacoustically, should result in more and\nfirst results from northern elephant seals. Deep-Sea mal sound classification. Technical Report WHOI-\nbetter data on cetacean species and pop- Research Part II 45:1,327–1,351. 94-13, Woods Hole Oceanographic Institution,\nBurtenshaw, J.C., E.M. Oleson, J.A. Hildebrand, M.A. Woods Hole, MA. 32 pp.\nulations and should ultimately lead to\nMcDonald, R.K. Andrew, B.M. Howe, and J.A. Gillespie, D. 2004. Detection and classification of\nincreased understanding of their role in Mercer. 2004. Acoustic and satellite remote sensing right whale calls using an edge detector operating\nmarine ecosystems. of blue whale seasonality and habitat in the north- on a smoothed spectrogram. Canadian Acoustics\neast Pacific. Deep Sea Research Part II 51:967–986. 32:39–47.\nCalupca, T.A., K.M. Fristrup, and C.W. Clark. 2000. A Gillespie, D., and O. Chappell. 2002. An automatic sys-\nACKNOWleDgeMeNtS compact digital recording system for autonomous tem for detecting and classifying the vocalisations\nbioacoustic monitoring. Journal of the Acoustical of harbour porpoises. Bioacoustics 13:37–61.\nThanks to Joe Haxel, Matt Fowler, and\nSociety of America 108:2582(A). Janik, V.M. 2000. Source levels and the estimated active\nSara Heimlich for assistance config- Cato, D. 1991. Songs of humpback whales: The space of bottlenose dolphin (Tursiops truncatus)\nuring, deploying, and recovering the Australian perspective. Memoirs of the Queensland whistles in the Moray Firth, Scotland. Journal of\nMuseum 30:277–290. Comparative Physiology A 186:673–680.\nhydrophone instruments used in this\nCato, D.H. 1998. Simple methods of estimating Johnson, D.H., and D.E. Dudgeon. 1993. Array Signal\nproject. Thanks also to Minkyu Park source levels and locations of marine animal Processing: Concepts and Techniques. Prentice-Hall,\nsounds. Journal of the Acoustical Society of America Englewood Cliffs, NJ, 533 pp.\nand the Korea Polar Research Institute\n104:1,667–1,678. Johnson, M.P., and P.L. Tyack. 2003. A digital acous-\n(KOPRI) for ship time in Antarctica. Clark, C.W. 1995. Application of U.S. Navy underwa- tic recording tag for measuring the response of\n44 Oceanography Vol. 20, No. 4\n\nwild marine mammals to sound. IEEE Journal of Moore, and J.A. Hildebrand. 2005. Performance of tion range in the Southern Ocean. Journal of the\nOceanic Engineering 28:3–12. spectrogram correlation in detecting right whale Acoustical Society of America 122:1,208–1,215.\nLammers, M.O., R.E. Brainard, W.W.L. Au, T.A. calls in long-term recordings from the Bering Sea. Stafford, K.M. 1995. Characterization of blue whale\nMooney, and K. Wong. In press. Ecological Canadian Acoustics 33:25–34. calls from the northeast Pacific and develop-\nAcoustic Recorder (EAR) for long-term monitor- Nieukirk, S.L., K.M. Stafford, D.K. Mellinger, R.P. ment of a matched filter to locate blue whales on\ning of biological and anthropogenic sounds on Dziak, and C.G. Fox. 2004. Low-frequency whale U.S. Navy SOSUS (SOund SUrveillance System)\ncoral reefs and other marine habitats. Journal of the and airgun sounds recorded from the mid-Atlantic arrays. Master’s Thesis, Oregon State University,\nAcoustical Society of America. Ocean. Journal of the Acoustical Society of America Corvallis, OR.\nLopatka, M., O. Adam, C. Laplanche, J. Zarzycki, and 115:1,832–1,843. Stafford, K.M., C.G. Fox, and D.S. Clark. 1998. Long-\nJ.-F. Motsch. 2005. An attractive alternative for ORION Program Office. 2007. ORION’s ocean obser- range detection and localization of blue whale calls\nsperm whale click detection using the wavelet vatories initiative conceptual network design. in the northeast Pacific using military hydrophone\ntransform in comparison to the Fourier spectro- Technical report, Joint Oceanographic Institutions, arrays. Journal of the Acoustical Society of America\ngram. Aquatic Mammals 31:463–467. Washington, D.C. 35 pp. 104:3,616–3,625.\nMadsen, P.T., R. Payne, N.U. Kristiansen, M. Wahlberg, Oswald, J.N., J. Barlow, and T. Norris. 2003. Acoustic Stafford, K.M., S.L. Nieukirk, and C.G. Fox. 1999. Low-\nI. Kerr, and B. Møhl. 2002. Sperm whale sound identification of nine delphinid species in the east- frequency whale sounds recorded on hydrophones\nproduction studied with ultrasound time/depth ern tropical Pacific Ocean. Marine Mammal Science moored in the eastern tropical Pacific. Journal of\nrecording tags. Journal of Experimental Biology 19:20–37. the Acoustical Society of America 106:3,687–3,698.\n205:1,899–1,906. Oswald, J.N., S. Rankin, and J. Barlow. 2004. The effect Stafford, K.M., S.L. Nieukirk, and C.G. Fox. 2001.\nMcDonald, M.A., and C.G. Fox. 1999. Passive acoustic of recording and analysis bandwidth on acoustic Geographic and seasonal variation of blue whale\nmethods applied to fin whale population density identification of delphinid species. Journal of the calls in the North Pacific. Journal of Cetacean\nestimation. Journal of the Acoustical Society of Acoustical Society of America 116:3,178–3,185. Research and Management 3:65–76.\nAmerica 105:2,643–2,651. Oswald, J.N., S. Rankin, J. Barlow, and M.O. Lammers. Stafford, K.M., S.E. Moore, M. Spillane, and S.\nMcDonald, M.A., and S.E. Moore. 2002. Calls recorded 2007. A tool for real-time acoustic species iden- Wiggins. 2007. Gray whale calls recorded near\nfrom North Pacific right whales (Eubalaena japon- tification of delphinid whistles. Journal of the Barrow, Alaska, throughout the winter of 2003–04.\nica) in the eastern Bering Sea. Journal of Cetacean Acoustical Society of America 122:587–595. Arctic 60:167–172.\nResearch and Management 4:261–266. Parks, S.E., and P.L. Tyack. 2005. Sound production by Stafford, K.M., D.K. Mellinger, S.E. Moore, and C.G.\nMcDonald, M.A., J. Calambokidis, A.M. Teranishi, and North Atlantic right whales (Eubalaena glacialis) Fox. In press. Seasonal variability and detection\nJ.A. Hildebrand. 2001. The acoustic calls of blue in surface active groups. Journal of the Acoustical range modeling of baleen whale calls in the Gulf of\nwhales off California with gender data. Journal of Society of America 117:3,297–3,306. Alaska, 1999–2002. Journal of the Acoustical Society\nthe Acoustical Society of America 109:1,728–1,735. Potter, J.R., D.K. Mellinger, and C.W. Clark. 1994. of America.\nMellinger, D.K., and J. Barlow. 2003. Future directions Marine mammal call discrimination using artificial Thode, A.M., G.L. D’Spain, and W.A. Kuperman. 2000.\nfor marine mammal acoustic surveys: Stock assess- neural networks. Journal of the Acoustical Society of Matched-field processing, geoacoustic inversion,\nment and habitat use. Report of a workshop held America 96:1,255–1,262. and source signature recovery of blue whale vocal-\nin La Jolla, CA, November 20–22, 2002. Technical Raftery, A.E., and J.E. Zeh. 1998. Estimating bowhead izations. Journal of the Acoustical Society of America\ncontribution No. 2557, NOAA Pacific Marine whale population size and rate of increase from 107:1,286–13,00.\nEnvironmental Laboratory, Seattle, WA, 45 pp. the 1993 census. Journal of the American Statistical Thompson, P.O., and W.A. Friedl. 1982. A long-term\nMellinger, D.K., and C.W. Clark. 2000. Recognizing Association 93:451–463. study of low frequency sounds from several species\ntransient low-frequency whale sounds by spectro- Rankin, S., D.K. Ljungblad, C.W. Clark, and H. Kato. of whales off Oahu, Hawaii. Cetology 45:1–19.\ngram correlation. Journal of the Acoustical Society of 2005. Vocalizations of Antarctic blue whales, Tynan, C.T. 2004. Cetacean populations on the SE\nAmerica 107:3,518–3,529. Balaenoptera musculus intermedia, recorded dur- Bering Sea shelf during the late 1990s: Implications\nMellinger, D.K., K.M. Stafford, and C.G. Fox. 2004a. ing the 2001–2002 and 2002–2003 IWC-SOWER for decadal changes in ecosystem structure and\nSeasonal occurrence of sperm whale (Physeter circumpolar cruises, Area V, Antarctica. Journal of carbon flow. Marine Ecology Progress Series\nmacrocephalus) sounds in the Gulf of Alaska, 1999– Cetacean Research and Management 7:13–20. 272:281–300.\n2001. Marine Mammal Science 20:48–62. Rankin, S., T.F. Norris, M.A. Smultea, C. Oedekoven, Urazghildiiev, I.R., and C.W. Clark. 2006. Acoustic\nMellinger, D.K., K.M. Stafford, S.E. Moore, L. Munger, A.M. Zoidis, E. Silva, and J. Rivers. 2007. A detection of North Atlantic right whale con-\nand C.G. Fox. 2004b. Detection of North Pacific visual sighting and acoustic detections of minke tact calls using the generalized likelihood ratio\nright whale (Eubalaena japonica) calls in the Gulf whales, Balaenoptera acutorostrata (Cetacea: test. Journal of the Acoustical Society of America\nof Alaska. Marine Mammal Science 20:872–879. Balaenopteridae), in nearshore Hawaiian waters. 120:1,956-1,963.\nMellinger, D.K., S.L. Nieukirk, H. Matsumoto, S.L. Pacific Science 61:395–398. Wiggins, S.M. 2003. Autonomous acoustic record-\nHeimlich, R.P. Dziak, J. Haxel, M. Fowler, C. Redfern, J.V., M.C. Ferguson, E.A. Becker, K.D. ing packages (ARPs) for long-term monitoring of\nMeinig, and H.V. Miller. 2007. Seasonal occurrence Hyrenbach, C. Good, J. Barlow, K. Kaschner, M.F. whale sounds. Marine Technology Society Journal\nof North Atlantic right whales (Eubalaena glacialis) Baumgartner, K.A. Forney, L.T. Ballance, and oth- 37(2):13–22.\nat two sites on the Scotian Shelf. Marine Mammal ers. 2006. Techniques for cetacean-habitat model-\nScience 23:856–867. ing. Marine Ecology Progress Series 310:271–295.\nMøhl, B., M. Wahlberg, P.T. Madsen, L.A. Miller, Širovic´, A., J.A. Hildebrand, S.M. Wiggins,\nand A. Surlykke. 2000. Sperm whale clicks: M.A. McDonald, S.E. Moore, and D. Thiele.\nDirectionality and source level revisited. Journal of 2004. Seasonality of blue and fin whale calls\nthe Acoustical Society of America 107:638–648. and the influence of sea ice in the Western\nMoore, S.E., K.M. Stafford, D.K. Mellinger, and J.A. Antarctic Peninsula. Deep-Sea Research Part II\nHildebrand. 2006. Listening for large whales in off- 51:2,327–2,344.\nshore waters of Alaska. BioScience 56:49–55. Širovic´, A., J.A. Hildebrand, and S.M. Wiggins. 2007.\nMunger, L.M., D.K. Mellinger, S.M. Wiggins, S.E. Blue and fin whale call source levels and propaga-\nOceanography December 2007 45", "affiliations": [{"university": "", "country": "", "discipline": ""}, {"university": "", "country": "", "discipline": ""}], "species_categories": ["Marine Mammal", "Other"], "specialized_species": ["blue whale", "fin whale", "gray whale", "Bryde’s whale", "right whales", "pygmy right whale", "minke whale", "bowhead whale", "humpback whale", "sei whale", "sperm whale", "narwhal", "beluga", "common dolphins", "bottlenose dolphin", "elephant seals"], "computational_stages": ["Data Collection", "Pre-processing", "Meaning Identification"], "linguistic_features": ["Semanticity", "Vocal Auditory Channel and Turn-taking", "Discreteness and Syntax"], "updated_at": "2026-01-27T21:01:48.666563", "committed_at": "2026-01-27T21:01:50.343878"}
{"id": "a60dc404-47d7-4bbb-9982-ed1e71bb6f1b", "doi": "https://doi.org/10.1101/2022.10.12.511740", "title": "Bioacoustic Event Detection with Self-Supervised Contrastive Learning", "authors": ["Peter C. Bermant", "Leandra Brickson", "Alexander Titus"], "year": "2022", "journal": "", "abstract": "ABSTRACT While deep learning has revolutionized ecological data analysis, existing strategies often rely on supervised learning, which is subject to limitations on real-world applicability. In this paper, we apply self-supervised deep learning methods to bioacoustic data to enable unsupervised detection of bioacoustic event boundaries. We propose a convolutional deep neural network that operates on the raw waveform directly and is trained in accordance with the Noise Contrastive Estimation principle, which enables the system to detect spectral changes in the input acoustic stream. The model learns a representation of the input audio sampled at low frequency that encodes information regarding dissimilarity between sequential acoustic windows. During inference, we use a peak finding algorithm to search for regions of high dissimilarity in order to identify temporal boundaries of bioacoustic events. We report results using these techniques to detect sperm whale ( Physeter macrocephalus ) coda clicks in real-world recordings, and we demonstrate the viability of analyzing the vocalizations of other species (e.g. Bengalese finch syllable segmentation) in addition to other data modalities (e.g. animal behavioral dynamics, embryo development and tracking). We find that the self-supervised deep representation learning-based technique outperforms established threshold-based baseline methods without requiring manual annotation of acoustic datasets. Quantitatively, our approach yields a maximal R-value and F1-score of 0.887 and 0.876, respectively, and an area under the Precision-Recall curve (PR-AUC) of 0.917, while a baseline threshold detector acting on signal energy amplitude returns a maximal R-value and F1-score of 0.620 and 0.576, respectively, and a PR-AUC of 0.571. We also compare with a threshold detector using preprocessed (e.g. denoised) acoustic input. The findings of this paper establish the validity of unsupervised bioacoustic event detection using deep neural networks and self-supervised contrastive learning as an effective alternative to conventional techniques that leverage supervised methods for signal presence indication. Providing a means for highly accurate unsupervised detection, this paper serves as an important step towards developing a fully automated system for real-time acoustic monitoring of bioacoustic signals in real-world acoustic data. All code and data used in this study are available online.", "pdf_filename": "https_doi.org_10.1101_2022.10.12.511740.pdf", "data_source": "subset", "status": "saved", "created_at": "2026-01-27T20:15:28.007549", "analysis_notes": "bioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\nBioacoustic Event Detection with Self-Supervised\nContrastive Learning\nPeter C. Bermant1,*, Leandra Brickson1, and Alexander J. Titus1,2,3\n1ColossalBiosciences,Austin,TX,USA\n2FormBio,Austin,TX,USA\n3InternationalComputerScienceInstitute,Berkeley,CA,USA\n*publications@colossal.com\nABSTRACT\nWhiledeeplearninghasrevolutionizedecologicaldataanalysis,existingstrategiesoftenrelyonsupervisedlearning,whichis\nsubjecttolimitationsonreal-worldapplicability. Inthispaper,weapplyself-superviseddeeplearningmethodstobioacoustic\ndatatoenableunsuperviseddetectionofbioacousticeventboundaries. Weproposeaconvolutionaldeepneuralnetworkthat\noperatesontherawwaveformdirectlyandistrainedinaccordancewiththeNoiseContrastiveEstimationprinciple,which\nenablesthesystemtodetectspectralchangesintheinputacousticstream. Themodellearnsarepresentationoftheinput\naudiosampledatlowfrequencythatencodesinformationregardingdissimilaritybetweensequentialacousticwindows. During\ninference,weuseapeakfindingalgorithmtosearchforregionsofhighdissimilarityinordertoidentifytemporalboundariesof\nbioacousticevents. Wereportresultsusingthesetechniquestodetectspermwhale(Physetermacrocephalus)codaclicksin\nreal-worldrecordings,andwedemonstratetheviabilityofanalyzingthevocalizationsofotherspecies(e.g. Bengalesefinch\nsyllablesegmentation)inadditiontootherdatamodalities(e.g. animalbehavioraldynamics,embryodevelopmentandtracking).\nWe find that the self-supervised deep representation learning-based technique outperforms established threshold-based\nbaselinemethodswithoutrequiringmanualannotationofacousticdatasets. Quantitatively,ourapproachyieldsamaximal\nR-valueandF1-scoreof0.887and0.876,respectively,andanareaunderthePrecision-Recallcurve(PR-AUC)of0.917,while\nabaselinethresholddetectoractingonsignalenergyamplitudereturnsamaximalR-valueandF1-scoreof0.620and0.576,\nrespectively,andaPR-AUCof0.571. Wealsocomparewithathresholddetectorusingpreprocessed(e.g. denoised)acoustic\ninput. Thefindingsofthispaperestablishthevalidityofunsupervisedbioacousticeventdetectionusingdeepneuralnetworks\nandself-supervisedcontrastivelearningasaneffectivealternativetoconventionaltechniquesthatleveragesupervisedmethods\nforsignalpresenceindication. Providingameansforhighlyaccurateunsuperviseddetection,thispaperservesasanimportant\nsteptowardsdevelopingafullyautomatedsystemforreal-timeacousticmonitoringofbioacousticsignalsinreal-worldacoustic\ndata. Allcodeanddatausedinthisstudyareavailableonline.\n1 Introduction\nAs conservation and management strategies encompass progressively more extreme solutions, ranging from species de-\nextinctiontotranslationofnon-humancommunication1,novelcomputationaltechniquescanprovideimprovedmethodsfor\nprocessing large quantities of multimodal ecological data. In recent years, advances in machine learning (ML) and deep\nlearning(DL),inparticular,haverevolutionizedtheanalysisofbioacousticdata,facilitatingthedevelopmentofautomated\npipelinesforprocessingdataandcontributingtoenhancedconservationtacticsacrossdiversetaxa. Todate,applicationsofDL\ntobioacousticstendtorelyonheavilysupervisedmethodsdemandinglargemanuallylabeleddatasets2,andtheseapproaches\noftentreatdetectionasabinaryclassificationtask3thatismorereflectiveofpresenceindicationthaneventdetection2. Inthis\npaper,weproposeaself-supervisedrepresentationlearning-basedapproachtobioacousticeventdetectionandsegmentation\nthatcanpredicttemporalonsetsandoffsetsofbioacousticsignalsinafullyunsupervisedregime.\nRecentdevelopmentsinobservationhardwareandmethodsnowofferecologistsandbiologistsimmensequantitiesofdata\ncapturedviahigh-endcameras,staticmicrophones,biologgingdevices,andsatellites,amongothers2. Theunprecedented\namounts of high-quality data supersede the conventional approach of manual annotation and demand the application of\nnovelcomputationalmethodstodiscoverpatternsofanimalbehaviorwithimportantecologicalimplications. Tothisend,\ncontemporarymethodsofdataanalysis–andbioacousticdataanalysis,inparticular–oftenleverageDLanddeepneuralnetworks\n(DNNs).\nIngeneral,implementationsofMLtoecologicaldataanalysisheavilydependonsupervisedtechniquesinwhichsufficiently\nlargetrainingdatasetshavebeencuratedwithaccurateinformation,oftencorrespondingtolabelssuchaspresence,location,\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\norotheridentifyingfeaturesofaspecies,object,sound,etc. Forexample,earlyresearchstudiesappliedDLtospermwhale\nbioacoustics4, usingsupervisedlearningtotrainDNNstoperformdetectionandclassificationtasksofinputspectrogram\nfeaturerepresentations,requiringmanuallylabeleddatasetsannotatedaccordingtorelevanttargetsincludingbutnotlimitedto\nsignalpresence,clanmembership,individualidentity,andcalltype. Large-scalestudiesoftenemployConvolutionalNeural\nNetwork(CNN)architecturestocarryoutsuperviseddetectionorclassificationtasksacrossdiversespecies. Theseinclude\ndetectionofhumpbackwhalevocalizationsinpassiveacousticmonitoringdatasets5;detection,classification,andcensusingof\nbluewhalesounds6;avianspeciesmonitoringbasedonaCNNtrainedtopredictthespeciesclasslabelgiveninputaudio7;\npresenceindicationofHainanesegibbonsusingaResNet-basedCNN8;and,recently,detectionandclassificationofmarine\nsoundsourcesusinganimage-basedapproachtospectrogramclassification9. However,suchsupervisedlearningapproaches\nremainlimitedintheirscope,hinderingtheircapacitytobedeployedinreal-timedataprocessingpipelines. Asalientlimitation\nremainsthedependencyonlargeamountsofhigh-qualitymanuallyannotateddatasets2,aproblemcompoundedbyitsreliance\nontrainedexperts,thelaboriousnature(i.e. extensivetimeandresources)involvedinmanuallabeling,andthepotentialfor\nbias,variability,oruncertaintyduringthelabelingprocessassociatedwithvariablehumanconfidenceandperception,especially\nacrosscohortsofindividualannotators10,11. Further,manualdataannotationremainsexpensiveintermsoftime2,expertise,\nandfinancialcost12,whichhasmotivatedcitizenscience-basedcrowdsourcingeffortstoannotatelargedatasetsatscale,though\ntheseapproachesaresimilarlyaccompaniedbyerrorandlimitations13,14. Relianceonmanualannotationsrepresentsakey\nconstraintonsupervisedDLmethodsforbioacousticdataanalysis,makingunsupervisedapproachesanattractivealternative\nwiththepotentialtoprovidegreaterinsightintoanimalcommunicationandbehavior.\nMorefundamentally,however,thefoundationalprinciplesuponwhichDNNclassifier-baseddetectorsareconstructedlimits\ntheirapplicabilitytoreal-worldecologicaldataanalysis. DNN-basedbioacousticdetectorsoftenposethedetectionproblem\nasabinaryclassificationproblem5inwhichaneuralnetworklearnsdiscriminativefeaturerepresentationsofinputacoustic\nfeatures(e.g. rawwaveforms,handcraftedacousticparameters,spectrograms,miscellaneoustime-frequencyrepresentations),\nenablingthemodeltopredictabinaryclasslabeldenotingsignalornon-signal(i.e. background). Whileattemptstoaddress\nthis shortcoming exist, often by employing smaller window (i.e. acoustic segment) sizes to artificially improve temporal\nresolution15,theseapproachescontinuetotreatdetectionasapresenceindicationtask,unabletolocalizethesignaldirectly\nwithinthewindow. Further,thisworkaroundmethodofsmallerwindowsisaccompaniedbyitsowndisadvantages,mainly\nanimpairedcomputationalefficiencyduetotheneedforoverlappingwindowstoaccountforsignalsthatcouldoccuratthe\ntransitionboundarybetweenadjacentspectrograms2. Theinabilitytopreciselylocatesignalinacontinuousrecordingrenders\nitchallengingtoaddressdownstreamquestionsofecologicaland/oranimalbehavioralimportance. Thisisespeciallytruein\nunsupervisedregimeswhenlabels(individualidentity,calltype,etc.) maynotbereadilyaccessible. Forinstance,unsupervised\nclustering-basedtechniquesforcall-typeclassification16oftenexpectmanuallysegmentedcalls,whichremainachallengeto\nextractfromDNN-basedpresenceindicators.\nAsanalternativeapproach,wetreatthedetectionproblemasasegmentationprobleminwhichweaimtopredicttemporal\nonsetsand/oroffsetsofbioacousticeventsignalsgivenacontinuousstreamofacousticdata. Whilepreviousstudieshave\nattemptedbothsupervised17–19 andunsupervised20,21 bioacousticsoundeventdetection(SED)andsegmentation,modern\nDNN-based–andself-supervised–methodsremainunderexplored3. Wetakeinspirationfromrecentadvancesinrepresentation\nlearningandself-supervisedcontrastivelearning22–25,especiallythoseappliedtohumanphonemeboundarydetectionand\nsegmentation26,27. WeproposeafullyunsupervisedDNN-basedsystemforbioacousticsignaleventdetectionthatleverages\nself-supervisedcontrastivelearningtomapinputaudiotoalearnablefeaturerepresentationthatencodesinformationregarding\nspectralboundaries,transitions,andchangesthatdistinguishnon-signalbackgroundfromsignalevents.\nFinally,weadjustourmethodstoaccountforuniquechallengesincurredbyprocessingreal-worldbioacousticdatarecorded\ninthenativeenvironment. Inparticular,akeychallengeremainsthepresenceofsignificantbackgroundnoiseinreal-world\nbioacousticrecordingsandtheassociateddifficultiesDNNshaveinprocessingnoisydata–oftenrequiringmodificationsto\nDL-basedmethodstoaccommodatenon-zeronoise28–anddistinguishingsignalfromnon-signal29,especiallyinunsupervised\nschemes.Asinpreviouswork30,weaccountforenvironmentalnoisethatwouldotherwiseinterferewiththecontrastivelearning\nobjectivebyintegratingon-the-flynoisereductionlayersintothemodelarchitecture. Inthispaper,weprovideanunsupervised\nframeworkforbioacousticeventdetectionbasedoncontrastiverepresentationlearning,andwedesignourmethodstooperate\ndirectlyonreal-worldacousticdata.\n2 Materials and Methods\nMotivatedbyrecentadvancesinacousticandvisualrepresentationlearninginvolvingself-supervisedcontrastivetraining\nobjectives,weapplyrepresentationlearningtechniquestobioacousticdatatoenableunsuperviseddetectionandsegmentation\nofbioacousticevents. Inparticular,givenacontinuousstreamofunlabeledacousticdata,weimplementaself-supervised\nlearningalgorithmthatexploitsanauxiliaryproxytaskwithpseudolabelsinferredfromunlabeleddata25 andaimstotrain\namodel,f,toencodetherawinputwaveformtoaspectralrepresentationthatemphasizesspectralboundariesinthesignal.\n2/11\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\nThemethodreliesontheNoiseContrastiveEstimationprinciple31,whichinvolvesoptimizingaprobabalisticcontrastiveloss\nfunctionthatallowsthemodel,f,todistinguishbetweensequentialacousticwindowsandrandomlysampleddistractorwindows.\nForthemodel,f,wemakeuseofadeepneuralnetworktodiscoverhiddenpatternsandcomplexrelationships32andtoextract\nfeaturesrelevantforoptimizingthecontrastivelearningobjective. Duringinference,apeak-findingalgorithmdetectsregionsof\nhighdissimilaritybetweentheencodedfeaturesofadjacentacousticwindows,whichcorrespondtotemporalboundariesof\nbioacousticevents.\n2.1 Dataset\nWeapplyourmethodstoaspermwhale(Physetermacrocephalus)clickdataset. Weprocesstherawacousticdatausingthe\n‘BestOf’cutsfromtheWilliamA.WatkinsCollectionofMarineMammalSoundRecordingsdatabasefromWoodsHole\nOceanographicInstitution(https://cis.whoi.edu/science/B/whalesounds/index.cfm),retrieving71wav\nfilesamountingto1.5hoursofacousticdata. Ofthese,wemanuallylabelwithhighconfidencetheclickspresentin38files\nusingRavenPro1.6.1,yieldingadatasetcontaining1738annotatedtransients. Allaudiofileswereresampledto48kHz.\n2.2 Self-SupervisedFeatureExtraction\nOperatingontherawacousticwaveformdirectly,themodelmapstheinputaudiotoarepresentationencodingdiscriminative\nfeaturesbetweensequentialacousticwindows. Featureextractioninvolvestraininganeuralnetwork, f :RN (cid:55)→RT×F,that\nencodestheN-sampleacousticwaveformtoaT-elementtemporalsequenceofF-dimensionalspectralfeatures. Whilethe\nmodelcanprocessvariable-lengthinputwaveforms,duringtraining,themodel f isgivenareal-valuedcontinuousvector\nwith fixed length N, i.e. x∈RN, that represents the input audio signal and outputs an encoded spectral representation\nz=(z ,...,z )∈RT×F consisting of a sequence of feature vectors sampled at low frequency. To train f, we follow the\n1 T\nparadigmofKreuketal.,202026. Weoptimizetheencodingfunction f inaccordancewiththeNoiseContrastiveEstimation\nprinciple such that the network learns to maximize the similarity between adjacent (i.e. sequential) windows z,z for\ni i+1\ni∈[1,T]inthelearnedrepresentationwhileminimizingthesimilaritybetweenrandomlysampleddistractorwindowsz,z for\ni j\nz ∈D(z),thesetofnonadjacentwindowstoz,i.e. D(z)={z :|i−j|>1,z ∈z}.\nj i i i j j\nFollowingKreuketal.,202026,weusethecosinesimilaritybetweentwofeaturevectorsasoursimilaritymetric\nzTz\nsim(z ,z )= 1 2\n1 2\n∥z ∥∥z ∥\n1 2\nForthecontrastivelossfunction,weimplementacross-entropyloss\nexpsim(zi,zi+1)\nLˆ(z,D (z))=−log\ni k i ∑ expsim(zi,zj)\nzj∈{zi+1}∪{Dk(zi)}\nThismeansthatgivenabatchofntrainingsamplesS={x}n ,thetotallossisgivenby26\ni i=1\nL = ∑ ∑ Lˆ(z,D (z))\ni k i\nx∈Sz∈f(x)\nGiven an acoustic waveform as input, this training criterion seeks to yield a representation in which the dissimilarity\nbetweensequentialencodedfeaturevectorsof(i.e. adjacent)audiowindowsisminimized.\n2.3 TheModel\nWeuseaConvolutionalNeuralNetwork(CNN)comprisedofone-dimensionalconvolutionallayers(Conv1d)thatoperateon\ntherawwaveformdirectly. Importantly,wealsoincludeon-the-flydatapreprocessinglayerstoremoveacousticinterferences.\nInparticular,thefirstlayerofthemodel f consistsofahigh-passfilterlayerconstructedusinganontrainableConv1dlayer\nwithfrozenweightsdeterminedbyawindowedsincfunction30,33,34. Giventhebroadbandnatureofspermwhaleclicks35,we\nselectacutofffrequencyof3kHZandatransitionbandwidth0.08toremovelow-frequencyenvironmentalbackgroundnoise\nwhilepreservingthespectralstructureoftheclicks.\nFollowingthedenoisinglayeristheconvolutionalmodel,consistingofConv1dlayerswithbatchnormalization(Batch-\nNorm1d)andLeakyReLUactivation.Weusea4-layernetworkwithkernelsizes[8,6,4,4]andstrides[4,3,2,2],corresponding\ntoahoplengthof48samples(i.e. 1ms)andareceptivefieldof136samples(i.e. ∼3ms). ForallConv1dlayers, weuse\n128outputchannels. Lastly,weincludeafully-connectedlinearprojectionlayerwithTanhactivationtooutputthespectral\nrepresentation. Withthisarchitecture,themodel f encodesinputaudiotoalearnedrepresentationcomprisedofasequenceof\n32-dimensionalfeaturevectorssampledat1kHz.\n3/11\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\nModel PR-AUC F1-Score R-Value\nSSLDetection 0.917 0.876 0.887\nEnergyAmplitudeDetection 0.571 0.576 0.620\nEnergyAmplitude+HPFDetection 0.706 0.639 0.643\nTable1– Summaryofexperimentsimplementedinthisstudy. Wecompareourapproachbasedonself-supervisedlearning\n(SSL)withbaselineenergyamplitudethresholddetectorswithandwithouthighpassfilter(HPF)preprocessing. Wereport\nPR-AUC,maximalF1-Score,andmaximalR-value.\nTotrainthemodel,weoptimizethecontrastivelearningobjectiveusingStochasticGradientDescent(SGD).Weusea\nlearningrate1e-4,momentum0.9,batchsize64for50epochs. Giventhestochasticityoftraining,weserializethemodel\nweightsaftereachepochandselectthetop-performing(intermsofthelearningobjective)modelforinference.\nThebaselineenergyamplitudedetectormodel(whichisfunctionallyequivalenttoasignal-to-noise(SNR)-basedthreshold\ndetector36)requiresnotrainableparametersandinvolvesusingapeak-findingalgorithmtodetectpeaksintheenergyofthe\ninputwaveform. Weusetherawaudioasinput. Werepeatthebaselinedetectorwithandwithoutdatapreprocessing(e.g. high\npassfilterdenoising). Inthiscase,allsuprathresholddetectionsareconsideredtobeclicks.\n2.4 InferenceandPeak-Finding\nDuringinference,weobtainthespectralrepresentationzgiventheacousticwaveformxusingthetrainedmodel(i.e. z= f(x)).\nWecomputethedistancebetweensequentialwindowsusingaboundedL2dissimilaritymetric:\n(cid:113)\nd(z ,z )= (z −z )2\n1 2 1 2\ndissim(z ,z )=tanh(α·d(z ,z )+β)\n1 2 1 2\nwhereα andβ arefixedhyperparametersobtainedbyexhaustivegridsearch. Unlikethecosinesimilaritymetric,which\nneglectsnormalization,theEuclideandistance-basedmetricpreservesinformationregardingscaling,whichisanimportant\nconsiderationforsignalsinthepresenceoflow-amplitudenonstationarynoise.Weevaluatethedissimilaritybetweenthefeature\nrepresentationsofadjacentwindowsdissim(z,z )fori∈[1,T−1]. Asthecontrastivelearningobjectiveaimstosuppress\ni i+1\ndissimilaritybetweensuccessivewindows,arelativelyhighdissimilaritymetricindicatesaspectralboundary,suggestinga\ntransitionfrombackgroundtobioacousticsignal(orviceversa)intheinputaudio. Inthecaseofspermwhaleclicks, we\nsmooththedissimilaritymetricoverthetemporalaxisbyconvolvingdissimwithaboxfunctionofwidthσ,operatingunder\ntheassumptionthatweareinterestedinsingleclicktimesasopposedtoboundariescorrespondingtoonsetsandoffsetsofthe\ntransientclicks.\nFinally,usingthescikit-learnpackage,weemployapeak-findingalgorithmtosearchforpeaksinthedissimilarity.\nWesearchoverpeakprominencesδ andattributeallsuprathresholdpeakdetectionstospermwhaleclicks.\n2.5 EvaluationMetrics\nMotivatedbyworkonphonemeboundarydetection26,weevaluatemodelperformanceusingprecision(P),recall(R)and\nF1-scorewithatolerancelevelτ. AsinKreuketal.,202026,wealsoincludetheR-value,whichservesasamorerobustmetric\nthanF1-scorethatislesssensitivetooversegmentation. Inthecaseofspermwhaleclickswechooseτ =20ms,avaluethat\nexceedsobservedinter-pulseintervals(IPIs)37butislessthantypicalinter-clickintervals(ICIs)38,eveninthecaseofbuzzes\nwhichcanexhibitclickrates1-2ordersofmagnitudelarger39. Thisemphasizestheobjectiveofresolvingacousticstructures\nonthetemporalorderofindividualclicks. Finally,wereporttheareaunderthePrecision-Recallcurve(PR-AUC).\n3 Results\nInTable1,wereportF1-score,R-value,andPR-AUCfortheproposedself-superviseddetectionmodelaswellastheenergy\nthresholddetectorbaselinewithandwithouthighpassfilter(HPF)preprocessing. OurmethodachievesamaximalF1-score\nof0.876,anR-valueof0.887,andaPR-AUCof0.917. Thebaselinethreshold-basedenergydetectionmethodoperatingon\nrawinputaudiowithnodatapreprocessing(e.g. denoising)yieldsanF1-scoreof0.576,anR-valueof0.620,andaPR-AUC\nof 0.571. Inclusion of denoising in the threshold-based energy detection method yields modest performance benefits by\neliminatinghigh-amplitudelowfrequencyenvironmentalnoise,boostingtheF1-score,R-value,andPR-AUCto0.639,0.643,\nand0.706, respectively. Theseresultssuggestthattheunsuperviseddeeplearning-basedapproachestobioacousticsignal\ndetectionimplementedinthispapercanoutperformestablishedbaselinetechniquesfordetectingeventsinacousticrecordings.\nPRcurvesarepresentedinFig. 3.\n4/11\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\nFigure1–Applicationofself-supervisedcontrastivelearningtospermwhaleclickdetection. (a)Inputacousticwaveform.\n(b)Spectrogramrepresentation. (c)Thelearnedrepresentation. (d)Thedistancemetricdissimdemonstratingboundariesof\nhighdissimilaritybetweenacousticwindows. Peaksweredetectedwithα =0.1,β =0.9,σ =5ms,δ =0.1\n5/11\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\nInFig. 1,wedemonstratetheself-superviseddetectionpipelineandresultsappliedtoanacousticrecordingcontaininga\nsequenceofclicks. Fig1a)representstheacousticwaveform,onwhichthemodel f operatesdirectly. Fig1b)presentsa\nspectrogramrepresentationtovisualizethespectrotemporalfeaturesofthesignal. Thelearnedrepresentationzresultingfrom\ntheoptimizationofthecontrastivelearningobjectiveisshowninFig1c). Finally,wecomputetheboundedL2dissimilarity\nmetric using α =0.1 and β =0.9 and plot the result in Fig 1 d). Employing a peak-finding algorithm, we search over\nprominencesδ toobtainthePR-AUC,observingmaximalF1-scorewithδ =0.1,andwesuperimposethedetectedpeaksin\nFig1d). Together,thequantitativemetricsandvisualizationsinFig1demonstratetheefficacyofapplyingself-supervised\ncontrastivelearninginthecontextofbioacousticseventdetection.\nInFig. 2,weprovideinitialexploratoryresultsapplyingself-supervisedcontrastivelearningtootherdatasetsanddata\nmodalities. InFig. 2a),wesegmentBengalesefinch(Lonchurastriatadomestica)birdsong40intopredictionsforsequential\nvocalunits,orsyllables. InFig. 2b),weusethecontrastivedetectionframeworktopredictbehavioraltransitionsingreensea\nturtle(Cheloniamydas)behavioraldynamicdata41. InFig. 2c),weinvestigateunsupervisedembryotrackingbyidentifying\ntransitionscharacteristicofcelldivisionsinamouseembryodevelopmentdataset42. Wedeferamorethoroughanalysisto\nfuturestudies.\n4 Discussion\nWhilestate-of-the-artcomputationalmethodsforbioacousticeventdetectiontendtotreatdetectionasabinaryclassification\nand/orpresenceindicationproblemandrelyonsupervisedlearningtechniques,weproposeanovelself-supervisedapproach\nto detect spectral changes associated with bioacoustic signals. Importantly, our unsupervised method requires no manual\nannotation during training, shifting the paradigm from existing techniques that expect large quantities of labeled training\ndata. WedemonstratethatunsupervisedDNN-basedapproachestobioacousticdetectionandsegmentationcanoutperform\nestablishedbaselinemethods,yieldingimprovedresultsintermsofprecision,recall,F1-score,R-value,andPR-AUC.\n4.1 VisionforaDL-BasedPipelineforBioacousticMonitoring\nInlinewithexistingresearch21,29,weenvisionacompletepipelineforecologicaldataanalysis–particularlyinthecontextof\nbioacoustics–involvingdiscreteDNN-basedmodulestoaddressecologicallyimportantquestionsregardinganimalcommuni-\ncationandbehavior. Further,weadvocatethedevelopmentandimplementationofunsupervisedtechniquestominimizethe\nroleofthehumanobserverandcircumventtherelianceon–anddrawbacksof–manualannotation2. Explicitly,wepropose\naframeworkencompassingdetection,classification,localization,andtracking(DCLT)36.Themulti-steppipelineinvolves\n(1)signalprocessing;(2)eventboundarydetection;(3)sourceseparation,localization,andtracking;and(4)clusteringand\nclassification. Thisstudyservesasaproof-of-concepttodemonstratetheapplicationofunsupervisedDNN-basedmethodsto\nsolveSteps(1)and(2). Weencouragefutureexperimentalsetupstoexpandonourresults.\nStep(1)(i.e. signalprocessing)requiresanemphasisondenoisinggiventheinfeasibilityofobtainingnon-noisyacoustic\ndatainreal-worldenvironmentalrecordings;bioacousticdenoisingisanactiveareaofresearch43,andtoourknowledge,there\nexistnoDNN-basedtechniquesforbioacousticnoisereduction,aregimethatdiffersfromtherelatedsubfieldofhumanspeech\nenhancementinthathumanspeechenhancementtypicallyreliesonsupervisedmethodstoseparatenoiseandsignal44. While\nweintegratefixeddenoisingintoourmodelarchitectureusingahigh-passfiltertoeliminatelow-frequencyenvironmentalnoise,\nourmethodscouldbenefitfromenhancedDL-baseddenoisingtoremovenoisethatspanstheentirespectralzoneofsupportfor\nthegivensignalofinterest.\nThefocusofourpaperremainsStep(2)(i.e. boundarydetection). WhileourresultsindicatethatDNN-basedapproaches\ncansignificantlyoutperformthreshold-basedmethods(whichwereusedinKoumura&Okanoya, 2016), morerecentDL\ntechniques could be employed to further boost performance. For instance, vector-quantized contrastive predictive coding\n(VQ-CPC)hasbeenusedtoadvancephonemesegmentationandacousticunitdiscovery27.\nStep(3)(i.e. bioacousticsourceseparation)hasseenmajorinroadsinrecentyears30,45. Thisincludesthedevelopmentof\nunsupervisedsourceseparationmethods45,animportantconsiderationthatcanimprovedownstreamclassificationperformance.\nWhile mixture invariant training (MixIT) algorithm45 provides an effective means for separating sources in bioacoustic\nrecordings,bioacousticsourceseparationcouldfurtherbenefitfromalternativeunsupervisedmethods. Therelatedproblemsof\nsourcelocalizationandsourcetrackinginvolveextractingsignalsfromcontinuousacousticstreams,evenwhenthesources\nresponsibleforgeneratingtheeventaretravelinginspaceandtime;whileDL-basedapproachestolocalizationhaveindicated\npromising results3,46, together, separation, localization, and tracking remain underexplored in the literature but represent\nimportantconsiderationswhenprocessingecologicaldatatoassessanimalbehaviorandcommunication36.\nStep (4) (i.e. clustering and classification) can provide important insight into questions concerning animal behavior,\ncommunication,andecology. Analyzingvocalrepertoires,classifyingvocalizationsaccordingtotype,identifyingunique\nindividuals,andcharacterizinganimalbehavioraldynamicsrepresentcentralclassification-basedobjectiveswithsignificant\necological and conservation implications. While existing methods have explored both supervised4 and unsupervised16\n6/11\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\napproachesforclassifyinginputdataaccordingtotargetlabel(i.e. calltype,animalidentity,etc.),DNN-basedadvancescould\nofferimprovedperformanceandreducedrelianceonthehumanobserver. DNNsarepowerfulinstrumentsfordiscovering\nhiddenandcrypticpatternsinlarge-scalemulti-modaldatasets2,andunsupervisedDNN-basedtechniquesforclassification\ntasks,inparticular,haveshownthepotentialforenhancingclusterabilityandfeaturediscrimination47,enablingthescientific\ncommunitytoanswercomplexquestionsregardingecologyandanimalbehavior.\nFigure2–Schematicexperimentalapplicationsofself-supervisedcontrastivelearningtootherdatasetsanddatamodalities.\n(a)Bengalesefinchsyllablesegmentation;vocalunitsweresegmentedusingaZ-score-basedpeakdetectionalgorithm;(b)\nDetectionofbehavioraltransitionsinfree-rangingGreenseaturtlesaccordingtoanimal-bornemulti-channelaccelerometry\nandgyroscopedata. (c)Trackingofmouseembryodevelopment.\n4.2 ApplicationstoOtherDatasetsandDataModalities\nFinally,inFig. 2weschematicallydemonstratethatthesemethodscangeneralizetootherdatasetsanddatamodalitiesbeyond\nbioacousticclickdetection. InFig. 2a),weapplyourmethodstoBengalesefinch40syllablesegmentation. Bengalesefinch\n7/11\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\nbirdsongiscomprisedofsequentialvocalelements,knownassyllables.Wereformulatethedetectionproblemasasegmentation\nproblem,whichinvolvesdetectingtemporalonsetsandoffsetsofsignalsandextractingthevocalunitboundedbythedetections.\nAsinthecaseofspermwhales,weuseadatasetcontainingacousticrecordings;inparticular,weapplythecontrastivelearning\nobjectivetoacollectionofsongfromfourBengalesefinches40 tosegmentthecontinuousaudiostreamintopredictionsfor\nvocalunits.\nInFig. 2b),weuseself-supervisedlearningtoaddressgreenseaturtlebehavioraldynamics41. Theaimistoautomatically\nidentify,predict,andmonitorvariousbehaviorsoffree-rangingseaturtlesintheirnaturalhabitatthroughtheuseofanimal-borne\nmulti-sensorrecorders48. Thedatasetconsistsofmulti-channeltimeseriescorrespondingtoacceleration, gyroscope, and\ndepthrecordedusinganimal-bornesensorsfrom13immaturegreenseaturtles. Weshowthatself-supervisedlearninghasthe\npotentialtorevealtransitionboundariesbetweenbehaviors,providingforautomaticsegmentationofanimalbehaviordata.\nFuturestudiescanconsiderdownstreamclassificationand/orclustering16topredicttheclasslabelofthesegmentedbehavior.\nAndinFig. 2c),weshowthatself-supervisedcontrastivelearningcanbeleveragedtoaddressquestionsregardingembryo\ntrackinganddevelopment42. Weuseamouseembryotrackingdatabasecontaining100samplesofembryosprogressingtothe\n8-cellstage42. ADNNtrainedbyoptimizingthecontrastivelearningobjectiveencodesembryoimageinputtoarepresentation\nemphasizingboundariesanddiscontinuitiesinembryodevelopment,whichisusedtopredicttransitionsbetweendevelopmental\nstages.\nWhile we demonstrate the potential for additional implementations, we suggest further studies exploring ecological\napplicationsofourmethodsbyemployingself-supervisedcontrastivelearningtodetectdiscontinuities,transitions,andevents\ninadditionalbioacousticsdatasetsaswellasanimalbehavioraldynamicsdatasets.\nFigure3–Precision-Recall(PR)curvesforourself-superviseddetector(blue),thebaselineenergyamplitudethreshold\ndetectorwithhighpassfilter(HPF)preprocessing(green),andthebaselineenergyamplitudethresholddetectorwithno\npreprocessing(red). DashedlinesdenotePRvaluescorrespondingtomaximalF1-scores.\n5 Conclusion\nInthispaper,weapplyself-supervisedrepresentationlearningtoecologicaldataanalysiswithanemphasisonbioacousticevent\ndetection. Inparticular,weconstructaCNN-basedmodelandoptimizeacontrastivelearningobjectiveinaccordancewith\ntheNoiseContrastiveEstimationprincipletoyieldarepresentationofinputaudiothatencodesfeaturesinvolvedindetecting\nspectralchangesand/orboundaries,enablingthemodeltopredicttemporalonsetsand/oroffsetsofbioacousticsignals. We\ncomputeadissimilaritymetrictocomparesequentialacousticwindows,andweemployapeak-findingalgorithmtodetect\nsuprathresholddissimilaritiesindicativeofatransitionfromnon-signaltosignalandviceversa. Inthecaseofspermwhale\nclicks,wepresentquantitativeperformancemetricsintheformofF1-scores,R-values,andPR-AUCs,concludingthatthe\n8/11\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\nunsupervisedDLapproachbasedoncontrastiverepresentationlearningoutperformsbaselinemethodssuchasenergyamplitude\ndetectors. Interestingly,weobservethatomittingthesmoothingoperationσ mayenabletheresolutionoffiner-temporal-scale\nstructuressuchasIPIs,whichcouldallowbiologiststoidentifyindividualspermwhales49usinganunsupervisedcomputational\ntechnique;weencourageadditionalstudiestofurtherexplorethisobservation. Wealsoconsiderapplicationsofthemethodsto\notherdatasetsanddatamodalities,includingbioacousticdataproducedbyothertaxa,behavioraldynamicdata,andimaging\ndata,demonstratingthatthecontrastivelearningobjectivecanhavewide-rangingimplicationsforecologicaldataanalysis.\nThis paper serves as an important step towards the realization of a fully automated system for processing bioacoustic\ndatawhileminimizingtheconventionalroleofthetrainedexperthumanobserver. Ourmethodsandproposalspavetheway\nforfuturestudiesthatshouldaimtoconstructacompleteframeworkforecologicaldataanalysisinordertoelucidateand\nunderstandanimalbehaviorand,subsequently,todesignbetter-informedstrategiesandapproachesforspeciesconservationat\nlarge.\n6 Data Availability\nThespermwhaleclickdatathatsupportthefindingsofthisstudyarepubliclyavailableinthe‘BestOf’cutsfromtheWatkins\nMarineMammalSoundDatabase,WoodsHoleOceanographicInstitution,andtheNewBedfordWhalingMuseum(https:\n//cis.whoi.edu/science/B/whalesounds/index.cfm). Thebengalesefinchdata40areavailablefromfigshare\nwith identifier https://doi.org/10.6084/M9.figshare.4805749.V5, and the green sea turtle data41 used in\nthisstudyareavailablefromDryad withidentifierhttps://doi.org/10.5061/dryad.hhmgqnkd9. Theembryo\ndevelopment data42 that support the results are publicly available online http://celltracking.bio.nyu.edu/.\nThe custom-written code (Python 3.8.3) is available at our GitHub https://github.com/colossal-compsci/\nSSLUnsupDet.\nReferences\n1. Andreas,J.etal. Towardunderstandingthecommunicationinspermwhales. iScience25,104393,DOI:https://doi.org/10.\n1016/j.isci.2022.104393(2022).\n2. Goodwin,M.etal. Unlockingthepotentialofdeeplearningformarineecology: overview,applications,andoutlook.\nICESJ.Mar.Sci.79,319–336,DOI:https://doi.org/10.1093/icesjms/fsab255(2022).\n3. Stowell,D. Computationalbioacousticswithdeeplearning: Areviewandroadmap. PeerJ 10,DOI:https://doi.org/10.\n7717/peerj.13152(2022).\n4. Bermant,P.C.,Bronstein,M.M.,Wood,R.J.,Gero,S.&Gruber,D.F. Deepmachinelearningtechniquesforthedetection\nandclassificationofspermwhalebioacoustics. Sci.Reports9,DOI:https://doi.org/10.1038/s41598-019-48909-4(2019).\n5. Allen,A.N.etal. Aconvolutionalneuralnetworkforautomateddetectionofhumpbackwhalesonginadiverse,long-term\npassiveacousticdataset. Front.Mar.Sci.8,DOI:https://doi.org/10.3389/fmars.2021.607321(2021).\n6. Zhong,M.etal. Detecting,classifying,andcountingbluewhalecallswithsiameseneuralnetworks. TheJ.Acoust.Soc.\nAm.149,3086–3094,DOI:https://doi.org/10.1121/10.0004828(2021).\n7. Kahl, S., Wood, C., Eibl, M. & Klinck, H. Birdnet: A deep learning solution for avian diversity monitoring. Ecol.\nInformatics61,101236,DOI:https://doi.org/10.1016/j.ecoinf.2021.101236(2021).\n8. Ruan,W.,Wu,K.,Chen,Q.&Zhang,C. Resnet-basedbio-acousticspresencedetectiontechnologyofhainangibboncalls.\nAppl.Acoust.198,108939,DOI:https://doi.org/10.1016/j.apacoust.2022.108939(2022).\n9. White,E.L.etal. Morethanawhistle: Automateddetectionofmarinesoundsourceswithaconvolutionalneuralnetwork.\nFront.Mar.Sci.9,DOI:https://10.3389/fmars.2022.879145(2022).\n10. Nguyen Hong Duc, P. et al. Assessing inter-annotator agreement from collaborative annotation campaign in marine\nbioacoustics. Ecol.Informatics61,101185,DOI:https://doi.org/10.1016/j.ecoinf.2020.101185(2021).\n11. Leroy, E. C., Thomisch, K., Royer, J.-Y., Boebel, O. & Van Opzeeland, I. On the reliability of acoustic annotations\nandautomaticdetectionsofantarcticbluewhalecallsunderdifferentacousticconditions. TheJ.Acoust.Soc.Am.144,\n740–754,DOI:https://doi.org/10.1121/1.5049803(2018).\n12. Cronkite,D.,Malin,B.,Aberdeen,J.,Hirschman,L.&Carrell,D. Isthejuiceworththesqueeze? costsandbenefitsof\nmultiplehumanannotatorsforclinicaltextde-identification. MethodsInf.Medicine55,356–364,DOI:https://doi.org/10.\n3414/me15-01-0122(2016).\n9/11\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\n13. Cartwright,M.,Dove,G.,Méndez,A.E.M.,Bello,J.P.&Nov,O. Crowdsourcingmulti-labelaudioannotationtasks\nwith citizen scientists. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, DOI:\nhttps://doi.org/10.1145/3290605.3300522(ACM,2019).\n14. Cartwright,M.etal. Seeingsound: Investigatingtheeffectsofvisualizationsandcomplexityoncrowdsourcedaudio\nannotations. Proc.ACMonHuman-ComputerInteract.1,1–21,DOI:https://doi.org/10.1145/3134664(2017).\n15. Bergler,C.etal. Orca-spot: Anautomatickillerwhalesounddetectiontoolkitusingdeeplearning. Sci.Reports9,10997,\nDOI:https://doi.org/10.1038/s41598-019-47335-w(2019).\n16. Thomas,M.etal. Apracticalguideforgeneratingunsupervised,spectrogram-basedlatentspacerepresentationsofanimal\nvocalizations. J.AnimalEcol.91,1567–1581,DOI:https://doi.org/10.1111/1365-2656.13754(2022).\n17. Xie,J.,Hu,K.,Zhu,M.&Guo,Y. Bioacousticsignalclassificationincontinuousrecordings: Syllable-segmentationvs\nsliding-window. Expert.Syst.withAppl.152,113390,DOI:https://doi.org/10.1016/j.eswa.2020.113390(2020).\n18. Steinfath,E.,Palacios-Muñoz,A.,Rottschäfer,J.R.,Yuezak,D.&Clemens,J. Fastandaccurateannotationofacoustic\nsignalswithdeepneuralnetworks. eLife10,e68837,DOI:https://doi.org/10.7554/eLife.68837(2021).\n19. Briggs,F.,Raich,R.&Fern,X. Asupervisedapproachforsegmentationofbioacousticsaudiorecordings. TheJ.Acoust.\nSoc.Am.133,3310–3310,DOI:https://doi.org/10.1121/1.4805500(2013).\n20. Roger,V.,Bartcus,M.,Chamroukhi,F.&Glotin,H. UnsupervisedBioacousticSegmentationbyHierarchicalDirichlet\nProcessHiddenMarkovModel,113–130(SpringerInternationalPublishing,2018).\n21. Koumura, T. & Okanoya, K. Automatic recognition of element classes and boundaries in the birdsong with variable\nsequences. PLOSONE11,1–24,DOI:https://doi.org/10.1371/journal.pone.0159188(2016).\n22. Papapanagiotou,V.,Diou,C.&Delopoulos,A. Self-supervisedfeaturelearningof1dconvolutionalneuralnetworkswith\ncontrastivelossforeatingdetectionusinganin-earmicrophone. 202143rdAnnu.Int.Conf.IEEEEng.Medicine&Biol.\nSoc.(EMBC)7186–7189,DOI:https://doi.org/10.1109/EMBC46164.2021.9630399(2021).\n23. Chen,T.,Kornblith,S.,Norouzi,M.&Hinton,G. Asimpleframeworkforcontrastivelearningofvisualrepresentations.\nInProceedingsofthe37thInternationalConferenceonMachineLearning,no.149inICML’20,11,DOI:https://doi.org/\n10.5555/3524938.3525087(JMLR.org,2020).\n24. van den Ooord, A., Li, Y. & Vinyals, O. Representation learning with contrastive predictive coding. Preprint at\nhttps://arxiv.org/abs/1807.03748(2018).\n25. Fonseca,E.,Ortego,D.,McGuinness,K.,O’Connor,N.E.&Serra,X. Unsupervisedcontrastivelearningofsoundevent\nrepresentations. Preprintathttps://arxiv.org/abs/2011.07616(2020).\n26. Kreuk,F.,Keshet,J.&Adi,Y. Self-supervisedcontrastivelearningforunsupervisedphonemesegmentation. Preprintat\nhttps://arxiv.org/abs/2007.13465(2020).\n27. vanNiekerk,B.,Nortje,L.&Kamper,H. Vector-quantizedneuralnetworksforacousticunitdiscoveryinthezerospeech\n2020challenge. Preprintathttps://arxiv.org/abs/2005.09409(2020).\n28. Denton,T.etal. Handlingbackgroundnoiseinneuralspeechgeneration. In202054thAsilomarConferenceonSignals,\nSystems,andComputers,667–671,DOI:10.1109/IEEECONF51394.2020.9443535(2020).\n29. Sainburg,T.&Gentner,T.Q. Towardacomputationalneuroethologyofvocalcommunication: Frombioacousticsto\nneurophysiology,emergingtoolsandfuturedirections. FrontiersDOI:https://doi.org/10.3389/fnbeh.2021.811737/full\n(2021).\n30. Bermant,P.C. Biocppnet: automaticbioacousticsourceseparationwithdeepneuralnetworks. Sci.Reports11,23502,\nDOI:https://doi.org/10.1038/s41598-021-02790-2(2021).\n31. Gutmann, M.&Hyvärinen, A. Noise-contrastiveestimation: Anewestimationprincipleforunnormalizedstatistical\nmodels. In Teh, Y. W. & Titterington, M. (eds.) Proceedings of the Thirteenth International Conference on Artificial\nIntelligenceandStatistics,vol.9ofProceedingsofMachineLearningResearch,297–304(2010).\n32. Lee,B.D.etal. Tenquicktipsfordeeplearninginbiology. PLOSComput.Biol.18,1–20,DOI:https://doi.org/10.1371/\njournal.pcbi.1009803(2022).\n33. Smith, S. The Scientist and Engineer’s Guide to Digital Signal Processing, chap. Windowed-Sinc Filters (California\nTechnicalPublishing,1999).\n34. Ravanelli,M.&Bengio,Y. Interpretableconvolutionalfilterswithsincnet. Preprintathttps://arxiv.org/abs/1811.09725\n(2019).\n10/11\n\nbioRxiv preprint doi: https://doi.org/10.1101/2022.10.12.511740; this version posted October 16, 2022. The copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\navailable under aCC-BY 4.0 International license.\n35. Lopatka, M., Adam, O., Laplanche, C., Motsch, J.-F. & Zarzycki, J. Sperm whale click analysis using a recursive\ntime-variant lattice filter. Appl. Acoust. 67, 1118–1133, DOI: https://doi.org/10.1016/j.apacoust.2006.05.011 (2006).\nDetectionandlocalizationofmarinemamalsusingpassiveacoustics.\n36. Zimmer,W.M. PassiveAcousticMonitoringofCetaceans(CambridgeUniversityPress,2011).\n37. Bøttcher,A.,Gero,S.,Beedholm,K.,Whitehead,H.&Madsen,P.T. Variabilityoftheinter-pulseintervalinspermwhale\nclickswithimplicationsforsizeestimationandindividualidentification. TheJ.Acoust.Soc.Am.144,365–374,DOI:\nhttps://doi.org/10.1121/1.5047657(2018).\n38. Whitehead, H. & Weilgart, L. Click rates from sperm whales. The J. Acoust. Soc. Am. 87, 1798–1806, DOI: https:\n//doi.org/10.1121/1.399376(1990).\n39. Fais,A.,Johnson,M.,Mand.Wilson,AguilarSoto,N.&Madsen,P. Spermwhalepredator-preyinteractionsinvolve\nchasingandbuzzing,butnoacousticstunning. Sci.Reports6,28562,DOI:https://doi.org/10.1038/srep28562(2016).\n40. Nicholson,D.,Queen,J.E.&Sober,S.J. Bengalesefinchsongrepository,DOI:https://doi.org/10.6084/M9.figshare.\n4805749.V5(2017).\n41. Jeantet,L.etal. Rawacceleration,gyroscopeanddepthprofilesassociatedwiththeobservedbehavioursoffree-ranging\nimmaturegreenturtlesinmartinique,DOI:https://doi.org/10.5061/dryad.hhmgqnkd9(2020).\n42. Cicconet,M.,Gutwein,M.,Gunsalus,K.C.&Geiger,D. Labelfreecell-trackinganddivisiondetectionbasedon2d\ntime-lapseimagesforlineageanalysisofearlyembryodevelopment(2014).\n43. Xie, J., Colonna, J. G. & Zhang, J. Bioacoustic signal denoising: a review. Artif. Intell. Rev. 54, 3575–3597, DOI:\nhttps://doi.org/10.1007/s10462-020-09932-4(2021).\n44. Saleem,N.&Khattak,M.I. Areviewofsupervisedlearningalgorithmsforsinglechannelspeechenhancement. Int.J.\nSpeechTechnol.22,1051–1075,DOI:https://doi.org/10.1007/s10772-019-09645-2(2019).\n45. Denton,T.,Wisdom,S.&Hershey,J.R. Improvingbirdclassificationwithunsupervisedsoundseparation. ICASSP2022-\n2022IEEEInt.Conf.onAcoust.SpeechSignalProcess.(ICASSP)636–640(2022).\n46. Francl,A.&McDermott,J.H. Deepneuralnetworkmodelsofsoundlocalizationrevealhowperceptionisadaptedto\nreal-worldenvironments. Nat.Hum.Behav.6,111–133,DOI:https://doi.org/10.1038/s41562-021-01244-z(2022).\n47. Károly, A.I., Fullér, R.&Galambos, P. Unsupervisedclusteringfordeeplearning: Atutorialsurvey. ActaPolytech.\nHungarica15(2018).\n48. Jeantet,L.etal. Behaviouralinferencefromsignalprocessingusinganimal-bornemulti-sensorloggers: anovelsolutionto\nextendtheknowledgeofseaturtleecology. RoyalSoc.OpenSci.7,200139,DOI:https://doi.org/10.1098/rsos.200139\n(2020).\n49. Hirotsu,R.,Ura,T.,Bahl,R.&Yanagisawa,M. Analysisofspermwhaleclickbymusicalgorithm. InOCEANS2006-\nAsiaPacific,1–6,DOI:https://10.1109/OCEANSAP.2006.4393900(2006).\n7 Author Information\nAffiliations\nColossalBiosciences,Austin,TX,USA\nFormBio,Austin,TX,USA\nContributions\nP.C.Bperformedtasksetting,dataprocessing,machinelearning,articlewriting,figuremaking,L.B.assistedinexperimental\ndesign,A.J.T.supervisedtheanalysis,andallauthorswroteandreviewedthemanuscript.\nCorrespondingAuthor\nCorrespondencetoP.C.Bermant\n8 Competing Interests\nTheauthorsdeclarenocompetinginterests.\n11/11", "affiliations": [{"university": "", "country": "", "discipline": ""}, {"university": "", "country": "", "discipline": ""}], "species_categories": ["Marine Mammal", "Bird", "Other"], "specialized_species": ["sperm whale", "Bengalese finch", "green sea turtle"], "computational_stages": ["Data Collection", "Pre-processing", "Sequence Representation", "Meaning Identification"], "linguistic_features": ["Semanticity", "Discreteness and Syntax"], "updated_at": "2026-01-27T21:01:58.155547", "committed_at": "2026-01-27T21:02:00.392545"}
